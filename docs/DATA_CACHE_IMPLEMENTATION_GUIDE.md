# ğŸ“Š data_panelæ¡†æ¶æ•°æ®ç¼“å­˜åŠŸèƒ½å®ç°æŒ‡å—

> **æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
> **æœ€åæ›´æ–°**: 2025å¹´8æœˆ1æ—¥  
> **ä½œè€…**: data_panelå¼€å‘å›¢é˜Ÿ

## ğŸ“‹ ç›®å½•

- [ğŸ“– æ¦‚è¿°](#-æ¦‚è¿°)
- [ğŸ—ï¸ é¡¹ç›®æ¶æ„](#ï¸-é¡¹ç›®æ¶æ„)
- [ğŸš€ ç¼“å­˜ç³»ç»ŸåŸç†](#-ç¼“å­˜ç³»ç»ŸåŸç†)
- [ğŸ“ å®æ–½æ­¥éª¤](#-å®æ–½æ­¥éª¤)
- [ğŸ’¡ é«˜çº§ç”¨æ³•](#-é«˜çº§ç”¨æ³•)
- [ğŸ”§ é…ç½®å‚è€ƒ](#-é…ç½®å‚è€ƒ)
- [ğŸ¯ æœ€ä½³å®è·µ](#-æœ€ä½³å®è·µ)
- [ğŸ› å¸¸è§é—®é¢˜](#-å¸¸è§é—®é¢˜)

## ğŸ“– æ¦‚è¿°

data_panelæ˜¯ä¸€ä¸ªåŸºäº**Vue.js + Flask**çš„ç½‘é¡µæ•°æ®æ˜¾ç¤ºæ¡†æ¶ï¼Œé€šè¿‡é…ç½®æ–‡ä»¶å¯ä»¥å¿«é€Ÿæ­å»ºæ•°æ®å¯è§†åŒ–é¡µé¢ã€‚æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨æ¡†æ¶çš„æ•°æ®ç¼“å­˜åŠŸèƒ½æ¥æå‡æ€§èƒ½ã€‚

### ğŸ¯ ç¼“å­˜ç³»ç»Ÿç‰¹ç‚¹

- **åŒå±‚ç¼“å­˜**: æ•°æ®ç¼“å­˜ + å“åº”ç¼“å­˜
- **æ™ºèƒ½æ›´æ–°**: åŸºäºæ–‡ä»¶æ—¶é—´æˆ³å’Œæ•°æ®å“ˆå¸Œçš„è‡ªåŠ¨å¤±æ•ˆ
- **å†…å­˜ç®¡ç†**: LRUç®—æ³•è‡ªåŠ¨æ¸…ç†ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
- **å‚æ•°åŒºåˆ†**: æ”¯æŒåŸºäºè¯·æ±‚å‚æ•°çš„ç¼“å­˜éš”ç¦»

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
data_panel/
â”œâ”€â”€ ğŸ“ api/                    # åç«¯APIæœåŠ¡
â”‚   â”œâ”€â”€ ğŸ“ conf/              # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”‚   â”œâ”€â”€ server_config.py  # æœåŠ¡å™¨é…ç½®
â”‚   â”‚   â””â”€â”€ component_config.py # ç»„ä»¶é…ç½®
â”‚   â”œâ”€â”€ ğŸ“ processors/        # æ•°æ®å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ base_processor.py # å¤„ç†å™¨åŸºç±»
â”‚   â”‚   â”œâ”€â”€ demo_processor.py # ç¤ºä¾‹å¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ your_processor.py # è‡ªå®šä¹‰å¤„ç†å™¨
â”‚   â””â”€â”€ ğŸ“ server/            # æœåŠ¡å™¨åŸºç±»å’Œå®ç°
â”‚       â”œâ”€â”€ base_server.py    # æœåŠ¡å™¨åŸºç±»ï¼ˆåŒ…å«ç¼“å­˜ï¼‰
â”‚       â””â”€â”€ demo_server.py    # ç¤ºä¾‹æœåŠ¡å™¨
â”œâ”€â”€ ğŸ“ src/                   # å‰ç«¯Vue.jsæºç 
â”‚   â”œâ”€â”€ ğŸ“ components/        # Vueç»„ä»¶
â”‚   â””â”€â”€ ğŸ“ views/             # é¡µé¢è§†å›¾
â”œâ”€â”€ ğŸ“„ project-config.json    # é¡¹ç›®ä¸»é…ç½®æ–‡ä»¶
â””â”€â”€ ğŸ“„ package.json          # å‰ç«¯ä¾èµ–é…ç½®
```

## ğŸš€ ç¼“å­˜ç³»ç»ŸåŸç†

### 1ï¸âƒ£ æ•°æ®ç¼“å­˜ (BaseDataCache)

**ä½ç½®**: `api/server/base_server.py:BaseDataCache`

**åŠŸèƒ½**:
- ç¼“å­˜ä»CSVæ–‡ä»¶æˆ–æ•°æ®åº“åŠ è½½çš„åŸå§‹æ•°æ®
- ç›‘æ§æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½å˜åŒ–çš„æ•°æ®
- æä¾›å†…å­˜ç¼“å­˜ï¼Œé¿å…é‡å¤I/Oæ“ä½œ

**å·¥ä½œæµç¨‹**:
```mermaid
graph LR
    A[è¯·æ±‚æ•°æ®] --> B{æ–‡ä»¶æ˜¯å¦å˜åŒ–?}
    B -->|æ˜¯| C[é‡æ–°åŠ è½½æ–‡ä»¶]
    B -->|å¦| D[ä½¿ç”¨ç¼“å­˜æ•°æ®]
    C --> E[æ›´æ–°ç¼“å­˜]
    E --> F[è¿”å›æ•°æ®]
    D --> F
```

### 2ï¸âƒ£ å“åº”ç¼“å­˜ (BaseResponseCache)

**ä½ç½®**: `api/server/base_server.py:BaseResponseCache`

**åŠŸèƒ½**:
- ç¼“å­˜APIæ¥å£çš„å®Œæ•´å“åº”ç»“æœ
- é€šè¿‡æ•°æ®å“ˆå¸Œå€¼åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—
- LRUç®—æ³•ç®¡ç†ç¼“å­˜å¤§å°

**å·¥ä½œæµç¨‹**:
```mermaid
graph LR
    A[APIè¯·æ±‚] --> B{å“åº”ç¼“å­˜å­˜åœ¨?}
    B -->|æ˜¯| C{æ•°æ®æ˜¯å¦å˜åŒ–?}
    B -->|å¦| G[å¤„ç†æ•°æ®]
    C -->|å¦| D[è¿”å›ç¼“å­˜å“åº”]
    C -->|æ˜¯| G
    G --> H[ç”Ÿæˆæ–°å“åº”]
    H --> I[å­˜å‚¨åˆ°ç¼“å­˜]
    I --> J[è¿”å›å“åº”]
    D --> J
```

## ğŸ“ å®æ–½æ­¥éª¤

### æ­¥éª¤1: åˆ›å»ºæ•°æ®å¤„ç†å™¨

åœ¨ `api/processors/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„å¤„ç†å™¨æ–‡ä»¶ï¼š

**æ–‡ä»¶**: `api/processors/your_table_processor.py`

```python
"""
è‡ªå®šä¹‰è¡¨æ ¼æ•°æ®å¤„ç†å™¨
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®ç¼“å­˜åŠŸèƒ½
"""
from .base_processor import BaseDataProcessor
from flask import jsonify
import pandas as pd
from typing import Dict, Any


class YourTableProcessor(BaseDataProcessor):
    """è‡ªå®šä¹‰è¡¨æ ¼å¤„ç†å™¨ - å±•ç¤ºç¼“å­˜åŠŸèƒ½çš„å®Œæ•´å®ç°"""
    
    def process_custom_table_data(self):
        """
        å¤„ç†è‡ªå®šä¹‰è¡¨æ ¼æ•°æ®
        
        å±•ç¤ºå®Œæ•´çš„ç¼“å­˜ä½¿ç”¨æµç¨‹ï¼š
        1. æ•°æ®åŠ è½½ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ•°æ®ç¼“å­˜ï¼‰
        2. å“åº”ç¼“å­˜æ£€æŸ¥
        3. æ•°æ®å¤„ç†
        4. å“åº”ç¼“å­˜å­˜å‚¨
        """
        try:
            self.logger.info("å¼€å§‹å¤„ç†è‡ªå®šä¹‰è¡¨æ ¼æ•°æ®")
            
            # ===== æ­¥éª¤1: åŠ è½½æ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨æ•°æ®ç¼“å­˜ï¼‰ =====
            df = self.data_cache.load_data('custom_table_df')
            
            if df.empty:
                return self.error_response("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º", 404)
            
            # ===== æ­¥éª¤2: æ„å»ºç¼“å­˜é”®å’Œå‚æ•° =====
            cache_endpoint = '/api/table-data/custom_table'
            cache_params = self.build_cache_params()  # è‡ªåŠ¨è·å–URLå‚æ•°
            
            # ===== æ­¥éª¤3: æ„å»ºæºæ•°æ®æ ‡è¯†ï¼ˆç”¨äºæ£€æµ‹æ•°æ®å˜åŒ–ï¼‰ =====
            source_data = {
                'data_count': len(df),
                'data_columns': list(df.columns),
                'last_update': df['update_time'].max() if 'update_time' in df.columns else None,
                'file_timestamp': self.data_cache.timestamps.get('custom_table_df', 0)
            }
            
            # ===== æ­¥éª¤4: æ£€æŸ¥å“åº”ç¼“å­˜ =====
            should_cache, cached_response = self.should_use_cache(
                cache_endpoint, cache_params, source_data
            )
            
            if should_cache and cached_response:
                self.logger.info("âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›è¡¨æ ¼")
                return cached_response
            
            # ===== æ­¥éª¤5: å¤„ç†æ•°æ®ï¼ˆç¼“å­˜æœªå‘½ä¸­æ—¶ï¼‰ =====
            self.logger.info("ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹å¤„ç†æ•°æ®")
            processed_data = self._process_table_data(df, cache_params)
            
            # ===== æ­¥éª¤6: æ„å»ºå“åº” =====
            response_data = jsonify({
                "success": True,
                "data": processed_data['records'],
                "pagination": {
                    "total": processed_data['total'],
                    "page": cache_params.get('page', 1),
                    "pageSize": cache_params.get('pageSize', 20),
                    "hasMore": processed_data['has_more']
                },
                "metadata": {
                    "columns": processed_data['columns'],
                    "updateTime": source_data['last_update'],
                    "cached": False
                },
                "message": "æ•°æ®åŠ è½½æˆåŠŸ"
            })
            
            # ===== æ­¥éª¤7: å­˜å‚¨åˆ°å“åº”ç¼“å­˜ =====
            self.store_cache(cache_endpoint, cache_params, source_data, response_data)
            self.logger.info("ğŸ’¾ å“åº”å·²å­˜å‚¨åˆ°ç¼“å­˜")
            
            return response_data
            
        except Exception as e:
            self.logger.error(f"å¤„ç†è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
            return self.error_response(f"å¤„ç†è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
    
    def _process_table_data(self, df: pd.DataFrame, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            params: è¯·æ±‚å‚æ•°
            
        Returns:
            å¤„ç†åçš„æ•°æ®å­—å…¸
        """
        # è·å–åˆ†é¡µå‚æ•°
        page = int(params.get('page', 1))
        page_size = int(params.get('pageSize', 20))
        
        # è·å–æ’åºå‚æ•°
        sort_field = params.get('sortField', None)
        sort_order = params.get('sortOrder', 'asc')
        
        # è·å–ç­›é€‰å‚æ•°
        filters = params.get('filters', {})
        
        # æ•°æ®ç­›é€‰
        filtered_df = self._apply_filters(df, filters)
        
        # æ•°æ®æ’åº
        if sort_field and sort_field in filtered_df.columns:
            ascending = sort_order.lower() == 'asc'
            filtered_df = filtered_df.sort_values(sort_field, ascending=ascending)
        
        # è®¡ç®—æ€»æ•°
        total = len(filtered_df)
        
        # åˆ†é¡µå¤„ç†
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        page_df = filtered_df.iloc[start_idx:end_idx]
        
        # è½¬æ¢ä¸ºå‰ç«¯æ‰€éœ€æ ¼å¼
        records = page_df.to_dict('records')
        
        # å¤„ç†æ•°å€¼æ ¼å¼åŒ–
        for record in records:
            for key, value in record.items():
                if pd.isna(value):
                    record[key] = None
                elif isinstance(value, (int, float)):
                    record[key] = round(value, 4) if isinstance(value, float) else value
        
        return {
            'records': records,
            'total': total,
            'has_more': end_idx < total,
            'columns': list(filtered_df.columns)
        }
    
    def _apply_filters(self, df: pd.DataFrame, filters: Dict[str, Any]) -> pd.DataFrame:
        """åº”ç”¨æ•°æ®ç­›é€‰"""
        filtered_df = df.copy()
        
        for field, filter_value in filters.items():
            if field not in df.columns:
                continue
                
            if isinstance(filter_value, dict):
                # èŒƒå›´ç­›é€‰
                if 'min' in filter_value and filter_value['min'] is not None:
                    filtered_df = filtered_df[filtered_df[field] >= filter_value['min']]
                if 'max' in filter_value and filter_value['max'] is not None:
                    filtered_df = filtered_df[filtered_df[field] <= filter_value['max']]
            elif isinstance(filter_value, str):
                # æ–‡æœ¬ç­›é€‰
                filtered_df = filtered_df[filtered_df[field].astype(str).str.contains(filter_value, na=False)]
            elif isinstance(filter_value, list):
                # å¤šé€‰ç­›é€‰
                filtered_df = filtered_df[filtered_df[field].isin(filter_value)]
        
        return filtered_df
    
    def get_table_summary(self):
        """è·å–è¡¨æ ¼æ‘˜è¦ä¿¡æ¯ - å¦ä¸€ä¸ªç¼“å­˜ç¤ºä¾‹"""
        try:
            cache_endpoint = '/api/table-summary/custom_table'
            df = self.data_cache.load_data('custom_table_df')
            
            if df.empty:
                return self.error_response("æ•°æ®ä¸å­˜åœ¨", 404)
            
            # æ„å»ºæºæ•°æ®æ ‡è¯†
            source_data = {
                'row_count': len(df),
                'file_timestamp': self.data_cache.timestamps.get('custom_table_df', 0)
            }
            
            # æ£€æŸ¥ç¼“å­˜
            should_cache, cached_response = self.should_use_cache(
                cache_endpoint, None, source_data
            )
            
            if should_cache and cached_response:
                return cached_response
            
            # è®¡ç®—æ‘˜è¦ä¿¡æ¯
            summary = {
                'totalRows': len(df),
                'totalColumns': len(df.columns),
                'numericColumns': len(df.select_dtypes(include=['number']).columns),
                'textColumns': len(df.select_dtypes(include=['object']).columns),
                'nullCounts': df.isnull().sum().to_dict(),
                'dataTypes': df.dtypes.astype(str).to_dict()
            }
            
            response_data = jsonify(summary)
            
            # å­˜å‚¨ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
            
        except Exception as e:
            return self.error_response(f"è·å–è¡¨æ ¼æ‘˜è¦å¤±è´¥: {e}")
```

### æ­¥éª¤2: é…ç½®æœåŠ¡å™¨æ•°æ®è·¯å¾„

ä¿®æ”¹æˆ–åˆ›å»ºæœåŠ¡å™¨ç±»ï¼Œé…ç½®æ•°æ®æ–‡ä»¶è·¯å¾„ï¼š

**æ–‡ä»¶**: `api/server/your_server.py`

```python
"""
è‡ªå®šä¹‰æœåŠ¡å™¨ - å±•ç¤ºç¼“å­˜é…ç½®
"""
from .base_server import BaseStockServer
from ..processors.your_table_processor import YourTableProcessor


class YourCustomServer(BaseStockServer):
    """è‡ªå®šä¹‰æœåŠ¡å™¨ç±»"""
    
    def __init__(self, port=5009):
        super().__init__(
            name="è‡ªå®šä¹‰æ•°æ®æœåŠ¡å™¨",
            port=port,
            auto_update_config={
                'enabled': True,
                'interval': 60,  # 60ç§’è‡ªåŠ¨æ›´æ–°
                'components': ["custom_table"],
                'random_selection': False
            }
        )
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        self.table_processor = YourTableProcessor(self)
    
    def get_data_cache_file_paths(self) -> dict:
        """
        é…ç½®æ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        é‡è¦ï¼šè¿™é‡Œé…ç½®çš„æ–‡ä»¶è·¯å¾„ä¼šè¢«è‡ªåŠ¨ç›‘æ§
        å½“æ–‡ä»¶ä¿®æ”¹æ—¶é—´å˜åŒ–æ—¶ï¼Œç¼“å­˜ä¼šè‡ªåŠ¨å¤±æ•ˆå¹¶é‡æ–°åŠ è½½
        """
        return {
            # åŸºç¡€æ•°æ®æ–‡ä»¶
            'stock_df': 'data/stock_list.csv',
            'plate_df': 'data/plate_data.csv',
            'market_df': 'data/market_summary.csv',
            
            # è‡ªå®šä¹‰è¡¨æ ¼æ•°æ®æ–‡ä»¶
            'custom_table_df': 'data/custom_table_data.csv',
            
            # æ›´å¤šæ•°æ®æ–‡ä»¶...
            'config_df': 'data/system_config.csv',
            'user_df': 'data/user_data.csv'
        }
    
    def register_custom_routes(self):
        """æ³¨å†Œè‡ªå®šä¹‰è·¯ç”±"""
        # è¡¨æ ¼æ•°æ®æ¥å£
        self.app.add_url_rule(
            '/api/table-data/custom_table',
            'get_custom_table_data',
            self.table_processor.process_custom_table_data,
            methods=['GET']
        )
        
        # è¡¨æ ¼æ‘˜è¦æ¥å£
        self.app.add_url_rule(
            '/api/table-summary/custom_table',
            'get_custom_table_summary',
            self.table_processor.get_table_summary,
            methods=['GET']
        )
        
        # ç¼“å­˜ç®¡ç†æ¥å£
        self.app.add_url_rule(
            '/api/cache/warm-up',
            'warm_up_cache',
            self._warm_up_cache,
            methods=['POST']
        )
    
    def _warm_up_cache(self):
        """ç¼“å­˜é¢„çƒ­æ¥å£"""
        try:
            # é¢„çƒ­å¸¸ç”¨æ¥å£
            endpoints_to_warm = [
                '/api/table-data/custom_table',
                '/api/table-summary/custom_table'
            ]
            
            warmed_count = 0
            for endpoint in endpoints_to_warm:
                try:
                    # æ¨¡æ‹Ÿè¯·æ±‚ä»¥é¢„çƒ­ç¼“å­˜
                    with self.app.test_client() as client:
                        client.get(endpoint)
                    warmed_count += 1
                except Exception as e:
                    self.logger.warning(f"é¢„çƒ­å¤±è´¥ {endpoint}: {e}")
            
            return jsonify({
                'success': True,
                'warmed_endpoints': warmed_count,
                'total_endpoints': len(endpoints_to_warm),
                'message': f'æˆåŠŸé¢„çƒ­ {warmed_count} ä¸ªæ¥å£ç¼“å­˜'
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500


def main():
    """å¯åŠ¨æœåŠ¡å™¨"""
    server = YourCustomServer(port=5009)
    print("ğŸš€ è‡ªå®šä¹‰æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print(f"ğŸ“‹ ç«¯å£: {server.port}")
    print(f"ğŸ—‚ï¸ ç¼“å­˜æ–‡ä»¶: {list(server.data_cache.file_paths.keys())}")
    
    server.run(debug=True)


if __name__ == '__main__':
    main()
```

### æ­¥éª¤3: åˆ›å»ºæ•°æ®æ–‡ä»¶

åˆ›å»ºæµ‹è¯•æ•°æ®æ–‡ä»¶ï¼š

**æ–‡ä»¶**: `data/custom_table_data.csv`

```csv
id,name,category,value,change,update_time
1,è‚¡ç¥¨A,ç§‘æŠ€,100.50,2.5,2025-08-01 09:30:00
2,è‚¡ç¥¨B,é‡‘è,85.20,-1.2,2025-08-01 09:30:00
3,è‚¡ç¥¨C,åŒ»è¯,120.80,3.1,2025-08-01 09:30:00
4,è‚¡ç¥¨D,èƒ½æº,95.40,0.8,2025-08-01 09:30:00
5,è‚¡ç¥¨E,æ¶ˆè´¹,110.60,-0.5,2025-08-01 09:30:00
```

### æ­¥éª¤4: é…ç½®å‰ç«¯ç»„ä»¶

åœ¨ `project-config.json` ä¸­æ·»åŠ é…ç½®ï¼š

```json
{
  "projectInfo": {
    "name": "è‡ªå®šä¹‰æ•°æ®é¢æ¿",
    "version": "1.0.0",
    "basePort": 5009
  },
  "services": [
    {
      "id": "custom_service",
      "name": "è‡ªå®šä¹‰æ•°æ®æœåŠ¡",
      "description": "å±•ç¤ºç¼“å­˜åŠŸèƒ½çš„è‡ªå®šä¹‰æœåŠ¡",
      "icon": "ğŸ“Š",
      "port": 5009,
      "path": "/custom",
      "title": "è‡ªå®šä¹‰æ•°æ®é¢æ¿",
      "serverFile": "your_server.py",
      "component": "Dashboard",
      "taskLabel": "è‡ªå®šä¹‰æœåŠ¡å™¨",
      "enabled": true
    }
  ],
  "components": [
    {
      "id": "custom_table",
      "type": "table",
      "title": "è‡ªå®šä¹‰æ•°æ®è¡¨æ ¼",
      "description": "å±•ç¤ºç¼“å­˜åŠŸèƒ½çš„æ•°æ®è¡¨æ ¼",
      "dataSource": "/api/table-data/custom_table",
      "refreshInterval": 30,
      "cache": {
        "enabled": true,
        "ttl": 300
      },
      "pagination": {
        "enabled": true,
        "pageSize": 20
      },
      "sorting": {
        "enabled": true,
        "defaultField": "value",
        "defaultOrder": "desc"
      },
      "filtering": {
        "enabled": true,
        "fields": ["category", "value"]
      }
    }
  ]
}
```

### æ­¥éª¤5: å¯åŠ¨å’Œæµ‹è¯•

1. **å¯åŠ¨æœåŠ¡å™¨**:
```bash
cd data_panel/api/server
python your_server.py
```

2. **æµ‹è¯•ç¼“å­˜åŠŸèƒ½**:
```bash
# ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆä¼šåŠ è½½æ•°æ®å¹¶ç¼“å­˜ï¼‰
curl "http://localhost:5009/api/table-data/custom_table?page=1&pageSize=10"

# ç¬¬äºŒæ¬¡è¯·æ±‚ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œå“åº”æ›´å¿«ï¼‰
curl "http://localhost:5009/api/table-data/custom_table?page=1&pageSize=10"

# æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
curl "http://localhost:5009/api/cache/status"
```

3. **è§‚å¯Ÿæ—¥å¿—**:
```
ğŸ”„ ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹å¤„ç†æ•°æ®
ğŸ’¾ å“åº”å·²å­˜å‚¨åˆ°ç¼“å­˜
âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›è¡¨æ ¼
```

## ï¿½ å¯åŠ¨ç¼“å­˜åŠŸèƒ½å®ç°

å¯åŠ¨ç¼“å­˜æ˜¯data_panelæ¡†æ¶æ–°å¢çš„é«˜çº§ç¼“å­˜åŠŸèƒ½ï¼Œä¸“é—¨ç”¨äºé‚£äº›è®¡ç®—å¤æ‚ä½†ç»“æœç›¸å¯¹ç¨³å®šçš„æ•°æ®ã€‚

### ğŸ¯ å¯åŠ¨ç¼“å­˜çš„ç‰¹ç‚¹

1. **åªè®¡ç®—ä¸€æ¬¡**: æœåŠ¡å™¨å¯åŠ¨æ—¶è®¡ç®—ï¼Œä¹‹åä¸€ç›´ä½¿ç”¨ç¼“å­˜
2. **é…ç½®é©±åŠ¨**: é€šè¿‡é…ç½®æ–‡ä»¶è‡ªåŠ¨è¯†åˆ«éœ€è¦å¯åŠ¨ç¼“å­˜çš„ç«¯ç‚¹
3. **æ™ºèƒ½é¢„çƒ­**: è‡ªåŠ¨åœ¨æœåŠ¡å™¨å¯åŠ¨åé¢„çƒ­ç¼“å­˜
4. **æ‰‹åŠ¨æ§åˆ¶**: æä¾›APIæ¥å£æ¸…é™¤å’ŒæŸ¥çœ‹ç¼“å­˜çŠ¶æ€

### ğŸ“ å¯åŠ¨ç¼“å­˜å®ç°æ­¥éª¤

#### æ­¥éª¤1: é…ç½®ç»„ä»¶çš„å¯åŠ¨ç¼“å­˜

åœ¨ `components_config.json` ä¸­ä¸ºç»„ä»¶æ·»åŠ å¯åŠ¨ç¼“å­˜é…ç½®ï¼š

```json
{
  "your_server_type": {
    "stock_basic_info": {
      "component_id": "stock_basic_info",
      "component_type": "chart",
      "title": "è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ç»Ÿè®¡",
      "api_path": "/api/chart-data/stock_basic_info",
      "description": "è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ç»Ÿè®¡å›¾è¡¨ï¼Œå¯åŠ¨æ—¶è®¡ç®—ä¸€æ¬¡",
      "enabled": true,
      "cache": {
        "strategy": "startup_once",
        "description": "åªåœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è®¡ç®—ä¸€æ¬¡"
      }
    },
    "sector_overview": {
      "component_id": "sector_overview", 
      "component_type": "chart",
      "title": "è¡Œä¸šåˆ†å¸ƒæ¦‚è§ˆ",
      "api_path": "/api/chart-data/sector_overview",
      "enabled": true,
      "cache": {
        "strategy": "startup_once",
        "params": {
          "type": "pie"
        }
      }
    }
  }
}
```

#### æ­¥éª¤2: åˆ›å»ºå¯åŠ¨ç¼“å­˜å¤„ç†å™¨

åˆ›å»ºä¸“ç”¨çš„å¤„ç†å™¨æ¥å¤„ç†å¯åŠ¨ç¼“å­˜çš„é€»è¾‘ï¼š

```python
# api/processors/your_startup_processor.py
from .base_processor import BaseDataProcessor
from flask import jsonify
import time

class YourStartupProcessor(BaseDataProcessor):
    """å¯åŠ¨ç¼“å­˜å¤„ç†å™¨"""
    
    def process_your_startup_chart(self):
        """å¤„ç†å¯åŠ¨ç¼“å­˜å›¾è¡¨"""
        try:
            cache_endpoint = '/api/chart-data/your_chart'
            cache_params = self.build_cache_params()
            
            # æ£€æŸ¥å¯åŠ¨ç¼“å­˜
            if self.server.startup_cache.is_startup_cached(cache_endpoint, cache_params):
                self.logger.info("ğŸ”’ ä½¿ç”¨å¯åŠ¨æ—¶ç¼“å­˜æ•°æ®")
                return self.server.startup_cache.get_startup_cache(cache_endpoint, cache_params)
            
            # æ‰§è¡Œå¤æ‚è®¡ç®—
            self.logger.info("ğŸ”„ å¯åŠ¨ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹è®¡ç®—...")
            
            # è¿™é‡Œæ”¾ä½ çš„å¤æ‚è®¡ç®—é€»è¾‘
            result_data = self._complex_calculation()
            
            # æ„å»ºå“åº”
            response_data = jsonify({
                "chartType": "bar",
                "data": result_data,
                "metadata": {
                    "calculatedAt": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "cached": False,
                    "cache_type": "startup_once"
                }
            })
            
            # å­˜å‚¨åˆ°å¯åŠ¨ç¼“å­˜
            self.server.startup_cache.set_startup_cache(cache_endpoint, cache_params, response_data)
            
            return response_data
            
        except Exception as e:
            return self.error_response(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    def _complex_calculation(self):
        """å¤æ‚è®¡ç®—é€»è¾‘"""
        time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—
        # ä½ çš„è®¡ç®—é€»è¾‘
        return [{"x": [1,2,3], "y": [4,5,6]}]
```

#### æ­¥éª¤3: æœåŠ¡å™¨é›†æˆå¯åŠ¨ç¼“å­˜

ç”±äº `BaseStockServer` å·²ç»é›†æˆäº†å¯åŠ¨ç¼“å­˜åŠŸèƒ½ï¼Œä½ åªéœ€è¦ï¼š

```python
# api/server/your_server.py
from .base_server import BaseStockServer
from ..processors.your_startup_processor import YourStartupProcessor

class YourServer(BaseStockServer):
    def __init__(self, port=5011):
        # ç»„ä»¶ç®¡ç†å™¨ä¼šè‡ªåŠ¨ä»é…ç½®è¯»å–å¯åŠ¨ç¼“å­˜è®¾ç½®
        self.component_manager = ComponentManager(self, "your_server_type")
        
        super().__init__(name="æ‚¨çš„æœåŠ¡å™¨", port=port)
        
        # åˆå§‹åŒ–å¯åŠ¨ç¼“å­˜å¤„ç†å™¨
        self.startup_processor = YourStartupProcessor(self)
    
    def register_custom_routes(self):
        """æ³¨å†Œå¯åŠ¨ç¼“å­˜è·¯ç”±"""
        self.app.add_url_rule(
            '/api/chart-data/your_chart',
            'your_chart',
            self.startup_processor.process_your_startup_chart,
            methods=['GET']
        )
```

#### æ­¥éª¤4: æµ‹è¯•å¯åŠ¨ç¼“å­˜

å¯åŠ¨æœåŠ¡å™¨åï¼Œå¯åŠ¨ç¼“å­˜ä¼šè‡ªåŠ¨å·¥ä½œï¼š

```bash
# å¯åŠ¨æœåŠ¡å™¨
python your_server.py

# æŸ¥çœ‹ç¼“å­˜çŠ¶æ€
curl "http://localhost:5011/api/startup-cache/status"

# æµ‹è¯•APIï¼ˆç¬¬ä¸€æ¬¡ä¼šè®¡ç®—ï¼‰
curl "http://localhost:5011/api/chart-data/your_chart"

# å†æ¬¡æµ‹è¯•ï¼ˆä½¿ç”¨ç¼“å­˜ï¼Œå¾ˆå¿«ï¼‰
curl "http://localhost:5011/api/chart-data/your_chart"

# æ¸…é™¤ç¼“å­˜
curl -X POST "http://localhost:5011/api/startup-cache/clear"
```

### ğŸ”§ å¯åŠ¨ç¼“å­˜çš„é…ç½®é€‰é¡¹

```json
{
  "cache": {
    "strategy": "startup_once",           // ç¼“å­˜ç­–ç•¥
    "description": "è¯´æ˜æ–‡å­—",            // ç¼“å­˜è¯´æ˜
    "params": {                          // é¢„çƒ­æ—¶çš„å‚æ•°
      "type": "pie",
      "period": "1d"
    }
  }
}
```

### ğŸ¯ é€‚ç”¨åœºæ™¯

å¯åŠ¨ç¼“å­˜ç‰¹åˆ«é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š

1. **åŸºç¡€é…ç½®æ•°æ®**: ç³»ç»Ÿé…ç½®ã€å­—å…¸æ•°æ®ç­‰
2. **å†å²ç»Ÿè®¡åˆ†æ**: é•¿æœŸæ•°æ®çš„ç»Ÿè®¡åˆ†æ
3. **å¤æ‚è®¡ç®—ç»“æœ**: éœ€è¦å¤§é‡è®¡ç®—ä½†ç»“æœç¨³å®šçš„æ•°æ®
4. **å‚è€ƒæ•°æ®**: è¡Œä¸šåˆ†æã€åŸºå‡†æ•°æ®ç­‰

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | ä¼ ç»Ÿæ–¹å¼ | å¯åŠ¨ç¼“å­˜ | æ€§èƒ½æå‡ |
|------|---------|---------|---------|
| é¦–æ¬¡è®¿é—® | 3-5ç§’ | 3-5ç§’ | æ— å˜åŒ– |
| åç»­è®¿é—® | 3-5ç§’ | <50ms | **60-100å€** |
| æœåŠ¡å™¨é‡å¯ | é‡æ–°è®¡ç®— | è‡ªåŠ¨é¢„çƒ­ | ç”¨æˆ·æ— æ„ŸçŸ¥ |

### ğŸ” ç¼“å­˜çŠ¶æ€ç›‘æ§

æ¡†æ¶æä¾›äº†å®Œæ•´çš„ç¼“å­˜ç›‘æ§APIï¼š

```bash
# è·å–å¯åŠ¨ç¼“å­˜çŠ¶æ€
GET /api/startup-cache/status
{
  "status": "success",
  "data": {
    "cached_endpoints": 3,
    "cache_age_seconds": 1234,
    "cache_keys": ["startup:/api/chart-data/stock_basic_info", ...]
  }
}

# æ¸…é™¤å¯åŠ¨ç¼“å­˜
POST /api/startup-cache/clear
{
  "status": "success", 
  "cleared_count": 3,
  "message": "å·²æ¸…é™¤ 3 ä¸ªå¯åŠ¨ç¼“å­˜é¡¹"
}
```

### 1. æ¡ä»¶ç¼“å­˜

æ ¹æ®ä¸šåŠ¡é€»è¾‘å†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼š

```python
def process_realtime_data(self):
    """å®æ—¶æ•°æ®å¤„ç† - äº¤æ˜“æ—¶é—´ä¸ä½¿ç”¨ç¼“å­˜"""
    # åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´
    if self._is_trading_time():
        self.logger.info("äº¤æ˜“æ—¶é—´ï¼Œè·³è¿‡ç¼“å­˜ç¡®ä¿æ•°æ®å®æ—¶æ€§")
        return self._process_without_cache()
    
    # éäº¤æ˜“æ—¶é—´ä½¿ç”¨ç¼“å­˜
    return self._process_with_cache()

def _is_trading_time(self) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¶é—´"""
    from datetime import datetime, time
    now = datetime.now()
    trading_start = time(9, 30)
    trading_end = time(15, 0)
    return trading_start <= now.time() <= trading_end
```

### 2. å¤šçº§ç¼“å­˜

ç»„åˆä½¿ç”¨ä¸åŒç±»å‹çš„ç¼“å­˜ï¼š

```python
def process_complex_data(self):
    """å¤æ‚æ•°æ®å¤„ç† - å¤šçº§ç¼“å­˜ç¤ºä¾‹"""
    # ç¬¬ä¸€çº§ï¼šåŸå§‹æ•°æ®ç¼“å­˜
    raw_data = self.data_cache.load_data('raw_df')
    
    # ç¬¬äºŒçº§ï¼šå¤„ç†ç»“æœç¼“å­˜
    cache_key = f"processed_data_{hash(str(raw_data.values.tolist()))}"
    
    if cache_key in self.custom_cache:
        processed_data = self.custom_cache[cache_key]
    else:
        processed_data = self._expensive_calculation(raw_data)
        self.custom_cache[cache_key] = processed_data
    
    # ç¬¬ä¸‰çº§ï¼šå“åº”ç¼“å­˜
    return self._build_cached_response(processed_data)
```

### 3. ç¼“å­˜é¢„çƒ­

åœ¨æœåŠ¡å¯åŠ¨æ—¶é¢„å…ˆåŠ è½½å¸¸ç”¨æ•°æ®ï¼š

```python
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    
    # å¯åŠ¨ç¼“å­˜é¢„çƒ­çº¿ç¨‹
    import threading
    self.warm_thread = threading.Thread(target=self._warm_up_cache, daemon=True)
    self.warm_thread.start()

def _warm_up_cache(self):
    """é¢„çƒ­ç¼“å­˜"""
    time.sleep(5)  # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
    
    warm_up_tasks = [
        ('stock_df', None),
        ('plate_df', None),
        ('/api/table-data/stock_list', {'page': 1, 'pageSize': 20}),
        ('/api/chart-data/trend', {'period': '1d'})
    ]
    
    for task in warm_up_tasks:
        try:
            if len(task) == 2 and task[1] is None:
                # é¢„çƒ­æ•°æ®ç¼“å­˜
                self.data_cache.load_data(task[0])
            else:
                # é¢„çƒ­å“åº”ç¼“å­˜
                self._simulate_request(task[0], task[1])
        except Exception as e:
            self.logger.warning(f"é¢„çƒ­å¤±è´¥: {task[0]} - {e}")
```

### 4. ç¼“å­˜å¤±æ•ˆç­–ç•¥

ä¸»åŠ¨æ§åˆ¶ç¼“å­˜å¤±æ•ˆï¼š

```python
def update_data_source(self, data_key: str, new_data_path: str):
    """æ›´æ–°æ•°æ®æºå¹¶å¤±æ•ˆç›¸å…³ç¼“å­˜"""
    # æ›´æ–°æ–‡ä»¶è·¯å¾„
    self.data_cache.add_file_path(data_key, new_data_path)
    
    # ä¸»åŠ¨å¤±æ•ˆæ•°æ®ç¼“å­˜
    if data_key in self.data_cache.cache:
        del self.data_cache.cache[data_key]
        self.logger.info(f"å·²å¤±æ•ˆæ•°æ®ç¼“å­˜: {data_key}")
    
    # å¤±æ•ˆç›¸å…³çš„å“åº”ç¼“å­˜
    related_endpoints = self._get_related_endpoints(data_key)
    for endpoint in related_endpoints:
        self._clear_response_cache(endpoint)
        
def _get_related_endpoints(self, data_key: str) -> list:
    """è·å–ä¸æ•°æ®ç›¸å…³çš„APIç«¯ç‚¹"""
    endpoint_mapping = {
        'stock_df': ['/api/table-data/stock_list', '/api/chart-data/stock_trend'],
        'plate_df': ['/api/table-data/plate_list', '/api/chart-data/plate_trend'],
        'custom_table_df': ['/api/table-data/custom_table', '/api/table-summary/custom_table']
    }
    return endpoint_mapping.get(data_key, [])
```

## ğŸ”§ é…ç½®å‚è€ƒ

### ç¼“å­˜é…ç½®å‚æ•°

```python
# æœåŠ¡å™¨åˆå§‹åŒ–æ—¶çš„ç¼“å­˜é…ç½®
auto_update_config = {
    'enabled': True,           # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ›´æ–°
    'interval': 30,            # æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
    'components': ["table1"],  # å‚ä¸è‡ªåŠ¨æ›´æ–°çš„ç»„ä»¶
    'random_selection': True,  # æ˜¯å¦éšæœºé€‰æ‹©ç»„ä»¶æ›´æ–°
    'max_clients': 50,         # æœ€å¤§SSEå®¢æˆ·ç«¯æ•°
    'heartbeat_interval': 30   # å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰
}

# å“åº”ç¼“å­˜é…ç½®
response_cache_config = {
    'max_cache_size': 100,     # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
    'default_ttl': 300,        # é»˜è®¤TTLï¼ˆç§’ï¼‰
    'cleanup_interval': 600    # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
}
```

### æ•°æ®æ–‡ä»¶é…ç½®

```python
def get_data_cache_file_paths(self) -> dict:
    """
    é…ç½®æ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„
    
    æ”¯æŒçš„è·¯å¾„æ ¼å¼ï¼š
    - ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
    - ç»å¯¹è·¯å¾„ï¼šå®Œæ•´çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„
    - ç½‘ç»œè·¯å¾„ï¼šæ”¯æŒUNCè·¯å¾„ï¼ˆWindowsï¼‰
    """
    return {
        # ç›¸å¯¹è·¯å¾„ç¤ºä¾‹
        'local_data': 'data/local_file.csv',
        
        # ç»å¯¹è·¯å¾„ç¤ºä¾‹
        'system_data': '/var/data/system_file.csv',
        
        # Windowsç½‘ç»œè·¯å¾„ç¤ºä¾‹
        'network_data': r'\\server\share\data\network_file.csv',
        
        # æ”¯æŒä¸åŒæ–‡ä»¶æ ¼å¼
        'json_data': 'data/config.json',
        'excel_data': 'data/report.xlsx'
    }
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç¼“å­˜ç­–ç•¥é€‰æ‹©

| æ•°æ®ç±»å‹ | æ¨èç­–ç•¥ | åŸå›  |
|---------|---------|------|
| **é™æ€é…ç½®** | é•¿æœŸç¼“å­˜ | å˜åŒ–é¢‘ç‡ä½ï¼Œå¯ä»¥ç¼“å­˜æ•°å°æ—¶ |
| **å®æ—¶è‚¡ä»·** | çŸ­æœŸç¼“å­˜ | å˜åŒ–é¢‘ç¹ï¼Œç¼“å­˜5-30ç§’ |
| **å†å²æ•°æ®** | ä¸­æœŸç¼“å­˜ | ç›¸å¯¹ç¨³å®šï¼Œç¼“å­˜10-60åˆ†é’Ÿ |
| **ç”¨æˆ·è®¾ç½®** | ä¼šè¯ç¼“å­˜ | ä¸ªäººç›¸å…³ï¼ŒæŒ‰ä¼šè¯ç¼“å­˜ |

### 2. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# âœ… å¥½çš„å®è·µï¼šä½¿ç”¨å…·ä½“çš„æºæ•°æ®æ ‡è¯†
source_data = {
    'last_modified': file_timestamp,
    'data_count': len(df),
    'key_fields_hash': hash(str(df[['key_field']].values))
}

# âŒ é¿å…ï¼šä½¿ç”¨æ•´ä¸ªDataFrameä½œä¸ºæºæ•°æ®
source_data = df  # ä¼šå¯¼è‡´å“ˆå¸Œè®¡ç®—è€—æ—¶
```

### 3. å†…å­˜ç®¡ç†

```python
# é…ç½®åˆç†çš„ç¼“å­˜å¤§å°
self.response_cache = BaseResponseCache(max_cache_size=50)  # æ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´

# å®šæœŸç›‘æ§ç¼“å­˜çŠ¶æ€
def monitor_cache_usage(self):
    stats = self.response_cache.get_cache_stats()
    if stats['cache_size'] > stats['max_cache_size'] * 0.8:
        self.logger.warning("ç¼“å­˜ä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œè€ƒè™‘è°ƒæ•´ç­–ç•¥")
```

### 4. é”™è¯¯å¤„ç†

```python
def process_with_fallback(self):
    """å¸¦é™çº§æœºåˆ¶çš„ç¼“å­˜å¤„ç†"""
    try:
        # å°è¯•ä½¿ç”¨ç¼“å­˜
        return self._process_with_cache()
    except Exception as cache_error:
        self.logger.warning(f"ç¼“å­˜å¤„ç†å¤±è´¥ï¼Œé™çº§åˆ°ç›´æ¥å¤„ç†: {cache_error}")
        # é™çº§åˆ°ä¸ä½¿ç”¨ç¼“å­˜
        return self._process_without_cache()
```

### 5. è°ƒè¯•å’Œç›‘æ§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger('cache').setLevel(logging.DEBUG)

# æ·»åŠ æ€§èƒ½æŒ‡æ ‡
import time

def process_with_metrics(self):
    start_time = time.time()
    
    # å¤„ç†é€»è¾‘
    result = self._process_data()
    
    # è®°å½•æ€§èƒ½æŒ‡æ ‡
    duration = time.time() - start_time
    self.logger.info(f"å¤„ç†è€—æ—¶: {duration:.3f}ç§’")
    
    return result
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ç¼“å­˜æ²¡æœ‰ç”Ÿæ•ˆï¼Œæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ï¼Ÿ

**å¯èƒ½åŸå› **:
1. æºæ•°æ®æ ‡è¯†æ„å»ºä¸æ­£ç¡®
2. æ–‡ä»¶è·¯å¾„é…ç½®é”™è¯¯
3. æƒé™é—®é¢˜å¯¼è‡´æ— æ³•è¯»å–æ–‡ä»¶æ—¶é—´æˆ³

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ–‡ä»¶è·¯å¾„é…ç½®
print(f"é…ç½®çš„æ–‡ä»¶è·¯å¾„: {self.data_cache.file_paths}")

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
import os
for key, path in self.data_cache.file_paths.items():
    if os.path.exists(path):
        print(f"âœ… {key}: {path}")
    else:
        print(f"âŒ {key}: {path} - æ–‡ä»¶ä¸å­˜åœ¨")

# æ£€æŸ¥æºæ•°æ®æ ‡è¯†
source_data = {...}
print(f"æºæ•°æ®æ ‡è¯†: {source_data}")
```

### Q2: ç¼“å­˜å ç”¨å†…å­˜è¿‡å¤§ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´æœ€å¤§ç¼“å­˜å¤§å°
self.response_cache = BaseResponseCache(max_cache_size=30)  # å‡å°‘ç¼“å­˜æ¡ç›®

# 2. ç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µ
stats = self.response_cache.get_cache_stats()
print(f"å½“å‰ç¼“å­˜å¤§å°: {stats['cache_size']}")

# 3. ä¸»åŠ¨æ¸…ç†ç¼“å­˜
self.response_cache.clear_cache()
```

### Q3: æ•°æ®æ›´æ–°äº†ä½†ç¼“å­˜æ²¡æœ‰å¤±æ•ˆï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ç¡®ä¿æ–‡ä»¶æ—¶é—´æˆ³æ›´æ–°
import os
file_path = "data/your_data.csv"
os.utime(file_path, None)  # æ›´æ–°æ–‡ä»¶æ—¶é—´æˆ³

# 2. ä¸»åŠ¨å¤±æ•ˆç¼“å­˜
del self.data_cache.cache['your_data_key']
self.response_cache.clear_cache()

# 3. æ£€æŸ¥æ–‡ä»¶ç›‘æ§æ˜¯å¦æ­£å¸¸
timestamp = self.data_cache.get_file_timestamp(file_path)
print(f"æ–‡ä»¶æ—¶é—´æˆ³: {timestamp}")
```

### Q4: å¦‚ä½•è°ƒè¯•ç¼“å­˜å‘½ä¸­æƒ…å†µï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨å¤„ç†å™¨ä¸­æ·»åŠ è¯¦ç»†æ—¥å¿—
def process_data_with_debug(self):
    cache_endpoint = '/api/your-endpoint'
    source_data = {...}
    
    # æ£€æŸ¥ç¼“å­˜å‰è®°å½•çŠ¶æ€
    self.logger.info(f"æ£€æŸ¥ç¼“å­˜: {cache_endpoint}")
    self.logger.info(f"æºæ•°æ®: {source_data}")
    
    should_cache, cached_response = self.should_use_cache(
        cache_endpoint, None, source_data
    )
    
    if should_cache:
        self.logger.info("âœ… ç¼“å­˜å‘½ä¸­")
        return cached_response
    else:
        self.logger.info("âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œé‡æ–°è®¡ç®—")
        # ç»§ç»­å¤„ç†...
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [Flaskæ–‡æ¡£](https://flask.palletsprojects.com/)
- [pandasæ•°æ®å¤„ç†](https://pandas.pydata.org/docs/)
- [Vue.jsç»„ä»¶å¼€å‘](https://v3.vuejs.org/guide/)
- [data_panelå®Œæ•´ç¤ºä¾‹](../api/processors/demo_processor.py)

---

**ğŸ’¡ æç¤º**: å¦‚æœæ‚¨åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æˆ–è”ç³»å¼€å‘å›¢é˜Ÿè·å–æ”¯æŒã€‚

**ğŸ“ æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£ä¼šéšç€æ¡†æ¶æ›´æ–°è€ŒæŒç»­ç»´æŠ¤ï¼Œæœ€æ–°ç‰ˆæœ¬è¯·æŸ¥çœ‹é¡¹ç›®ä»“åº“ã€‚
