#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨è‚¡ç¥¨ä»ªè¡¨ç›˜æœåŠ¡å™¨åŸºç±»
Base Stock Dashboard Server - å¯é‡ç”¨çš„æœåŠ¡å™¨æ¡†æ¶

Author: chenlei
"""

from flask import Flask, jsonify, request, Response, send_from_directory
from flask_cors import CORS
import logging
import json
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly
import time
import threading
import queue
import hashlib
from datetime import datetime, timedelta
import random
import sys
import os
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple

# å¯¼å…¥å¯åŠ¨ç¼“å­˜
from startup_cache import StartupOnceCache


class BaseDataCache:
    """åŸºç¡€æ•°æ®ç¼“å­˜ç±» - ç”¨äºæ•°æ®æ–‡ä»¶çš„ç¼“å­˜ç®¡ç†"""
    
    def __init__(self, file_paths: Optional[Dict[str, str]] = None):
        self.cache = {}
        self.timestamps = {}
        self.file_paths = file_paths or {}
        
    def get_file_path(self, file_key: str) -> Optional[str]:
        """è·å–æ–‡ä»¶è·¯å¾„"""
        return self.file_paths.get(file_key)
        
    def get_file_timestamp(self, file_path: str) -> float:
        """è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æˆ³"""
        try:
            return os.path.getmtime(file_path)
        except OSError:
            return 0
            
    def should_reload(self, file_key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ–‡ä»¶"""
        file_path = self.get_file_path(file_key)
        if not file_path or not os.path.exists(file_path):
            return False
            
        current_timestamp = self.get_file_timestamp(file_path)
        cached_timestamp = self.timestamps.get(file_key, 0)
        
        return current_timestamp > cached_timestamp
        
    def load_data(self, key: str):
        """åŠ è½½æ•°æ®ï¼Œæ”¯æŒä»æ–‡ä»¶è·¯å¾„è‡ªåŠ¨åŠ è½½"""
        # å¦‚æœæœ‰é…ç½®çš„æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æ–‡ä»¶åŠ è½½é€»è¾‘
        if key in self.file_paths:
            return self._load_from_file(key)
        
        # å¦åˆ™è¿”å›ç¼“å­˜ä¸­çš„æ•°æ®
        return self.cache.get(key, pd.DataFrame())
    
    def _load_from_file(self, file_key: str):
        """ä»æ–‡ä»¶åŠ è½½æ•°æ®"""
        if file_key not in self.cache or self.should_reload(file_key):
            file_path = self.get_file_path(file_key)
            if not file_path or not os.path.exists(file_path):
                print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
                return pd.DataFrame()
                
            try:
                print(f"é‡æ–°åŠ è½½æ–‡ä»¶: {file_path}")
                df = pd.read_csv(file_path)
                self.cache[file_key] = df
                self.timestamps[file_key] = self.get_file_timestamp(file_path)
                return df.copy()
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                return pd.DataFrame()
        else:
            print(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {file_key}")
            return self.cache[file_key].copy()
    
    def update_data(self, key: str, data: Any):
        """æ›´æ–°ç¼“å­˜æ•°æ®"""
        self.cache[key] = data
        self.timestamps[key] = time.time()
        
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.timestamps.clear()
    
    def add_file_path(self, key: str, path: str):
        """æ·»åŠ æ–‡ä»¶è·¯å¾„æ˜ å°„"""
        self.file_paths[key] = path
    
    def update_file_paths(self, paths: Dict[str, str]):
        """æ‰¹é‡æ›´æ–°æ–‡ä»¶è·¯å¾„æ˜ å°„"""
        self.file_paths.update(paths)


class BaseResponseCache:
    """åŸºç¡€å“åº”ç¼“å­˜ç±» - ç”¨äºç¼“å­˜APIå“åº”æ•°æ®"""
    
    def __init__(self, max_cache_size=100):
        self.cache = {}  # å­˜å‚¨å®é™…å“åº”æ•°æ®
        self.hash_cache = {}  # å­˜å‚¨æ•°æ®å“ˆå¸Œå€¼
        self.access_times = {}  # å­˜å‚¨è®¿é—®æ—¶é—´ï¼Œç”¨äºLRUæ¸…ç†
        self.max_cache_size = max_cache_size
        
    def _generate_cache_key(self, endpoint: str, params: Optional[Dict] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if params:
            # å¯¹å‚æ•°è¿›è¡Œæ’åºï¼Œç¡®ä¿é”®çš„ä¸€è‡´æ€§
            param_str = json.dumps(params, sort_keys=True, default=str)
            return f"{endpoint}:{param_str}"
        return endpoint
        
    def _calculate_data_hash(self, data: Any) -> str:
        """è®¡ç®—æ•°æ®çš„å“ˆå¸Œå€¼"""
        try:
            # å°†æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²å†è®¡ç®—å“ˆå¸Œ
            data_str = json.dumps(data, sort_keys=True, default=str)
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()
        except Exception as e:
            print(f"è®¡ç®—å“ˆå¸Œå¤±è´¥: {e}")
            return str(time.time())  # å¦‚æœå“ˆå¸Œå¤±è´¥ï¼Œä½¿ç”¨æ—¶é—´æˆ³
            
    def _cleanup_cache(self):
        """æ¸…ç†ç¼“å­˜ï¼Œä¿æŒåœ¨æœ€å¤§å¤§å°é™åˆ¶å†…"""
        if len(self.cache) <= self.max_cache_size:
            return
            
        # æŒ‰è®¿é—®æ—¶é—´æ’åºï¼Œç§»é™¤æœ€æ—§çš„æ¡ç›®
        sorted_keys = sorted(self.access_times.items(), key=lambda x: x[1])
        keys_to_remove = [key for key, _ in sorted_keys[:len(sorted_keys) - self.max_cache_size + 10]]
        
        for key in keys_to_remove:
            self.cache.pop(key, None)
            self.hash_cache.pop(key, None)
            self.access_times.pop(key, None)
            
        print(f"ç¼“å­˜æ¸…ç†å®Œæˆï¼Œç§»é™¤ {len(keys_to_remove)} ä¸ªæ¡ç›®")
        
    def should_use_cache(self, endpoint: str, params: Optional[Dict] = None, current_data: Optional[Any] = None) -> Tuple[bool, Optional[Any]]:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼“å­˜çš„å“åº”"""
        cache_key = self._generate_cache_key(endpoint, params)
        
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›False
        if cache_key not in self.cache:
            return False, None
            
        # å¦‚æœæ²¡æœ‰æä¾›å½“å‰æ•°æ®ï¼Œæ— æ³•æ¯”è¾ƒï¼Œä½¿ç”¨ç¼“å­˜
        if current_data is None:
            self.access_times[cache_key] = time.time()
            return True, self.cache[cache_key]
            
        # è®¡ç®—å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
        current_hash = self._calculate_data_hash(current_data)
        cached_hash = self.hash_cache.get(cache_key)
        
        # æ¯”è¾ƒå“ˆå¸Œå€¼
        if current_hash == cached_hash:
            print(f"æ•°æ®æœªå˜åŒ–ï¼Œä½¿ç”¨ç¼“å­˜å“åº”: {endpoint}")
            self.access_times[cache_key] = time.time()
            return True, self.cache[cache_key]
        else:
            print(f"æ•°æ®å·²å˜åŒ–ï¼Œéœ€è¦é‡æ–°è®¡ç®—: {endpoint}")
            return False, None
            
    def store_response(self, endpoint: str, params: Optional[Dict] = None, 
                      source_data: Optional[Any] = None, response_data: Optional[Any] = None):
        """å­˜å‚¨å“åº”åˆ°ç¼“å­˜"""
        cache_key = self._generate_cache_key(endpoint, params)
        
        if source_data is not None:
            # è®¡ç®—æºæ•°æ®çš„å“ˆå¸Œå€¼
            data_hash = self._calculate_data_hash(source_data)
            self.hash_cache[cache_key] = data_hash
            
        if response_data is not None:
            # å­˜å‚¨å“åº”æ•°æ®
            self.cache[cache_key] = response_data
            self.access_times[cache_key] = time.time()
            
            # æ¸…ç†ç¼“å­˜
            self._cleanup_cache()
            
            print(f"å“åº”å·²ç¼“å­˜: {endpoint}")
            
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.cache.clear()
        self.hash_cache.clear()
        self.access_times.clear()
        
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "cache_size": len(self.cache),
            "hash_cache_size": len(self.hash_cache),
            "max_cache_size": self.max_cache_size,
            "oldest_access": min(self.access_times.values()) if self.access_times else None,
            "newest_access": max(self.access_times.values()) if self.access_times else None
        }


class BaseStockServer(ABC):
    """è‚¡ç¥¨ä»ªè¡¨ç›˜æœåŠ¡å™¨åŸºç±»"""
    
    def __init__(self, name: str = "è‚¡ç¥¨ä»ªè¡¨ç›˜æœåŠ¡", port: int = 5004, auto_update_config: Optional[Dict[str, Any]] = None):
        self.name = name
        self.port = port
        self.app = Flask(__name__)
        CORS(self.app)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # è‡ªåŠ¨æ›´æ–°é…ç½®
        self.auto_update_config = auto_update_config or {
            'enabled': True,           # æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ›´æ–°
            'interval': 30,            # æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
            'components': ["chart1", "chart2", "table1", "table2"],  # å‚ä¸è‡ªåŠ¨æ›´æ–°çš„ç»„ä»¶
            'random_selection': True,  # æ˜¯å¦éšæœºé€‰æ‹©ç»„ä»¶æ›´æ–°
            'max_clients': 50,         # æœ€å¤§SSEå®¢æˆ·ç«¯æ•°
            'heartbeat_interval': 30   # å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰
        }
        
        # åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
        self.data_cache = BaseDataCache(self.get_data_cache_file_paths())
        self.response_cache = BaseResponseCache()
        
        # åˆå§‹åŒ–å¯åŠ¨ç¼“å­˜
        self.startup_cache = StartupOnceCache()
        
        # SSEç›¸å…³
        self.sse_clients = []
        self.latest_update = {"componentId": None, "params": {}}
        
        # æ³¨å†Œé€šç”¨è·¯ç”±
        self._register_routes()
        
        # è®¡åˆ’å¯åŠ¨ç¼“å­˜é¢„çƒ­
        self._schedule_startup_cache_warmup()
        
        # å¯åŠ¨åå°æ›´æ–°çº¿ç¨‹ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
        if self.auto_update_config['enabled']:
            self.update_thread = threading.Thread(target=self._background_data_update, daemon=True)
            self.update_thread.start()
            self.logger.info(f"è‡ªåŠ¨æ›´æ–°çº¿ç¨‹å·²å¯åŠ¨ï¼Œé—´éš”: {self.auto_update_config['interval']}ç§’")
        else:
            self.logger.info("è‡ªåŠ¨æ›´æ–°åŠŸèƒ½å·²ç¦ç”¨")
    
    def get_data_cache_file_paths(self) -> Dict[str, str]:
        """è·å–æ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„é…ç½® - å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•"""
        return {}
    
    def _register_routes(self):
        """æ³¨å†Œæ‰€æœ‰è·¯ç”±"""
        # é€šç”¨è·¯ç”±
        self.app.add_url_rule('/health', 'health_check', self.health_check, methods=['GET'])
        self.app.add_url_rule('/api/system/info', 'get_system_info', self.get_system_info, methods=['GET'])
        self.app.add_url_rule('/api/dashboard-config', 'get_dashboard_config', self.get_dashboard_config, methods=['GET'])
        
        # æ•°æ®è·¯ç”±
        self.app.add_url_rule('/api/table-data/<data_type>', 'get_table_data', self.get_table_data, methods=['GET'])
        self.app.add_url_rule('/api/chart-data/<chart_type>', 'get_chart_data', self.get_chart_data, methods=['GET'])
        
        # SSEè·¯ç”±
        self.app.add_url_rule('/api/dashboard/update', 'update_dashboard', self.update_dashboard, methods=['POST'])
        self.app.add_url_rule('/api/dashboard/updates', 'dashboard_updates', self.dashboard_updates, methods=['GET'])
        
        # ç¼“å­˜ç®¡ç†è·¯ç”±
        self.app.add_url_rule('/api/cache/status', 'get_cache_status', self.get_cache_status, methods=['GET'])
        self.app.add_url_rule('/api/cache/clear', 'clear_cache', self.clear_cache, methods=['POST'])
        
        # å¯åŠ¨ç¼“å­˜ç®¡ç†è·¯ç”±
        if hasattr(self, 'startup_cache'):
            self.app.add_url_rule('/api/startup-cache/status', 'get_startup_cache_status', self.get_startup_cache_status, methods=['GET'])
            self.app.add_url_rule('/api/startup-cache/clear', 'clear_startup_cache', self.clear_startup_cache, methods=['POST'])
        
        # è‡ªåŠ¨æ›´æ–°é…ç½®è·¯ç”±
        self.app.add_url_rule('/api/auto-update/config', 'get_auto_update_config', self.get_auto_update_config, methods=['GET'])
        self.app.add_url_rule('/api/auto-update/config', 'update_auto_update_config', self.update_auto_update_config, methods=['PUT'])
        self.app.add_url_rule('/api/auto-update/status', 'get_auto_update_status', self.get_auto_update_status, methods=['GET'])
        self.app.add_url_rule('/api/auto-update/toggle', 'toggle_auto_update', self.toggle_auto_update, methods=['POST'])
        
        # é™æ€æ–‡ä»¶æœåŠ¡è·¯ç”±ï¼ˆé…ç½®ç®¡ç†ç•Œé¢ï¼‰
        self.app.add_url_rule('/config', 'config_page', self.serve_config_page, methods=['GET'])
        self.app.add_url_rule('/static/<path:filename>', 'static_files', self.serve_static_files, methods=['GET'])
        
        # å…è®¸å­ç±»æ³¨å†Œè‡ªå®šä¹‰è·¯ç”±
        self.register_custom_routes()
        
        # å…¼å®¹æ—§æ–¹æ³•å
        if hasattr(self, 'register_routes') and self.register_routes != self.register_custom_routes:
            self.register_routes()
    
    def _schedule_startup_cache_warmup(self):
        """è®¡åˆ’å¯åŠ¨ç¼“å­˜é¢„çƒ­"""
        import threading
        
        def warmup_startup_cache():
            time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿æ‰€æœ‰åˆå§‹åŒ–å®Œæˆ
            self._warmup_startup_cache()
        
        warmup_thread = threading.Thread(target=warmup_startup_cache, daemon=True)
        warmup_thread.start()
        self.logger.info("ğŸ“… å·²è®¡åˆ’å¯åŠ¨ç¼“å­˜é¢„çƒ­ (5ç§’åå¼€å§‹)")
    
    def trigger_startup_cache_warmup(self):
        """æ‰‹åŠ¨è§¦å‘å¯åŠ¨ç¼“å­˜é¢„çƒ­"""
        import threading
        
        def warmup_startup_cache():
            self._warmup_startup_cache()
        
        warmup_thread = threading.Thread(target=warmup_startup_cache, daemon=True)
        warmup_thread.start()
        self.logger.info("ğŸ”¥ æ‰‹åŠ¨è§¦å‘å¯åŠ¨ç¼“å­˜é¢„çƒ­")
    
    def _warmup_startup_cache(self):
        """é¢„çƒ­å¯åŠ¨ç¼“å­˜"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å‡†å¤‡å¥½è¿›è¡Œç¼“å­˜é¢„çƒ­
            if not hasattr(self, 'component_manager') or not self.component_manager:
                self.logger.info("ç»„ä»¶ç®¡ç†å™¨å°šæœªåˆå§‹åŒ–ï¼Œè·³è¿‡å¯åŠ¨ç¼“å­˜é¢„çƒ­")
                return
            
            # è·å–éœ€è¦å¯åŠ¨æ—¶ç¼“å­˜çš„ç«¯ç‚¹åˆ—è¡¨
            startup_endpoints = self._get_startup_cache_endpoints()
            
            if not startup_endpoints:
                self.logger.info("æ²¡æœ‰é…ç½®å¯åŠ¨ç¼“å­˜ç«¯ç‚¹")
                return
            
            for endpoint_config in startup_endpoints:
                try:
                    self._warmup_single_endpoint(endpoint_config)
                except Exception as e:
                    self.logger.warning(f"é¢„çƒ­ç«¯ç‚¹å¤±è´¥ {endpoint_config.get('endpoint', 'unknown')}: {e}")
            
            stats = self.startup_cache.get_startup_cache_stats()
            self.logger.info(f"ğŸš€ å¯åŠ¨ç¼“å­˜é¢„çƒ­å®Œæˆ: {stats['cached_endpoints']} ä¸ªç«¯ç‚¹")
            
        except Exception as e:
            self.logger.error(f"å¯åŠ¨ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")
    
    def _get_startup_cache_endpoints(self) -> list:
        """
        è·å–éœ€è¦å¯åŠ¨æ—¶ç¼“å­˜çš„ç«¯ç‚¹é…ç½®
        ä»ç»„ä»¶é…ç½®ä¸­è‡ªåŠ¨è¯»å–æ ‡è®°ä¸ºstartup_cacheçš„ç«¯ç‚¹
        """
        try:
            startup_endpoints = []
            
            # å°è¯•è·å–ç»„ä»¶é…ç½®
            if hasattr(self, 'component_manager') and self.component_manager:
                # ä»ç»„ä»¶ç®¡ç†å™¨è·å–é…ç½®
                components = getattr(self.component_manager, 'components', {})
                
                for component_id, component_config in components.items():
                    # æ£€æŸ¥ç»„ä»¶çš„ç¼“å­˜é…ç½®
                    extra_config = getattr(component_config, 'extra_config', {})
                    cache_config = extra_config.get('cache', {})
                    
                    # å¦‚æœé…ç½®äº†å¯åŠ¨ç¼“å­˜
                    if cache_config.get('strategy') == 'startup_once' or cache_config.get('type') == 'startup_once':
                        api_path = getattr(component_config, 'api_path', '')
                        if api_path:
                            endpoint_config = {
                                'endpoint': api_path,
                                'params': cache_config.get('params'),
                                'description': f"{component_id} - {getattr(component_config, 'title', 'æœªçŸ¥ç»„ä»¶')}"
                            }
                            startup_endpoints.append(endpoint_config)
                            self.logger.info(f"å‘ç°å¯åŠ¨ç¼“å­˜ç«¯ç‚¹: {api_path}")
            
            # å¦‚æœæ²¡æœ‰ç»„ä»¶ç®¡ç†å™¨ï¼Œå°è¯•ç›´æ¥è¯»å–é…ç½®æ–‡ä»¶
            elif hasattr(self, 'get_startup_cache_config'):
                startup_endpoints = self.get_startup_cache_config()
            
            # é»˜è®¤é…ç½®ï¼ˆå­ç±»å¯ä»¥é‡å†™ get_default_startup_cache_endpoints æ–¹æ³•ï¼‰
            elif hasattr(self, 'get_default_startup_cache_endpoints'):
                startup_endpoints = self.get_default_startup_cache_endpoints()
            
            return startup_endpoints
            
        except Exception as e:
            self.logger.warning(f"è·å–å¯åŠ¨ç¼“å­˜ç«¯ç‚¹é…ç½®å¤±è´¥: {e}")
            return []
    
    def get_default_startup_cache_endpoints(self) -> list:
        """
        è·å–é»˜è®¤çš„å¯åŠ¨ç¼“å­˜ç«¯ç‚¹é…ç½®
        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æä¾›é»˜è®¤é…ç½®
        """
        return []
    
    def _warmup_single_endpoint(self, endpoint_config: dict):
        """é¢„çƒ­å•ä¸ªç«¯ç‚¹"""
        endpoint = endpoint_config['endpoint']
        params = endpoint_config.get('params')
        
        self.logger.info(f"ğŸ”¥ é¢„çƒ­ç«¯ç‚¹: {endpoint}")
        
        # æ£€æŸ¥å¤„ç†å™¨ç®¡ç†å™¨æ˜¯å¦å·²åˆå§‹åŒ–
        if hasattr(self, 'processor_manager') and self.processor_manager is None:
            self.logger.warning(f"å¤„ç†å™¨ç®¡ç†å™¨å°šæœªåˆå§‹åŒ–ï¼Œè·³è¿‡é¢„çƒ­: {endpoint}")
            return
        
        # æ¨¡æ‹Ÿè¯·æ±‚ä»¥è§¦å‘ç¼“å­˜
        with self.app.test_client() as client:
            url = endpoint
            if params:
                # æ„å»ºæŸ¥è¯¢å‚æ•°
                if isinstance(params, dict):
                    query_string = '&'.join([f"{k}={v}" for k, v in params.items()])
                    url = f"{endpoint}?{query_string}"
            
            response = client.get(url)
            if response.status_code == 200:
                self.logger.info(f"âœ… é¢„çƒ­æˆåŠŸ: {endpoint}")
            else:
                self.logger.warning(f"âŒ é¢„çƒ­å¤±è´¥: {endpoint} - {response.status_code}")
    
    def register_custom_routes(self):
        """å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ³¨å†Œè‡ªå®šä¹‰è·¯ç”±"""
        pass
    
    @abstractmethod
    def get_dashboard_config(self) -> Dict[str, Any]:
        """è·å–ä»ªè¡¨ç›˜é…ç½® - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    @abstractmethod
    def get_data_sources(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æºé…ç½® - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_cache_observables(self) -> Dict[str, Dict[str, Any]]:
        """
        å®šä¹‰éœ€è¦è§‚å¯Ÿçš„æ•°æ®æºï¼Œç”¨äºé˜²é‡å¤ç¼“å­˜
        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ¥æŒ‡å®šå“ªäº›æ•°æ®éœ€è¦è¢«è§‚å¯Ÿ
        
        è¿”å›æ ¼å¼:
        {
            "endpoint_pattern": {
                "data_keys": ["data_key1", "data_key2"],  # éœ€è¦è§‚å¯Ÿçš„æ•°æ®é”®
                "params_keys": ["param1", "param2"],      # éœ€è¦è§‚å¯Ÿçš„å‚æ•°é”®
                "custom_hash_func": function,             # å¯é€‰ï¼šè‡ªå®šä¹‰å“ˆå¸Œå‡½æ•°
                "ttl": 300                                # å¯é€‰ï¼šç¼“å­˜TTLï¼ˆç§’ï¼‰
            }
        }
        """
        return {
            # é»˜è®¤è§‚å¯Ÿé…ç½®ï¼Œå­ç±»å¯ä»¥é‡å†™
            "table-data": {
                "data_keys": ["data_time", "data_count"],
                "params_keys": ["sector_name", "component_id"]
            },
            "chart-data": {
                "data_keys": ["data_time", "data_count"],
                "params_keys": ["sector_name", "component_id"]
            }
        }
    
    def with_cache_protection(self, endpoint: str, handler_func: callable, 
                             source_data_func: Optional[callable] = None,
                             params_extractor: Optional[callable] = None):
        """
        é€šç”¨çš„ç¼“å­˜ä¿æŠ¤è£…é¥°å™¨æ–¹æ³•
        
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„
            handler_func: å®é™…çš„å¤„ç†å‡½æ•°
            source_data_func: è·å–æºæ•°æ®çš„å‡½æ•°ï¼ˆç”¨äºå“ˆå¸Œæ¯”è¾ƒï¼‰
            params_extractor: ä»requestä¸­æå–å‚æ•°çš„å‡½æ•°
        """
        def wrapper(*args, **kwargs):
            try:
                # æå–å‚æ•°
                params = {}
                if params_extractor:
                    params = params_extractor()
                else:
                    # é»˜è®¤å‚æ•°æå–
                    if hasattr(request, 'args'):
                        params = dict(request.args)
                
                # è·å–æºæ•°æ®
                source_data = None
                if source_data_func:
                    source_data = source_data_func()
                
                # æ£€æŸ¥ç¼“å­˜
                should_cache, cached_response = self.response_cache.should_use_cache(
                    endpoint, params, source_data
                )
                
                if should_cache and cached_response:
                    self.logger.info(f"ä½¿ç”¨ç¼“å­˜å“åº”: {endpoint}")
                    return cached_response
                
                # æ‰§è¡Œå®é™…å¤„ç†
                self.logger.info(f"æ‰§è¡Œæ•°æ®å¤„ç†: {endpoint}")
                response_data = handler_func(*args, **kwargs)
                
                # å­˜å‚¨åˆ°ç¼“å­˜
                self.response_cache.store_response(endpoint, params, source_data, response_data)
                
                return response_data
                
            except Exception as e:
                self.logger.error(f"ç¼“å­˜ä¿æŠ¤è£…é¥°å™¨å¤±è´¥ {endpoint}: {e}")
                # å¤±è´¥æ—¶ç›´æ¥è°ƒç”¨åŸå‡½æ•°
                return handler_func(*args, **kwargs)
        
        return wrapper
    
    def get_cache_status(self):
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        try:
            cache_stats = self.response_cache.get_cache_stats()
            data_cache_info = {
                "data_cache_size": len(self.data_cache.cache),
                "data_timestamps": len(self.data_cache.timestamps)
            }
            
            return jsonify({
                "status": "success",
                "response_cache": cache_stats,
                "data_cache": data_cache_info,
                "server_info": {
                    "name": self.name,
                    "connected_clients": len(self.sse_clients)
                }
            })
        except Exception as e:
            self.logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def clear_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        try:
            # æ¸…ç†å“åº”ç¼“å­˜
            response_initial_size = len(self.response_cache.cache)
            self.response_cache.clear_cache()
            
            # æ¸…ç†æ•°æ®ç¼“å­˜
            data_initial_size = len(self.data_cache.cache)
            self.data_cache.clear_cache()
            
            message = f"ç¼“å­˜å·²æ¸…ç†ï¼Œç§»é™¤å“åº”ç¼“å­˜ {response_initial_size} ä¸ªæ¡ç›®ï¼Œæ•°æ®ç¼“å­˜ {data_initial_size} ä¸ªæ¡ç›®"
            self.logger.info(message)
            return jsonify({
                "status": "success", 
                "message": message
            })
        except Exception as e:
            self.logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    # === å¯åŠ¨ç¼“å­˜ç®¡ç†æ–¹æ³• ===
    
    def get_startup_cache_status(self):
        """è·å–å¯åŠ¨ç¼“å­˜çŠ¶æ€"""
        try:
            if not hasattr(self, 'startup_cache'):
                return jsonify({
                    "status": "error",
                    "message": "å¯åŠ¨ç¼“å­˜åŠŸèƒ½æœªå¯ç”¨"
                }), 404
            
            stats = self.startup_cache.get_startup_cache_stats()
            return jsonify({
                "status": "success",
                "data": stats,
                "message": "å¯åŠ¨ç¼“å­˜çŠ¶æ€è·å–æˆåŠŸ"
            })
        except Exception as e:
            self.logger.error(f"è·å–å¯åŠ¨ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            return jsonify({
                "status": "error",
                "error": str(e)
            }), 500
    
    def clear_startup_cache(self):
        """æ¸…é™¤å¯åŠ¨ç¼“å­˜"""
        try:
            if not hasattr(self, 'startup_cache'):
                return jsonify({
                    "status": "error",
                    "message": "å¯åŠ¨ç¼“å­˜åŠŸèƒ½æœªå¯ç”¨"
                }), 404
            
            cleared_count = self.startup_cache.clear_startup_cache()
            message = f"å·²æ¸…é™¤ {cleared_count} ä¸ªå¯åŠ¨ç¼“å­˜é¡¹"
            self.logger.info(message)
            
            return jsonify({
                "status": "success",
                "cleared_count": cleared_count,
                "message": message
            })
        except Exception as e:
            self.logger.error(f"æ¸…é™¤å¯åŠ¨ç¼“å­˜å¤±è´¥: {e}")
            return jsonify({
                "status": "error",
                "error": str(e)
            }), 500
    
    # === è‡ªåŠ¨æ›´æ–°é…ç½®ç®¡ç†æ–¹æ³• ===
    
    def get_auto_update_config(self):
        """è·å–è‡ªåŠ¨æ›´æ–°é…ç½®"""
        try:
            return jsonify({
                "status": "success",
                "config": self.auto_update_config.copy(),
                "server_info": {
                    "name": self.name,
                    "port": self.port,
                    "sse_clients": len(self.sse_clients)
                }
            })
        except Exception as e:
            self.logger.error(f"è·å–è‡ªåŠ¨æ›´æ–°é…ç½®å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def update_auto_update_config(self):
        """æ›´æ–°è‡ªåŠ¨æ›´æ–°é…ç½®"""
        try:
            data = request.get_json()
            if not data:
                return jsonify({"error": "ç¼ºå°‘é…ç½®æ•°æ®"}), 400
            
            old_enabled = self.auto_update_config['enabled']
            
            # æ›´æ–°é…ç½®
            for key, value in data.items():
                if key in self.auto_update_config:
                    self.auto_update_config[key] = value
            
            # å¦‚æœå¯ç”¨çŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡å¯æˆ–åœæ­¢åå°çº¿ç¨‹
            new_enabled = self.auto_update_config['enabled']
            if old_enabled != new_enabled:
                if new_enabled and not hasattr(self, 'update_thread') or not self.update_thread.is_alive():
                    # å¯åŠ¨æ–°çš„åå°çº¿ç¨‹
                    self.update_thread = threading.Thread(target=self._background_data_update, daemon=True)
                    self.update_thread.start()
                    self.logger.info("è‡ªåŠ¨æ›´æ–°çº¿ç¨‹å·²é‡æ–°å¯åŠ¨")
                elif not new_enabled:
                    self.logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼ˆç°æœ‰çº¿ç¨‹å°†åœ¨ä¸‹ä¸€æ¬¡æ£€æŸ¥æ—¶é€€å‡ºï¼‰")
            
            self.logger.info(f"è‡ªåŠ¨æ›´æ–°é…ç½®å·²æ›´æ–°: {data}")
            return jsonify({
                "status": "success",
                "message": "é…ç½®æ›´æ–°æˆåŠŸ",
                "config": self.auto_update_config.copy()
            })
        except Exception as e:
            self.logger.error(f"æ›´æ–°è‡ªåŠ¨æ›´æ–°é…ç½®å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def get_auto_update_status(self):
        """è·å–è‡ªåŠ¨æ›´æ–°çŠ¶æ€"""
        try:
            thread_alive = hasattr(self, 'update_thread') and self.update_thread.is_alive()
            
            # ä» ComponentManager è·å–ç»„ä»¶åˆ—è¡¨
            available_components = []
            if hasattr(self, 'component_manager'):
                available_components = list(self.component_manager.components.keys())
            
            return jsonify({
                "status": "success",
                "auto_update": {
                    "enabled": self.auto_update_config['enabled'],
                    "thread_running": thread_alive,
                    "interval": self.auto_update_config['interval'],
                    "components": available_components,
                    "sse_clients": len(self.sse_clients),
                    "max_clients": self.auto_update_config['max_clients']
                }
            })
        except Exception as e:
            self.logger.error(f"è·å–è‡ªåŠ¨æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def toggle_auto_update(self):
        """åˆ‡æ¢è‡ªåŠ¨æ›´æ–°å¼€å…³"""
        try:
            current_status = self.auto_update_config['enabled']
            new_status = not current_status
            
            self.auto_update_config['enabled'] = new_status
            
            if new_status and (not hasattr(self, 'update_thread') or not self.update_thread.is_alive()):
                # å¯åŠ¨åå°çº¿ç¨‹
                self.update_thread = threading.Thread(target=self._background_data_update, daemon=True)
                self.update_thread.start()
                message = "è‡ªåŠ¨æ›´æ–°å·²å¯ç”¨ï¼Œåå°çº¿ç¨‹å·²å¯åŠ¨"
            elif not new_status:
                message = "è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼ˆåå°çº¿ç¨‹å°†åœ¨ä¸‹ä¸€æ¬¡æ£€æŸ¥æ—¶é€€å‡ºï¼‰"
            else:
                message = f"è‡ªåŠ¨æ›´æ–°çŠ¶æ€å·²åˆ‡æ¢ä¸º: {'å¯ç”¨' if new_status else 'ç¦ç”¨'}"
                
            self.logger.info(message)
            return jsonify({
                "status": "success",
                "message": message,
                "enabled": new_status,
                "thread_running": hasattr(self, 'update_thread') and self.update_thread.is_alive()
            })
        except Exception as e:
            self.logger.error(f"åˆ‡æ¢è‡ªåŠ¨æ›´æ–°çŠ¶æ€å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def _cleanup_sse_clients(self):
        """æ¸…ç†æ— æ•ˆçš„SSEå®¢æˆ·ç«¯è¿æ¥"""
        try:
            initial_count = len(self.sse_clients)
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å®¢æˆ·ç«¯æœ‰æ•ˆæ€§æ£€æŸ¥é€»è¾‘
            # ç›®å‰ç®€å•åœ°å‡è®¾æ‰€æœ‰å®¢æˆ·ç«¯éƒ½æœ‰æ•ˆï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦pingæ£€æŸ¥
            
            # å¦‚æœå®¢æˆ·ç«¯æ•°é‡è¿‡å¤šï¼Œç§»é™¤ä¸€äº›è¾ƒæ—§çš„è¿æ¥
            if len(self.sse_clients) > self.auto_update_config['max_clients']:
                remove_count = len(self.sse_clients) - self.auto_update_config['max_clients']
                self.sse_clients = self.sse_clients[remove_count:]
                self.logger.info(f"æ¸…ç†äº† {remove_count} ä¸ªSSEå®¢æˆ·ç«¯è¿æ¥")
            
            return initial_count - len(self.sse_clients)
        except Exception as e:
            self.logger.error(f"æ¸…ç†SSEå®¢æˆ·ç«¯å¤±è´¥: {e}")
            return 0
    
    # === é™æ€æ–‡ä»¶æœåŠ¡æ–¹æ³• ===
    
    def serve_config_page(self):
        """æä¾›é…ç½®ç®¡ç†é¡µé¢"""
        try:
            static_dir = os.path.join(os.path.dirname(__file__), 'static')
            return send_from_directory(static_dir, 'config.html')
        except Exception as e:
            self.logger.error(f"æä¾›é…ç½®é¡µé¢å¤±è´¥: {e}")
            return jsonify({"error": "é…ç½®é¡µé¢ä¸å¯ç”¨"}), 404
    
    def serve_static_files(self, filename):
        """æä¾›é™æ€æ–‡ä»¶æœåŠ¡"""
        try:
            static_dir = os.path.join(os.path.dirname(__file__), 'static')
            return send_from_directory(static_dir, filename)
        except Exception as e:
            self.logger.error(f"æä¾›é™æ€æ–‡ä»¶å¤±è´¥: {e}")
            return jsonify({"error": "æ–‡ä»¶æœªæ‰¾åˆ°"}), 404
    
    # === é€šç”¨æ•°æ®ç”Ÿæˆæ–¹æ³• ===
    
    def generate_mock_stock_data(self, count: int = 20) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®"""
        stock_codes = [f"{1000 + i:04d}" for i in range(count)]
        stock_names = [
            "å¹³å®‰é“¶è¡Œ", "ä¸‡ç§‘A", "æ‹›å•†é“¶è¡Œ", "ä¸­å›½å¹³å®‰", "ç¾çš„é›†å›¢",
            "äº”ç²®æ¶²", "è´µå·èŒ…å°", "æ’ç‘åŒ»è¯", "è¿ˆç‘åŒ»ç–—", "å®å¾·æ—¶ä»£",
            "æ¯”äºšè¿ª", "æµ·åº·å¨è§†", "ç«‹è®¯ç²¾å¯†", "è¯æ˜åº·å¾·", "çˆ±å°”çœ¼ç§‘",
            "ä¸œæ–¹è´¢å¯Œ", "æµ·å°”æ™ºå®¶", "æ ¼åŠ›ç”µå™¨", "æ´‹æ²³è‚¡ä»½", "ä¸‰å®‰å…‰ç”µ"
        ]
        
        data = []
        for i in range(count):
            price = round(random.uniform(5, 150), 2)
            change_pct = round(random.uniform(-10, 10), 2)
            volume = random.randint(1000, 50000)
            
            data.append({
                "è‚¡ç¥¨ä»£ç ": stock_codes[i],
                "è‚¡ç¥¨åç§°": stock_names[i % len(stock_names)],
                "å½“å‰ä»·æ ¼": price,
                "æ¶¨è·Œå¹…": f"{change_pct:+.2f}%",
                "æˆäº¤é‡": volume,
                "å¸‚å€¼": round(price * volume / 100, 2)
            })
        
        return data
    
    def generate_mock_sector_data(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ¿å—æ•°æ®"""
        sectors = [
            "ç§‘æŠ€æ¿å—", "åŒ»è¯æ¿å—", "æ–°èƒ½æº", "é‡‘èæ¿å—", "æ¶ˆè´¹æ¿å—",
            "å†›å·¥æ¿å—", "åœ°äº§æ¿å—", "æ±½è½¦æ¿å—", "é’¢é“æ¿å—", "åŒ–å·¥æ¿å—"
        ]
        
        data = []
        for sector in sectors:
            change_pct = round(random.uniform(-5, 5), 2)
            data.append({
                "æ¿å—åç§°": sector,
                "æ¶¨è·Œå¹…": f"{change_pct:+.2f}%",
                "æˆäº¤é¢": round(random.uniform(100, 1000), 2),
                "é¢†æ¶¨è‚¡": random.choice(["è‚¡ç¥¨A", "è‚¡ç¥¨B", "è‚¡ç¥¨C"]),
                "æ´»è·ƒåº¦": random.choice(["é«˜", "ä¸­", "ä½"])
            })
        
        return data
    
    def generate_mock_time_series(self) -> Tuple[List[str], List[float]]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®"""
        base_time = datetime.now().replace(hour=9, minute=30, second=0, microsecond=0)
        times = []
        values = []
        
        current_time = base_time
        current_value = 100
        
        while current_time.hour < 15:
            times.append(current_time.strftime("%H:%M"))
            
            change = random.uniform(-2, 2)
            current_value += change
            values.append(round(current_value, 2))
            
            current_time += timedelta(minutes=5)
            
            if current_time.hour == 11 and current_time.minute == 30:
                current_time = current_time.replace(hour=13, minute=0)
        
        return times, values
    
    # === é€šç”¨å›¾è¡¨ç”Ÿæˆæ–¹æ³• ===
    
    def create_line_chart(self, x_data: List, y_data: List, title: str = "çº¿æ€§å›¾", 
                         x_title: str = "Xè½´", y_title: str = "Yè½´") -> str:
        """åˆ›å»ºçº¿æ€§å›¾"""
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=x_data,
            y=y_data,
            mode='lines+markers',
            name=title,
            line=dict(color='#2196F3', width=2)
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title=x_title,
            yaxis_title=y_title,
            template='plotly_white',
            height=300
        )
        
        return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)
    
    def create_bar_chart(self, x_data: List, y_data: List, title: str = "æŸ±çŠ¶å›¾",
                        x_title: str = "Xè½´", y_title: str = "Yè½´", colors: Optional[List] = None) -> str:
        """åˆ›å»ºæŸ±çŠ¶å›¾"""
        fig = go.Figure()
        fig.add_trace(go.Bar(
            x=x_data,
            y=y_data,
            marker_color=colors or 'lightblue',
            name=title
        ))
        
        fig.update_layout(
            title=title,
            xaxis_title=x_title,
            yaxis_title=y_title,
            template='plotly_white',
            height=300
        )
        
        return json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)
    
    # === é€šç”¨è·¯ç”±å¤„ç†æ–¹æ³• ===
    
    def get_table_data(self, data_type: str):
        """è·å–è¡¨æ ¼æ•°æ® - æ”¯æŒç¼“å­˜é˜²é‡å¤æœºåˆ¶"""
        try:
            data_sources = self.get_data_sources()
            
            # æ£€æŸ¥æ•°æ®æºé…ç½®ï¼ˆæ”¯æŒå®Œæ•´è·¯å¾„åŒ¹é…ï¼‰
            table_path = f"/api/table-data/{data_type}"
            if table_path in data_sources:
                config = data_sources[table_path]
                
                # å¦‚æœé…ç½®ä¸­æœ‰ handlerï¼Œå°è¯•è°ƒç”¨å¯¹åº”çš„æ–¹æ³•
                if isinstance(config, dict) and 'handler' in config:
                    handler_name = config['handler']
                    if hasattr(self, handler_name):
                        handler_method = getattr(self, handler_name)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¼“å­˜ä¿æŠ¤
                        cache_ttl = config.get('cache_ttl', None)
                        if cache_ttl is not None and cache_ttl > 0:
                            # ä½¿ç”¨ç¼“å­˜ä¿æŠ¤æœºåˆ¶
                            def source_data_extractor():
                                # å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ¥æä¾›æ›´ç²¾ç¡®çš„æºæ•°æ®
                                return self._get_source_data_for_endpoint(table_path)
                                
                            protected_handler = self.with_cache_protection(
                                table_path, 
                                handler_method,
                                source_data_extractor
                            )
                            return protected_handler()
                        else:
                            # ç›´æ¥è°ƒç”¨handler
                            return handler_method()
                    else:
                        self.logger.warning(f"Handleræ–¹æ³• {handler_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†")
                else:
                    # å¦‚æœæ²¡æœ‰handlerï¼Œç›´æ¥è¿”å›é…ç½®æ•°æ®
                    return jsonify(config)
            
            # å…¼å®¹æ—§çš„é…ç½®æ ¼å¼ (tables å­—å…¸)
            if data_type in data_sources.get('tables', {}):
                config = data_sources['tables'][data_type]
                if isinstance(config, dict) and 'handler' in config:
                    handler_name = config['handler']
                    if hasattr(self, handler_name):
                        handler_method = getattr(self, handler_name)
                        return handler_method()
                return jsonify(config)
            
            # é»˜è®¤å¤„ç†
            return self._handle_default_table_data(data_type)
            
        except Exception as e:
            self.logger.error(f"è·å–è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def _handle_default_table_data(self, data_type: str):
        """å¤„ç†é»˜è®¤çš„è¡¨æ ¼æ•°æ®"""
        if data_type == "stock-list":
            stock_data = self.generate_mock_stock_data(20)
            return jsonify({
                "columns": ["è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°", "å½“å‰ä»·æ ¼", "æ¶¨è·Œå¹…", "æˆäº¤é‡", "å¸‚å€¼"],
                "data": [[item[col] for col in ["è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°", "å½“å‰ä»·æ ¼", "æ¶¨è·Œå¹…", "æˆäº¤é‡", "å¸‚å€¼"]] 
                        for item in stock_data]
            })
        elif data_type == "sector-list":
            sector_data = self.generate_mock_sector_data()
            return jsonify({
                "columns": ["æ¿å—åç§°", "æ¶¨è·Œå¹…", "æˆäº¤é¢", "é¢†æ¶¨è‚¡", "æ´»è·ƒåº¦"],
                "data": [[item[col] for col in ["æ¿å—åç§°", "æ¶¨è·Œå¹…", "æˆäº¤é¢", "é¢†æ¶¨è‚¡", "æ´»è·ƒåº¦"]] 
                        for item in sector_data]
            })
        
        return jsonify({"columns": [], "data": []})
    
    def _get_source_data_for_endpoint(self, endpoint: str) -> Dict[str, Any]:
        """
        è·å–ç«¯ç‚¹çš„æºæ•°æ®ï¼Œç”¨äºç¼“å­˜å“ˆå¸Œæ¯”è¾ƒ
        å­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•æ¥æä¾›æ›´ç²¾ç¡®çš„æºæ•°æ®
        """
        # é»˜è®¤å®ç°ï¼šè¿”å›åŸºæœ¬ä¿¡æ¯
        return {
            "timestamp": time.time(),
            "endpoint": endpoint,
            "data_cache_timestamps": self.data_cache.timestamps.copy()
        }
    
    def get_chart_data(self, chart_type: str):
        """è·å–å›¾è¡¨æ•°æ® - å¯è¢«å­ç±»é‡å†™"""
        try:
            data_sources = self.get_data_sources()
            
            # æ£€æŸ¥æ•°æ®æºé…ç½®ï¼ˆæ”¯æŒå®Œæ•´è·¯å¾„åŒ¹é…ï¼‰
            chart_path = f"/api/chart-data/{chart_type}"
            if chart_path in data_sources:
                config = data_sources[chart_path]
                
                # å¦‚æœé…ç½®ä¸­æœ‰ handlerï¼Œå°è¯•è°ƒç”¨å¯¹åº”çš„æ–¹æ³•
                if isinstance(config, dict) and 'handler' in config:
                    handler_name = config['handler']
                    if hasattr(self, handler_name):
                        handler_method = getattr(self, handler_name)
                        return handler_method()
                    else:
                        self.logger.warning(f"Handleræ–¹æ³• {handler_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†")
                else:
                    # å¦‚æœæ²¡æœ‰handlerï¼Œç›´æ¥è¿”å›é…ç½®æ•°æ®
                    return config
            
            # å…¼å®¹æ—§çš„é…ç½®æ ¼å¼ (charts å­—å…¸)
            if chart_type in data_sources.get('charts', {}):
                config = data_sources['charts'][chart_type]
                if isinstance(config, dict) and 'handler' in config:
                    handler_name = config['handler']
                    if hasattr(self, handler_name):
                        handler_method = getattr(self, handler_name)
                        return handler_method()
                return config
            
            # é»˜è®¤å¤„ç†
            if chart_type == "stock-trend":
                times, values = self.generate_mock_time_series()
                return self.create_line_chart(times, values, 'è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿', 'æ—¶é—´', 'ä»·æ ¼ (å…ƒ)')
            
            elif chart_type == "sector-performance":
                sector_data = self.generate_mock_sector_data()
                sector_names = [item["æ¿å—åç§°"] for item in sector_data]
                sector_changes = [float(item["æ¶¨è·Œå¹…"].replace("%", "").replace("+", "")) for item in sector_data]
                colors = ['red' if x < 0 else 'green' for x in sector_changes]
                return self.create_bar_chart(sector_names, sector_changes, 'æ¿å—è¡¨ç°', 'æ¿å—', 'æ¶¨è·Œå¹… (%)', colors)
            
            elif chart_type == "volume-analysis":
                stock_data = self.generate_mock_stock_data(10)
                stock_names = [item["è‚¡ç¥¨åç§°"] for item in stock_data]
                volumes = [item["æˆäº¤é‡"] for item in stock_data]
                return self.create_bar_chart(stock_names, volumes, 'æˆäº¤é‡åˆ†æ', 'è‚¡ç¥¨', 'æˆäº¤é‡')
            
            return jsonify({"error": "æœªçŸ¥å›¾è¡¨ç±»å‹"})
            
        except Exception as e:
            self.logger.error(f"è·å–å›¾è¡¨æ•°æ®å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    # === SSEç›¸å…³æ–¹æ³• ===
    
    def send_update_to_clients(self, data: Dict[str, Any]):
        """å‘æ‰€æœ‰å®¢æˆ·ç«¯å‘é€æ›´æ–°"""
        message = f"data: {json.dumps(data)}\n\n"
        clients_to_remove = []
        
        for client in list(self.sse_clients):
            try:
                client.put(message)
            except Exception as e:
                self.logger.error(f"å‘é€æ›´æ–°å¤±è´¥: {e}")
                clients_to_remove.append(client)
        
        for client in clients_to_remove:
            try:
                self.sse_clients.remove(client)
            except ValueError:
                pass
    
    def update_dashboard(self):
        """æ¥æ”¶ä»ªè¡¨ç›˜æ›´æ–°è¯·æ±‚"""
        try:
            data = request.json
            component_id = data.get('componentId')
            params = data.get('params', {})
            
            self.logger.info(f"æ¥æ”¶åˆ°æ›´æ–°è¯·æ±‚: componentId={component_id}, params={params}")
            
            self.latest_update = {
                "componentId": component_id,
                "params": params,
                "timestamp": int(time.time() * 1000)
            }
            
            self.send_update_to_clients(self.latest_update)
            
            return jsonify({"status": "success", "message": "æ›´æ–°å·²å‘é€"})
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ›´æ–°è¯·æ±‚å¤±è´¥: {e}")
            return jsonify({"error": str(e)}), 500
    
    def dashboard_updates(self):
        """SSEç«¯ç‚¹ï¼Œå‘å‰ç«¯æ¨é€å®æ—¶æ›´æ–°"""
        def event_stream():
            client_queue = queue.Queue()
            self.sse_clients.append(client_queue)
            
            try:
                if self.latest_update["componentId"]:
                    yield f"data: {json.dumps(self.latest_update)}\n\n"
                
                while True:
                    try:
                        message = client_queue.get(block=True, timeout=30)
                        yield message
                    except queue.Empty:
                        yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': int(time.time() * 1000)})}\n\n"
                        
            except Exception as e:
                self.logger.error(f"SSEè¿æ¥é”™è¯¯: {e}")
            finally:
                try:
                    self.sse_clients.remove(client_queue)
                except ValueError:
                    pass
        
        return Response(event_stream(), mimetype='text/event-stream')
    
    def health_check(self):
        """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        return jsonify({
            "status": "healthy",
            "service": self.name,
            "version": "1.0.0",
            "timestamp": int(time.time() * 1000),
            "connected_clients": len(self.sse_clients)
        })
    
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        return jsonify({
            "name": self.name,
            "version": "1.0.0",
            "description": "åŸºäºé€šç”¨æ¡†æ¶çš„è‚¡ç¥¨ä»ªè¡¨ç›˜æœåŠ¡",
            "features": [
                "å®æ—¶æ•°æ®æ¨¡æ‹Ÿ",
                "å¤šç§å›¾è¡¨ç±»å‹", 
                "äº¤äº’å¼ä»ªè¡¨ç›˜",
                "SSEå®æ—¶æ›´æ–°"
            ],
            "endpoints": {
                "dashboard-config": "/api/dashboard-config",
                "table-data": "/api/table-data/<data_type>",
                "chart-data": "/api/chart-data/<chart_type>",
                "dashboard-updates": "/api/dashboard/updates",
                "health": "/health"
            }
        })
    
    def _background_data_update(self):
        """åå°å®šæœŸæ›´æ–°æ•°æ®"""
        if not self.auto_update_config['enabled']:
            self.logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œåå°æ›´æ–°çº¿ç¨‹é€€å‡º")
            return
            
        self.logger.info("è‡ªåŠ¨æ›´æ–°åå°çº¿ç¨‹å¼€å§‹è¿è¡Œ")
        
        while True:
            try:
                # ä½¿ç”¨é…ç½®çš„æ›´æ–°é—´éš”
                time.sleep(self.auto_update_config['interval'])
                
                # æ£€æŸ¥æ˜¯å¦æœ‰SSEå®¢æˆ·ç«¯è¿æ¥
                if not self.sse_clients:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å®¢æˆ·ç«¯æ•°é™åˆ¶
                if len(self.sse_clients) > self.auto_update_config['max_clients']:
                    self.logger.warning(f"SSEå®¢æˆ·ç«¯æ•°é‡ ({len(self.sse_clients)}) è¶…è¿‡é™åˆ¶ ({self.auto_update_config['max_clients']})")
                    # æ¸…ç†æ— æ•ˆè¿æ¥
                    self._cleanup_sse_clients()
                
                # è·å–å‚ä¸è‡ªåŠ¨æ›´æ–°çš„ç»„ä»¶åˆ—è¡¨
                components = []
                if hasattr(self, 'component_manager'):
                    # ä» ComponentManager è·å–å¯ç”¨çš„ç»„ä»¶
                    components = [comp_id for comp_id, comp_config in self.component_manager.components.items() 
                                if comp_config.extra_config.get('enabled', True)]
                
                if not components:
                    self.logger.warning("è‡ªåŠ¨æ›´æ–°ç»„ä»¶åˆ—è¡¨ä¸ºç©º")
                    continue
                
                # æ ¹æ®é…ç½®é€‰æ‹©ç»„ä»¶
                if self.auto_update_config['random_selection']:
                    selected_component = random.choice(components)
                else:
                    # æŒ‰é¡ºåºé€‰æ‹©ï¼ˆå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é€»è¾‘ï¼‰
                    current_time = int(time.time())
                    index = (current_time // self.auto_update_config['interval']) % len(components)
                    selected_component = components[index]
                
                update_data = {
                    "componentId": selected_component,
                    "params": {"auto_refresh": True},
                    "timestamp": int(time.time() * 1000),
                    "type": "auto_update",
                    "server_config": {
                        "interval": self.auto_update_config['interval'],
                        "random_mode": self.auto_update_config['random_selection']
                    }
                }
                
                # å‘é€æ›´æ–°åˆ°å®¢æˆ·ç«¯
                if self.sse_clients:
                    self.send_update_to_clients(update_data)
                    self.logger.info(f"è‡ªåŠ¨æ›´æ–°æ¨é€: {selected_component} (å®¢æˆ·ç«¯æ•°: {len(self.sse_clients)})")
                    
            except Exception as e:
                self.logger.error(f"è‡ªåŠ¨æ›´æ–°çº¿ç¨‹å¼‚å¸¸: {e}")
                # å‡ºç°å¼‚å¸¸æ—¶ç­‰å¾…ä¸€æ®µæ—¶é—´å†ç»§ç»­ï¼Œé¿å…å¿«é€Ÿå¤±è´¥å¾ªç¯
                time.sleep(min(self.auto_update_config['interval'], 10))
    
    def _find_available_port(self, start_port: int) -> int:
        """æŸ¥æ‰¾å¯ç”¨ç«¯å£"""
        import socket
        for port in range(start_port, start_port + 100):
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.bind(('localhost', port))
                sock.close()
                return port
            except OSError:
                continue
        raise RuntimeError("æ— æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£")
    
    def run(self, debug: bool = True, host: str = '0.0.0.0'):
        """å¯åŠ¨æœåŠ¡å™¨"""
        self.logger.info(f"å¯åŠ¨{self.name}ï¼Œç«¯å£: {self.port}")
        
        try:
            # æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨
            import socket
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex((host, self.port))
            sock.close()
            
            if result == 0:
                self.logger.warning(f"ç«¯å£ {self.port} å·²è¢«å ç”¨ï¼Œå°è¯•è‡ªåŠ¨å¯»æ‰¾å¯ç”¨ç«¯å£")
                self.port = self._find_available_port(self.port)
                self.logger.info(f"ä½¿ç”¨æ–°ç«¯å£: {self.port}")
            
            # é…ç½®Flaskåº”ç”¨ä»¥å‡å°‘é‡è½½é—®é¢˜
            self.app.config.update(
                ENV='development' if debug else 'production',
                DEBUG=debug,
                TESTING=False,
                PROPAGATE_EXCEPTIONS=True
            )
            
            # å¯åŠ¨æœåŠ¡å™¨
            self.app.run(
                debug=debug, 
                host=host, 
                port=self.port,
                use_reloader=False,  # ç¦ç”¨è‡ªåŠ¨é‡è½½ä»¥é¿å…å¥—æ¥å­—é—®é¢˜
                threaded=True
            )
            
        except KeyboardInterrupt:
            self.logger.info("æœåŠ¡å™¨è¢«ç”¨æˆ·ä¸­æ–­")
        except OSError as e:
            if hasattr(e, 'winerror') and e.winerror == 10038:  # WinError 10038
                self.logger.error("å¥—æ¥å­—é”™è¯¯ï¼Œå¯èƒ½æ˜¯ç«¯å£å†²çªæˆ–é‡è½½é—®é¢˜")
                self.logger.info("å»ºè®®é‡å¯ç¨‹åºæˆ–ä½¿ç”¨ä¸åŒç«¯å£")
            else:
                self.logger.error(f"ç½‘ç»œé”™è¯¯: {e}")
            sys.exit(3)
        except Exception as e:
            self.logger.error(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
            sys.exit(1)


def parse_command_line_args() -> int:
    """è§£æå‘½ä»¤è¡Œå‚æ•°è·å–ç«¯å£"""
    port = 5004  # é»˜è®¤ç«¯å£
    
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            print("âŒ ç«¯å£å‚æ•°å¿…é¡»æ˜¯æ•°å­—")
            sys.exit(1)
    
    # ä»ç¯å¢ƒå˜é‡è·å–ç«¯å£
    if 'SERVER_PORT' in os.environ:
        try:
            port = int(os.environ['SERVER_PORT'])
        except ValueError:
            pass
    
    return port
