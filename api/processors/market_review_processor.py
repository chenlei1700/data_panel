"""
market_reviewå¤„ç†å™¨æ¨¡æ¿
Author: Auto-generated
Date: 2025-07-26
Description: å¤ç›˜é¡µé¢
"""
import os
from stock_data.factor.index.daily import FactorIndexDailyData
from stock_data.kpl.up_limit import KplUpLimitData
from stock_data.sentiment.market.daily import MarketSentimentDailyData
from stock_data.stock.index_daily import IndexDailyData
from stock_data.stock.stock_daily import StockDailyData
from stock_data.stock_minute import StockMinuteData
from stock_data.ths.concept_data import ThsConceptData
from stock_data.ths.concept_index import ThsConceptIndexData
from utils.common import get_trade_date_by_offset, get_trade_date_list
from .base_processor import BaseDataProcessor
from flask import jsonify, request
import pandas as pd
import numpy as np
import time


class MarketReviewProcessor(BaseDataProcessor):
    """å¤ç›˜é¡µé¢æ•°æ®å¤„ç†å™¨"""
    
    def process_market_sentiment_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_sentiment_daily', self._original_market_sentiment_daily)
    
    def _original_market_sentiment_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = FactorIndexDailyData()
        df = d.get_daily_data(start_date='2025-03-01')
        # æˆäº¤é¢amountå˜ä¸ºä»¥äº¿ä¸ºå•ä½ï¼Œå¹¶ä¿ç•™2ä½å°æ•°
        df['amount'] = df['amount'] / 1e8
        df['amount'] = df['amount'].round(2)
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        market_list = ['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿', 'ST','å…¨å¸‚åœº']
        chart_data = []
        for market in market_list:
            market_data = df[df['name'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æˆäº¤é¢',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['amount'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„å¸‚åœºæˆäº¤é¢",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æˆäº¤é¢(äº¿å…ƒ)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })
    
    def process_market_change_daily(self):
        """å„å¸‚åœºæ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_change_daily', self._original_market_change_daily)
    
    def _original_market_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = FactorIndexDailyData()
        df = d.get_daily_data(start_date='2025-03-01')
        # æˆäº¤é¢amountå˜ä¸ºä»¥äº¿ä¸ºå•ä½ï¼Œå¹¶ä¿ç•™2ä½å°æ•°
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        market_list = ['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ST','å…¨å¸‚åœº']
        # å¯¹nameåˆ—groupbyï¼Œè®¡ç®—changeåˆ—çš„cumsum
        df['change'] = df.groupby('name')['change'].cumsum()
        chart_data = []
        for market in market_list:
            market_data = df[df['name'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æ¶¨å¹…',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['change'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„å¸‚åœºæˆäº¤é¢",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æˆäº¤é¢(äº¿å…ƒ)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })
    
    def process_plate_stocks_change_daily(self):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_stocks_change_daily', self._original_plate_stocks_change_daily)
    
    def _original_plate_stocks_change_daily(self):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹…"""
        d = ThsConceptIndexData()
        df = d.get_daily_data(start_date='2025-07-01')

        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        #è·å–æœ€æ–°æ—¥æœŸ
        latest_date = df['trade_date'].max()
        latest_date_str = latest_date.strftime('%Y%m%d')
        start_date_str = get_trade_date_by_offset(latest_date_str, 20)
        # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹ä»¥ä¾¿æ¯”è¾ƒ
        start_date = pd.to_datetime(start_date_str, format='%Y%m%d')
        # è·å–ä»start_dateåˆ°latest_dateçš„æ‰€æœ‰æ•°æ®
        df = df[(df['trade_date'] >= start_date) & (df['trade_date'] <= latest_date)]

        
        #è·å–è¿‘5æ—¥ï¼Œè¿‘10æ—¥ï¼Œè¿‘20æ—¥çš„cumsumå„æ’åå‰10çš„æ¿å—åˆ—è¡¨
        ranking_results = self.get_sector_rankings_by_period(df, latest_date_str, [5, 10, 20], 10)
        

        # è·å–ranking_resultsçš„åˆ—è¡¨ï¼Œå¹¶åœ¨dfä¸­ç­›é€‰å‡ºè¿™äº›æ¿å—
        sector_names = [item['sector_name'] for sublist in ranking_results.values() for item in sublist]
        # å»é™¤é‡å¤çš„æ¿å—åç§°
        sector_names = list(set(sector_names))
        
        # è·å–æ—¥çº¿è‚¡ç¥¨æ•°æ®
        d = StockDailyData()
        stock_df = d.get_daily_data(start_date='2025-07-01')
        stock_df['trade_date'] = pd.to_datetime(stock_df['trade_date'], errors='coerce')
        # è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨
        d = ThsConceptData()
        concept_df = d.get_daily_data(start_date='2025-07-01')
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        latest_date = concept_df['trade_date'].max()
        concept_df = concept_df[concept_df['trade_date'] == latest_date]
        sector_name=sector_names[0]
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = concept_df[concept_df['concept_name'] == sector_name]['stocks'].values[0].split(',')
        # å¹¶å˜æˆæ•´æ•°å‹
        stock_list = [int(stock) for stock in stock_list if stock.isdigit()]
        # è·å–stock_dfä¸­idåœ¨stock_listä¸­çš„æ•°æ®
        stock_df = stock_df[stock_df['id'].isin(stock_list)]
        # groupby idï¼Œè®¡ç®—changeåˆ—çš„cumsum
        stock_df['change'] = stock_df.groupby('id')['change'].cumsum()
        stock_df['date_str'] = stock_df['trade_date'].dt.strftime('%m/%d')
        chart_data = []
        for stock_id in stock_list:
            stock_data = stock_df[stock_df['id'] == stock_id]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='trade_date')
                # è·å–è‚¡ç¥¨åç§° - å–ç¬¬ä¸€è¡Œçš„å€¼
                stock_name = stock_data['stock_name'].iloc[0]
                
                x_data = stock_data['date_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                chart_data.append({
                    "name": f'{stock_name}â€”â€”{sector_name}',
                    "x": x_data,
                    "y": y_data,
                    "mode": "lines+markers+text",  # æ·»åŠ  +text æ¨¡å¼
                    "line": {"width": 2},
                    "text": [f'{stock_name}â€”â€”{sector_name}' if i == len(x_data)-1 else '' for i in range(len(x_data))],
                    "textposition": "middle right",
                    "textfont": {"size": 25, "color": "black"},
                    "showlegend": True
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—å†…è‚¡ç¥¨æ¶¨å¹…",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æˆäº¤é¢(äº¿å…ƒ)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })
    
    def process_plate_change_daily(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_change_daily', self._original_plate_change_daily)

    def _original_plate_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = ThsConceptIndexData()
        df = d.get_daily_data(start_date='2025-07-01')

        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        #è·å–æœ€æ–°æ—¥æœŸ
        latest_date = df['trade_date'].max()
        latest_date_str = latest_date.strftime('%Y%m%d')
        start_date_str = get_trade_date_by_offset(latest_date_str, 20)
        # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹ä»¥ä¾¿æ¯”è¾ƒ
        start_date = pd.to_datetime(start_date_str, format='%Y%m%d')
        # è·å–ä»start_dateåˆ°latest_dateçš„æ‰€æœ‰æ•°æ®
        df = df[(df['trade_date'] >= start_date) & (df['trade_date'] <= latest_date)]

        
        #è·å–è¿‘5æ—¥ï¼Œè¿‘10æ—¥ï¼Œè¿‘20æ—¥çš„cumsumå„æ’åå‰10çš„æ¿å—åˆ—è¡¨
        ranking_results = self.get_sector_rankings_by_period(df, latest_date_str, [5, 10, 20], 10)
        
        print(f"ğŸ“Š æ¿å—æ’åç»Ÿè®¡:")
        for period_key in ranking_results:
            if ranking_results[period_key]:  # ç¡®ä¿æœ‰æ•°æ®
                top_3_names = [item['sector_name'] for item in ranking_results[period_key][:3]]
                print(f"   {period_key}å‰3: {top_3_names}")

        # è·å–ranking_resultsçš„åˆ—è¡¨ï¼Œå¹¶åœ¨dfä¸­ç­›é€‰å‡ºè¿™äº›æ¿å—
        sector_names = [item['sector_name'] for sublist in ranking_results.values() for item in sublist]
        # å»é™¤é‡å¤çš„æ¿å—åç§°
        sector_names = list(set(sector_names))
        
        print(f"ğŸ” ç­›é€‰å‰æ•°æ®èŒƒå›´: {df['date_str'].min()} åˆ° {df['date_str'].max()}")
        print(f"ğŸ” ç­›é€‰å‰æ€»æ¿å—æ•°: {df['name'].nunique()}")
        print(f"ğŸ” æ’åæ¿å—æ•°: {len(sector_names)}")
        
        df = df[df['name'].isin(sector_names)]
        
        print(f"ğŸ” ç­›é€‰åæ•°æ®èŒƒå›´: {df['date_str'].min()} åˆ° {df['date_str'].max()}")
        print(f"ğŸ” ç­›é€‰åæ€»è®°å½•æ•°: {len(df)}")
        
        # æ£€æŸ¥æ¯ä¸ªæ¿å—çš„æ•°æ®èŒƒå›´
        for sector in sector_names[:3]:  # åªæ£€æŸ¥å‰3ä¸ª
            sector_data = df[df['name'] == sector]
            if not sector_data.empty:
                print(f"ğŸ” æ¿å— '{sector}' æ•°æ®èŒƒå›´: {sector_data['date_str'].min()} åˆ° {sector_data['date_str'].max()}")
        
        # groupby nameï¼Œè®¡ç®—changeåˆ—çš„cumsum
        df['change'] = df.groupby('name')['change'].cumsum()

        
        chart_data = []
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['date_str'].unique())
        print(f"ğŸ“… å®Œæ•´æ—¥æœŸèŒƒå›´: {all_dates}")
        
        for sector in sector_names:
            sector_data = df[df['name'] == sector]
            if not sector_data.empty:
                sector_data = sector_data.sort_values(by='trade_date')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = sector_data['date_str'].tolist()
                y_data = sector_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = sector_data[['date_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{sector}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['trade_date'].strftime('%Y-%m-%d') for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
                
                print(f"ğŸ“Š æ¿å— '{sector}': æ•°æ®èŒƒå›´ {sector_data['date_str'].min()} åˆ° {sector_data['date_str'].max()}, å…±{len(sector_data)}å¤©")
                
                # è¯¦ç»†æ£€æŸ¥å‰å‡ ä¸ªæ¿å—çš„xè½´æ•°æ®é¡ºåº
                if sector in sector_names[:2]:  # åªæ£€æŸ¥å‰2ä¸ªæ¿å—
                    print(f"ğŸ” æ¿å— '{sector}' xè½´æ•°æ®é¡ºåº: {x_data}")
                    print(f"ğŸ” æ¿å— '{sector}' å‰5ä¸ªyå€¼: {y_data[:5]}")
        
        # æ£€æŸ¥æ€»çš„chart_dataç»“æ„
        print(f"ğŸ“ˆ ç”Ÿæˆå›¾è¡¨æ•°æ®: å…±{len(chart_data)}ä¸ªç³»åˆ—")
        if chart_data:
            first_series = chart_data[0]
            print(f"ğŸ” ç¬¬ä¸€ä¸ªç³»åˆ— '{first_series['name']}' xè½´: {first_series['x']}")
        
        
        
        
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })
    
    def process_market_stocks_change_daily(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily', self._original_market_stocks_change_daily)

    def _original_market_stocks_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df[df['time_str'] == '15:00']
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['change'] > 5]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
        bottom_stocks = tempdf[tempdf['change'] < -5]['id'].unique().tolist()
        # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
        all_stocks = list(set(top_stocks + bottom_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })

    def process_market_stocks_change_daily_uplimit(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily_uplimit', self._original_market_stocks_change_daily_uplimit)


    def _original_market_stocks_change_daily_uplimit(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df[df['time_str'] == '15:00']
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['change'] > 9.7]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
        bottom_stocks = tempdf[tempdf['change'] < -9.7]['id'].unique().tolist()
        # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
        all_stocks = list(set(top_stocks + bottom_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })

    def process_market_stocks_change_daily_speed(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily_speed', self._original_market_stocks_change_daily_speed)


    def _original_market_stocks_change_daily_speed(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # è·å–5åˆ†æ¶¨é€Ÿ
        df['speed5'] = df.groupby('id')['change'].diff(5).fillna(0)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df.copy()
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['speed5'] > 5]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
       
        all_stocks = list(set(top_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            # è¿‡æ»¤æ‰å¼‚å¸¸æ•°æ®ç‚¹ï¼Œä½†ä¿ç•™æ­£å¸¸æ•°æ®
            stock_data = stock_data[(stock_data['change'] >= -31) & (stock_data['change'] <= 31)]
            
            if not stock_data.empty:  # å¦‚æœè¿‡æ»¤åè¿˜æœ‰æ•°æ®
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })
    
    def process_shizhiyu_change_daily(self):
        """å„å¸‚å€¼åŸŸæƒ…ç»ªæ—¥æ•°æ®çš„å¹³å‡æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/shizhiyu_change_daily', self._original_shizhiyu_change_daily)
    
    def _original_shizhiyu_change_daily(self):
        """å„å¸‚å€¼åŸŸæƒ…ç»ªæ—¥æ•°æ®çš„å¹³å‡æ¶¨å¹…"""
        d = StockDailyData()

        df = d.get_daily_data(start_date='2025-07-01')
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        # æ–°å»ºä¸€åˆ—â€˜å¸‚å€¼åŸŸâ€™ï¼ŒæŒ‰ç…§total_mvåˆ—çš„å€¼è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»è§„åˆ™ä¸º
        # 0-20äº¿ï¼šå¾®ç›˜ 20-50äº¿ï¼šå°ç›˜ 50-100äº¿ï¼šä¸­ç›˜ 100äº¿-300äº¿ï¼šä¸­å¤§ç›˜ï¼Œ300äº¿ä»¥ä¸Šï¼šå¤§ç›˜
        df['å¸‚å€¼åŸŸ'] = pd.cut(df['total_mv'], bins=[0, 20e8, 50e8, 100e8, 300e8, float('inf')],
                           labels=['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'])
        # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—changeåˆ—çš„å¹³å‡å€¼
        df['day_change'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('mean')
        # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
        df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
        # æŒ‰å¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå¸‚å€¼åŸŸçš„cumsum
        df_temp['cumsum_change'] = df_temp.groupby('å¸‚å€¼åŸŸ')['day_change'].cumsum()
        market_list = ['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']
        chart_data = []
        for market in market_list:
            market_data = df_temp[df_temp['å¸‚å€¼åŸŸ'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æ¶¨å¹…',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['cumsum_change'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å¸‚å€¼åŸŸæ—¥çº¿æ¶¨å¹…",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })

    def process_lianban_jiji_rate(self):
        """è¿æ¿æ™‹çº§ç‡ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/lianban_jiji_rate', self._original_lianban_jiji_rate)
    
    def _original_lianban_jiji_rate(self):
        """è¿æ¿æ™‹çº§ç‡"""
        d = StockDailyData()

        df = d.get_daily_data(start_date='2025-03-01')
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        # æ–°å»ºä¸€åˆ—â€˜å¸‚å€¼åŸŸâ€™ï¼ŒæŒ‰ç…§total_mvåˆ—çš„å€¼è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»è§„åˆ™ä¸º
        # 0-20äº¿ï¼šå¾®ç›˜ 20-50äº¿ï¼šå°ç›˜ 50-100äº¿ï¼šä¸­ç›˜ 100äº¿-300äº¿ï¼šä¸­å¤§ç›˜ï¼Œ300äº¿ä»¥ä¸Šï¼šå¤§ç›˜
        df['next_day_change'] = df.groupby('id')['change'].shift(-1)  # è·å–ä¸‹ä¸€å¤©çš„æ¶¨è·Œå¹…
        # å› ä¸ºshiftäº†-1ï¼Œæ‰€ä»¥éœ€è¦groupbyåå»æ‰æœ€åä¸€è¡Œ
        df = df[df['next_day_change'].notna()]
        df = df[df['change']>=9.7]
        
        # å…ˆè®¡ç®—æ¯ä¸ªäº¤æ˜“æ—¥çš„æ¶¨åœæ™‹çº§ç‡
        daily_stats = df.groupby('trade_date')['next_day_change'].agg([
            ('total_count', 'count'),
            ('upgrade_count', lambda x: (x >= 9.7).sum())
        ]).reset_index()
        daily_stats['æ¶¨åœæ™‹çº§ç‡'] = daily_stats['upgrade_count'] / daily_stats['total_count']
        
        # æ·»åŠ date_stråˆ—
        daily_stats['date_str'] = pd.to_datetime(daily_stats['trade_date']).dt.strftime('%m/%d')
        
        df = daily_stats[['trade_date', 'date_str', 'æ¶¨åœæ™‹çº§ç‡']]
        chart_data = []
        chart_data.append({
            "name": f'æ¶¨åœæ™‹çº§ç‡',
            "x": df['date_str'].tolist(),
            "y": df['æ¶¨åœæ™‹çº§ç‡'].tolist()
        })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å¸‚å€¼åŸŸæ—¥çº¿æ¶¨å¹…",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æ¶¨åœæ™‹çº§ç‡(%)"},
                "legend": {"title": "æ¶¨åœæ™‹çº§ç‡"}
            }
        })

    def process_every_lianban_jiji_rate(self):
        """å„è¿æ¿æ™‹çº§ç‡ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/every_lianban_jiji_rate', self._original_every_lianban_jiji_rate)
    
    def _original_every_lianban_jiji_rate(self):
        """å„è¿æ¿æ™‹çº§ç‡"""
        # è¯»å–data\kpl_market_sentiment_data.csvçš„æ•°æ®
        kpl_data = pd.read_csv('data/kpl_market_sentiment_data.csv', encoding='gbk')
        kpl_data['trade_date'] = pd.to_datetime(kpl_data['trade_date'], errors='coerce')
        # å‡åºæ’åˆ—
        kpl_data = kpl_data.sort_values(by='trade_date').reset_index(drop=True)
        # å–æœ€æ–°çš„100å¤©æ•°æ®
        kpl_data = kpl_data.tail(100)
        kpl_data['date_str'] = kpl_data['trade_date'].dt.strftime('%m/%d')
        # rename up_limit_2_rateï¼š2è¿æ¿æ™‹çº§ç‡ï¼Œup_limit_3_rateï¼š3è¿æ¿æ™‹çº§ç‡ï¼Œup_limit_4_rateï¼š4è¿æ¿æ™‹çº§ç‡
        kpl_data = kpl_data.rename(columns={
            'up_limit_2_rate': '1è¿›2æ™‹çº§ç‡',
            'up_limit_3_rate': '2è¿›3æ™‹çº§ç‡',
            'up_limit_high_rate': '3æ¿ä»¥ä¸Šæ™‹çº§ç‡'
        })
        
        chart_data = []
        for column in ['1è¿›2æ™‹çº§ç‡', '2è¿›3æ™‹çº§ç‡', '3æ¿ä»¥ä¸Šæ™‹çº§ç‡']:
            if column in kpl_data.columns:
                chart_data.append({
                    "name": column,
                    "x": kpl_data['trade_date'].dt.strftime('%Y-%m-%d').tolist(),  # ä½¿ç”¨å®Œæ•´æ—¥æœŸæ ¼å¼
                    "y": kpl_data[column].tolist()
                })
       
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„è¿æ¿æ™‹çº§ç‡",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "date",  # æŒ‡å®šxè½´ä¸ºæ—¥æœŸç±»å‹
                    "tickformat": "%m/%d"  # æ˜¾ç¤ºæ ¼å¼ä»ä¸ºæœˆ/æ—¥
                },
                "yaxis": {"title": "æ¶¨åœæ™‹çº§ç‡(%)"},
                "legend": {"title": "æ¶¨åœæ™‹çº§ç‡"}
            }
        })


    def process_sector_line_chart_change(self):
        """æ¿å—æ¶¨å¹…æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_change', self._original_sector_line_chart_change)
    
    def _original_sector_line_chart_change(self):
        """æ¿å—æ¶¨å¹…æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(10)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(20)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}æ¶¨å¹…',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['æ¿å—æ¶¨å¹…'].tolist()
                    })
            
            return jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—æ¶¨å¹…",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "æ¶¨å¹…(%)"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨å¹…æ•°æ®å¤±è´¥: {e}")

    def process_sector_speed_chart(self):
        """æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_speed_chart', self._original_sector_speed_chart)
    
    def _original_sector_speed_chart(self):
        """æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨æ•°æ®"""
        try:
            self.logger.info("å¼€å§‹å¤„ç†æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨")
            
            # è¯»å–åŸºç¡€æ•°æ®
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "chartType": "line",
                    "data": [],
                    "layout": {
                        "title": "æ¿å—æ¶¨é€Ÿå˜åŒ–ç´¯è®¡",
                        "xaxis": {"title": "æ—¶é—´"},
                        "yaxis": {"title": "ç´¯è®¡æ¶¨é€Ÿ"},
                        "legend": {"title": "æ¿å—åç§°"}
                    },
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # è·å–æ¿å—æ¶¨å¹…æ’åº
            top_sectors = stock_df['Sector'].value_counts().head(10).index.tolist()
            if not top_sectors:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¿å—æ•°æ®"
                })
            
            stock_minute_df = self.data_cache.load_data('stock_minute_df')
            if stock_minute_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "åˆ†é’Ÿæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            stock_df['time'] = pd.to_datetime(stock_df['time'])
            latest_time = stock_df['time'].max()
            
            source_data = {
                'top_sectors': sorted(top_sectors[:10]),  # åªå–å‰10ä¸ªç”¨äºå“ˆå¸Œï¼Œé¿å…æ•°æ®é‡è¿‡å¤§
                'stock_data_time': str(latest_time),
                'stock_minute_count': len(stock_minute_df),
                'file_timestamps': {
                    'stock_df': self.data_cache.timestamps.get('stock_df', 0),
                    'stock_minute_df': self.data_cache.timestamps.get('stock_minute_df', 0),
                    'affinity_df': self.data_cache.timestamps.get('affinity_df', 0)
                }
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector_speed_chart'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            self.logger.info(f"é‡æ–°è®¡ç®—æ¿å—æ¶¨é€Ÿæ•°æ®ï¼Œå¤„ç† {len(top_sectors)} ä¸ªæ¿å—")
            
            # æ•°æ®æ¸…ç†å’Œé¢„å¤„ç†
            stock_df = stock_df.replace([np.inf, -np.inf], 0).fillna(0)
            stock_minute_df = stock_minute_df.replace([np.inf, -np.inf], 0).fillna(0)
            
            stock_df['Sector'] = stock_df['Sector'].astype(str)
            stock_df['id'] = stock_df['id'].astype(int)
            stock_df['change'] = stock_df['change'].astype(float)
            stock_df['time'] = pd.to_datetime(stock_df['time'])
            
            latest_time = stock_df['time'].max()
            stock_df = stock_df[stock_df['time'] == latest_time]
            
            affinity_df = self.data_cache.load_data('affinity_df')
            if affinity_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—å…³è”æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            chart_data = []
            
            # ä¼˜åŒ–ï¼šé™åˆ¶å¤„ç†çš„æ¿å—æ•°é‡ä»¥æå‡æ€§èƒ½
            process_sectors = top_sectors[:20]  # åªå¤„ç†å‰20ä¸ªæ¿å—ï¼Œæå‡æ€§èƒ½
            self.logger.info(f"å®é™…å¤„ç†æ¿å—æ•°é‡: {len(process_sectors)}")
            
            for sector_name in process_sectors:
                # æ¨¡ç³ŠåŒ¹é…æ¿å—
                sector_affinity_df = affinity_df[
                    affinity_df['æ¿å—'].str.contains(sector_name, na=False, case=False) |
                    affinity_df['æ¿å—'].apply(lambda x: sector_name in str(x) if pd.notna(x) else False)
                ]
                
                if sector_affinity_df.empty:
                    continue
                
                stock_ids = list(set(sector_affinity_df['è‚¡ç¥¨id'].tolist()))
                stock_count = len(stock_ids)
                
                # è¿‡æ»¤è‚¡ç¥¨ID
                stock_ids = [id for id in stock_ids if id < 680000 and (id < 400000 or id > 600000)]
                filtered_count = len(stock_ids)
                
                stock_minute_df_temp = stock_minute_df[stock_minute_df['id'].isin(stock_ids)]
                stock_minute_df_temp = stock_minute_df_temp[stock_minute_df_temp['change'] > 2]
                
                if stock_minute_df_temp.empty:
                    continue
                
                # æŒ‰æ—¶é—´ç»„èšåˆ
                stock_minute_df_temp = stock_minute_df_temp.groupby('time').agg({
                    'speed_change_1min': 'mean',
                    'id': 'count'
                }).reset_index()
                
                stock_minute_df_temp = stock_minute_df_temp.rename(columns={'id': 'stock_count'})
                stock_minute_df_temp = stock_minute_df_temp.sort_values('time')
                
                stock_minute_df_temp['speed_change_1min_rate'] = (
                    stock_minute_df_temp['speed_change_1min'] * 
                    stock_minute_df_temp['stock_count'] / filtered_count
                )
                stock_minute_df_temp['speed_change_1min_cumsum'] = (
                    stock_minute_df_temp['speed_change_1min_rate'].cumsum()
                )
                
                stock_minute_df_temp['time'] = pd.to_datetime(stock_minute_df_temp['time'])
                
                chart_data.append({
                    "name": f'{sector_name}æ¶¨é€Ÿå˜åŒ–ç´¯è®¡',
                    "x": stock_minute_df_temp['time'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                    "y": stock_minute_df_temp['speed_change_1min_cumsum'].tolist()
                })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—æ¶¨é€Ÿå˜åŒ–ç´¯è®¡",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "ç´¯è®¡æ¶¨é€Ÿ"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            self.logger.info(f"æ¿å—æ¶¨é€Ÿæ•°æ®è®¡ç®—å®Œæˆï¼Œç”Ÿæˆ {len(chart_data)} ä¸ªæ¿å—çš„å›¾è¡¨æ•°æ®")
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨é€Ÿæ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uplimit(self):
        """æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uplimit', self._original_sector_line_chart_uplimit)
    
    def _original_sector_line_chart_uplimit(self):
        """æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            latest_time = sector_df['æ—¶é—´'].max()
            source_data = {
                'data_time': str(latest_time),
                'data_count': len(sector_df),
                'sector_names': sorted(sector_names),  # æ’åºç¡®ä¿ä¸€è‡´æ€§
                'dynamic_titles': self.server.dynamic_titles.copy(),
                'file_timestamp': self.data_cache.timestamps.get('plate_df', 0)  # æ·»åŠ æ–‡ä»¶æ—¶é—´æˆ³
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector-line-chart_uplimit'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # æ·»åŠ è¿‘ä¼¼æ¶¨åœæ•°åˆ—
            sector_df['è¿‘ä¼¼æ¶¨åœæ•°'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(
                lambda x: int(x.split('-')[-1]) if '-' in x else 0
            )
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(220)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}è¿‘ä¼¼æ¶¨åœæ•°',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['è¿‘ä¼¼æ¶¨åœæ•°'].tolist()
                    })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—è¿‘ä¼¼æ¶¨åœæ•°",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "è¿‘ä¼¼æ¶¨åœæ•°"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uprate(self):
        """æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uprate', self._original_sector_line_chart_uprate)
    
    def _original_sector_line_chart_uprate(self):
        """æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            latest_time = sector_df['æ—¶é—´'].max()
            source_data = {
                'data_time': str(latest_time),
                'data_count': len(sector_df),
                'sector_names': sorted(sector_names),  # æ’åºç¡®ä¿ä¸€è‡´æ€§
                'dynamic_titles': self.server.dynamic_titles.copy(),
                'file_timestamp': self.data_cache.timestamps.get('plate_df', 0)  # æ·»åŠ æ–‡ä»¶æ—¶é—´æˆ³
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector-line-chart_uprate'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            sector_df['uprate'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(lambda x: self.server._calculate_tail_ratio(x, n=6))
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}çº¢ç›˜ç‡',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['uprate'].tolist()
                    })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—çº¢ç›˜ç‡",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "çº¢ç›˜ç‡"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—çº¢ç›˜ç‡æ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uprate5(self):
        """æ¿å—uprate5æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uprate5', self._original_sector_line_chart_uprate5)
    
    def _original_sector_line_chart_uprate5(self):
        """æ¿å—uprate5æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            sector_df['uprate5'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(lambda x: self.server._calculate_tail_ratio(x, n=3))
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}çº¢ç›˜ç‡',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['uprate5'].tolist()
                    })
            
            return jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—uprate5",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "uprate5"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—uprate5æ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # è¡¨æ ¼å¤„ç†æ–¹æ³• (åŸ table_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    
    def process_plate_info(self):
        """æ¿å—æ¦‚è¦æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_info', self._original_plate_info)
    
    def _original_plate_info(self):
        """æ¿å—æ¦‚è¦æ•°æ®è¡¨"""
        try:
            sector_df = self.data_cache.load_data('plate_df')
            if sector_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆæ•°æ®å¤„ç†
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            latest_data = sector_df[sector_df['æ—¶é—´'] == sector_df['æ—¶é—´'].max()]
            
            # æ„å»ºè¡¨æ ¼æ•°æ®
            table_data = []
            for _, row in latest_data.iterrows():
                table_data.append({
                    "æ¿å—å": row['æ¿å—å'],
                    "æ¿å—æ¶¨å¹…": f"{row['æ¿å—æ¶¨å¹…']:.2f}%",
                    "æ¿å—5åˆ†æ¶¨é€Ÿ": f"{row['æ¿å—5åˆ†æ¶¨é€Ÿ']:.2f}%",
                    "æ¶¨å¹…åˆ†å¸ƒ": row['æ¶¨å¹…åˆ†å¸ƒ']
                })
            
            return jsonify({
                "columns": ["æ¿å—å", "æ¿å—æ¶¨å¹…", "æ¿å—5åˆ†æ¶¨é€Ÿ", "æ¶¨å¹…åˆ†å¸ƒ"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¦‚è¦æ•°æ®å¤±è´¥: {e}")

    def process_stocks(self):
        """è‚¡ç¥¨æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/stocks', self._original_stocks)
    
    def _original_stocks(self):
        """è‚¡ç¥¨æ•°æ®è¡¨"""
        try:
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†
            latest_data = stock_df.head(100)  # é™åˆ¶æ•°æ®é‡
            
            table_data = []
            for _, row in latest_data.iterrows():
                table_data.append({
                    "è‚¡ç¥¨ä»£ç ": row.get('id', ''),
                    "è‚¡ç¥¨åç§°": row.get('name', ''),
                    "æ¶¨è·Œå¹…": f"{row.get('change', 0):.2f}%",
                    "æ¿å—": row.get('Sector', '')
                })
            
            return jsonify({
                "columns": ["è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°", "æ¶¨è·Œå¹…", "æ¿å—"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # è¡¨æ ¼å¤„ç†æ–¹æ³• (åŸ table_processor.py ä¸­çš„æ–¹æ³•)  
    # =========================================================================
    
    def process_plate_info_table_data(self):
        """è¿”å›æ¿å—æ¦‚è¦æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_info_table_data', self._original_plate_info_table_data)
    
    def _original_plate_info_table_data(self):
        """è¿”å›æ¿å—æ¦‚è¦æ•°æ®è¡¨"""
        try:
            start_time = time.time()
            sector_name = request.args.get('sectors', 'èˆªè¿æ¦‚å¿µ')
            
            # æ„å»ºç¼“å­˜å‚æ•°
            cache_params = self.build_cache_params(sector_name=sector_name)
            
            plate_df = self.data_cache.load_data('plate_df')
            if plate_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ•°æ®å¤„ç†
            plate_df['æ—¶é—´'] = pd.to_datetime(plate_df['æ—¶é—´'])
            latest_time = plate_df['æ—¶é—´'].max()
            plate_df = plate_df[plate_df['æ—¶é—´'] == latest_time]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            source_data = {
                'sector_name': sector_name,
                'data_time': str(latest_time),
                'data_count': len(plate_df),
                # æ·»åŠ å½±å“ç»“æœçš„å…³é”®å­—æ®µçš„å“ˆå¸Œ
                'plate_summary': plate_df[['æ¿å—å', 'æ¿å—æ¶¨å¹…', 'æ¿å—5åˆ†æ¶¨é€Ÿ']].to_dict('records')[:10]  # åªå–å‰10ä¸ªä½œä¸ºæ‘˜è¦
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/table-data/plate_info'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, cache_params, source_data)
            
            if should_cache and cached_response:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—æ¦‚è¦è¡¨æ ¼: {sector_name}")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # è®¡ç®—å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ
            speed_bins = [-10, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 10]
            speed_counts = []
            for i in range(len(speed_bins)-1):
                count = len(plate_df[
                    (plate_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] >= speed_bins[i]) & 
                    (plate_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] < speed_bins[i+1])
                ])
                speed_counts.append(str(count))
            market_speed_distribution = "-".join(speed_counts)
            plate_df['å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ'] = market_speed_distribution
            
            plate_df['æ¿å—å'] = plate_df['æ¿å—å'].astype(str)
            
            # è·å–æ’åå‰15çš„æ¿å—
            top_by_change = plate_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(15)['æ¿å—å'].tolist()
            top_by_turnover = plate_df.sort_values(by='å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”', ascending=False).head(15)['æ¿å—å'].tolist()
            top_plates_list = list(set(top_by_change + top_by_turnover))
            
            if sector_name not in top_plates_list:
                top_plates_list.append(sector_name)
            
            plate_df = plate_df[plate_df['æ¿å—å'].isin(top_plates_list)]
            plate_df = plate_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False)
            plate_df = plate_df.drop_duplicates(subset=['æ¿å—å']).reset_index(drop=True)
            
            # å®šä¹‰è¡¨æ ¼åˆ—
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "æ¿å—å", "header": "æ¿å—å"},
                {"field": "æ¿å—æ¶¨å¹…", "header": "æ¿å—æ¶¨å¹…", "backgroundColor": "redGreen"},
                {"field": "æ¿å—æ˜¨æ—¥æ¶¨å¹…", "header": "æ¿å—æ˜¨æ—¥æ¶¨å¹…", "backgroundColor": "redGreen"},
                {"field": "å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”", "header": "å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”", "backgroundColor": "redGreen"},
                {"field": "æ¿å—5åˆ†æ¶¨é€Ÿ", "header": "æ¿å—5åˆ†æ¶¨é€Ÿ", "backgroundColor": "redGreen"},
                {"field": "æ¿å—é‡æ¯”", "header": "æ¿å—é‡æ¯”", "backgroundColor": "redGreen"},
                {"field": "æ¶¨é€Ÿåˆ†å¸ƒ", "header": "æ¶¨é€Ÿåˆ†å¸ƒ"},
                {"field": "æ¶¨å¹…åˆ†å¸ƒ", "header": "æ¶¨å¹…åˆ†å¸ƒ"},
                {"field": "æ¶¨åœæ¢¯åº¦", "header": "æ¶¨åœæ¢¯åº¦"},
                {"field": "æ¶¨é€Ÿæ’å", "header": "æ¶¨é€Ÿæ’å"},
                {"field": "æ¶¨å¹…æ’å", "header": "æ¶¨å¹…æ’å"},
                {"field": "å¤§ç›˜é‡æ¯”", "header": "å¤§ç›˜é‡æ¯”"},
                {"field": "å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ", "header": "å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ"},
            ]
            
            valid_columns = [col for col in columns if col["field"] in plate_df.columns]
            
            rows = []
            for _, row_data in plate_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, cache_params, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—ä¿¡æ¯å¤±è´¥: {e}")

    def process_stocks_table_data(self):
        """è¿”å›è‚¡ç¥¨æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/stocks_table_data', self._original_stocks_table_data)
    
    def _original_stocks_table_data(self):
        """è¿”å›è‚¡ç¥¨æ•°æ®è¡¨"""
        try:
            # TODO: ä» table_processor.py è¿ç§»å®Œæ•´å®ç°
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†é€»è¾‘
            columns = [
                {"field": "name", "header": "è‚¡ç¥¨åç§°"},
                {"field": "change", "header": "æ¶¨è·Œå¹…", "backgroundColor": "redGreen"},
                {"field": "Sector", "header": "æ¿å—"},
                {"field": "speed_change_1min", "header": "1åˆ†é’Ÿæ¶¨é€Ÿ", "backgroundColor": "redGreen"}
            ]
            
            rows = []
            for _, row in stock_df.head(100).iterrows():  # é™åˆ¶è¿”å›æ•°é‡
                row_data = {}
                for col in columns:
                    field = col["field"]
                    value = row.get(field, '')
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    row_data[field] = value
                rows.append(row_data)
            
            return jsonify({
                "columns": columns,
                "rows": rows
            })
        
        except Exception as e:
            return self.error_response(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
    def process_get_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®ï¼Œä»å›ºå®šCSVæ–‡ä»¶è¯»å– - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/get_up_limit_table_data', self._original_get_up_limit_table_data)
    
    def _original_get_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®ï¼Œä»å›ºå®šCSVæ–‡ä»¶è¯»å–"""
        try:
            # è¯»å–CSVæ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šè·¯å¾„ï¼‰
            
            # è¯»å–æ¶¨åœæ•°æ®
            up_limit_df = pd.read_csv(r'strategy\showhtml\server\up_limit_df.csv')
            
            # # ç¡®ä¿æ—¶é—´åˆ—è¢«æ­£ç¡®å¤„ç†
            
            # up_limit_df['æ—¶é—´'] = pd.to_datetime(up_limit_df['æ—¶é—´'])
            # # è·å–æœ€æ–°çš„æ—¶é—´çš„è¡Œ
            # latest_time = up_limit_df['æ—¶é—´'].max()
            # # for test è·å–09ï¼š41çš„æ—¶é—´
            # # latest_time = plate_df['æ—¶é—´'].max().replace(hour=10, minute=5, second=0)
            # up_limit_df = up_limit_df[up_limit_df['æ—¶é—´'] == latest_time]
            # # è·å–è¯¥æ—¶åˆ»çš„å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒï¼Œè®¡ç®—æ–¹æ³•ä¸ºï¼šç»Ÿè®¡è¯¥æ—¶åˆ»â€˜æ¿å—5åˆ†æ¶¨é€Ÿåˆ—â€™-10~-0.6ï¼Œ-0.6~-0.4ï¼Œ-0.4~-0.2ï¼Œ-0.2~0, 0~0.2ï¼Œ0.2~0.4ï¼Œ0.4~0.6,0.6~10çš„æ•°é‡ï¼Œå¹¶ç”¨-å·è¿æ¥
            # speed_bins = [-10, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 10]
            # speed_counts = []
            # for i in range(len(speed_bins)-1):
            #     count = len(up_limit_df[(up_limit_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] >= speed_bins[i]) & 
            #                         (up_limit_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] < speed_bins[i+1])])
            #     speed_counts.append(str(count))
            # market_speed_distribution = "-".join(speed_counts)
            # up_limit_df['å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ'] = market_speed_distribution
            # # è·å–åˆ—Sectorçš„å€¼ä¸ºsector_nameçš„è¡Œ
            # up_limit_df['æ¿å—å'] = up_limit_df['æ¿å—å'].astype(str)
            # up_limit_df = up_limit_df[up_limit_df['æ¿å—å'] == sector_name]
            
            
            # å®šä¹‰è¡¨æ ¼åˆ—ï¼ˆæ ¹æ®CSVæ–‡ä»¶çš„å®é™…åˆ—è¿›è¡Œè°ƒæ•´ï¼‰
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "è‚¡ç¥¨ID", "header": "è‚¡ç¥¨ID", "visible": False},
                {"field": "è‚¡ç¥¨åç§°", "header": "è‚¡ç¥¨åç§°"},
                {"field": "æ¿å—1", "header": "æ¿å—1"},
                {"field": "æ¿å—2", "header": "æ¿å—2"},
                {"field": "æ¿å—3", "header": "æ¿å—3", "visible": False},
                {"field": "æ¿å—4", "header": "æ¿å—4", "visible": False},
                {"field": "æ¿å—5", "header": "æ¿å—5", "visible": False},
                {"field": "10æ—¥æ¶¨åœæ•°", "header": "10æ—¥æ¶¨åœæ•°"},
                {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "visible": False},

            ]
            
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨äºCSVæ–‡ä»¶ä¸­
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            # è½¬æ¢DataFrameä¸ºæ‰€éœ€æ ¼å¼
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    # å¤„ç†æ•°å€¼æ ¼å¼ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            return jsonify({
                "columns": valid_columns,
                "rows": rows
            })
        
        except Exception as e:
            import traceback
            traceback.print_exc()
            return jsonify({"error": str(e)}), 500
        
    def process_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/up_limit_table_data', self._original_up_limit_table_data)
    
    def _original_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨"""
        # try:
        #     # TODO: ä» table_processor.py è¿ç§»å®Œæ•´å®ç°
        #     up_limit_df = self.data_cache.load_data('up_limit_df')
        #     if up_limit_df.empty:
        #         return jsonify({
        #             "columns": [],
        #             "rows": [],
        #             "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
        #         })
            
        #     # ç®€åŒ–ç‰ˆå¤„ç†é€»è¾‘
        #     columns = [
        #         {"field": "name", "header": "è‚¡ç¥¨åç§°"},
        #         {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "backgroundColor": "redGreen"},
        #         {"field": "Sector", "header": "æ¿å—"},
        #         {"field": "æ¶¨åœæ—¶é—´", "header": "æ¶¨åœæ—¶é—´"}
        #     ]
            
        #     rows = []
        #     for _, row in up_limit_df.iterrows():
        #         row_data = {}
        #         for col in columns:
        #             field = col["field"]
        #             value = row.get(field, '')
        #             if isinstance(value, (float, np.float64, np.float32)):
        #                 value = round(value, 2)
        #             row_data[field] = value
        #         rows.append(row_data)
            
        #     return jsonify({
        #         "columns": columns,
        #         "rows": rows
        #     })
        
        # except Exception as e:
        #     return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")
        try:
            up_limit_df = self.data_cache.load_data('up_limit_df')
            
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            source_data = {
                'data_count': len(up_limit_df),
                'file_timestamp': self.data_cache.timestamps.get('up_limit_df', 0),
                'data_sample': up_limit_df.head(5).to_dict('records') if not up_limit_df.empty else []
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/table-data/up_limit'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¶¨åœæ•°æ®è¡¨")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # å®šä¹‰è¡¨æ ¼åˆ— - æ ¹æ®å®é™…CSVæ–‡ä»¶çš„åˆ—å
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "è‚¡ç¥¨åç§°", "header": "è‚¡ç¥¨åç§°"},
                {"field": "æ¿å—1", "header": "æ¿å—1"},
                {"field": "æ¿å—2", "header": "æ¿å—2"},  
                {"field": "æ¿å—3", "header": "æ¿å—3", "visible": False},
                {"field": "æ¿å—4", "header": "æ¿å—4", "visible": False},
                {"field": "æ¿å—5", "header": "æ¿å—5", "visible": False},
                {"field": "10æ—¥æ¶¨åœæ•°", "header": "10æ—¥æ¶¨åœæ•°", "backgroundColor": "redGreen"},
                {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "backgroundColor": "redGreen"},
                {"field": "è‚¡ç¥¨ID", "header": "è‚¡ç¥¨ID", "visible": False},
            ]
            
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    def process_up_limit_stocks_review(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/up_limit_stocks_review', self._original_up_limit_stocks_review)

    def _original_up_limit_stocks_review(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨"""
        
        try:
            d = KplUpLimitData()
            up_limit_df = d.get_daily_data(start_date='2025-07-01')
            
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # è·å–æœ€æ–°çš„æ—¥æœŸçš„è¡Œ
            up_limit_df['trade_date'] = pd.to_datetime(up_limit_df['trade_date'])
            up_limit_df = up_limit_df[up_limit_df['trade_date'].dt.date == up_limit_df['trade_date'].dt.date.max()]
            # å°†trade_dateæ”¹ä¸ºæœˆæ—¥çš„æ ¼å¼
            up_limit_df['trade_date'] = up_limit_df['trade_date'].dt.strftime('%m-%d')
            # å°†up_limit_amountï¼Œfamcåˆ—è½¬æ¢ä¸ºäº¿å…ƒä¸ºå•ä½
            up_limit_df['up_limit_amount'] = up_limit_df['up_limit_amount'] / 1e8
            up_limit_df['famc'] = up_limit_df['famc'] / 1e8
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # å®šä¹‰è¡¨æ ¼åˆ— - æ ¹æ®å®é™…CSVæ–‡ä»¶çš„åˆ—å
            columns = [
                {"field": "trade_date", "header": "æ—¶é—´"},
                {"field": "stock_name", "header": "è‚¡ç¥¨åç§°"},
                {"field": "up_limit_reason", "header": "æ¶¨åœåŸå› "},
                {"field": "sector", "header": "æ¿å—"},  
                {"field": "up_limit_strength", "header": "æ¶¨åœå¼ºåº¦"},
                {"field": "up_limit_amount", "header": "å°å•é‡‘é¢", "backgroundColor": "redGreen"},
                {"field": "day_up_limit_count", "header": "æ¶¨åœæ—¥æ•°", "backgroundColor": "redGreen"},
                {"field": "up_limit_count", "header": "æ¶¨åœæ¬¡æ•°", "backgroundColor": "redGreen"},
                {"field": "famc", "header": "æµé€šå¸‚å€¼", "backgroundColor": "redGreen"},
                {"field": "intro", "header": "å…¬å¸ä»‹ç»"},
                {"field": "id", "header": "è‚¡ç¥¨ID", "visible": False},
            ]
            
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")
        
    def process_up_limit(self):
        """æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/up_limit', self._original_up_limit)
    
    def _original_up_limit(self):
        """æ¶¨åœæ•°æ®è¡¨"""
        try:
            up_limit_df = self.data_cache.load_data('up_limit_df')
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†
            table_data = []
            for _, row in up_limit_df.iterrows():
                table_data.append({
                    "è‚¡ç¥¨åç§°": row.get('name', ''),
                    "è¿æ¿æ•°": row.get('è¿æ¿æ•°', 0),
                    "æ¿å—": row.get('Sector', ''),
                    "æ¶¨åœæ—¶é—´": row.get('æ¶¨åœæ—¶é—´', '')
                })
            
            return jsonify({
                "columns": ["è‚¡ç¥¨åç§°", "è¿æ¿æ•°", "æ¿å—", "æ¶¨åœæ—¶é—´"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # æ¿å—å¤„ç†æ–¹æ³• (åŸ sector_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    
    # =========================================================================
    # æ¿å—å¤„ç†æ–¹æ³• (åŸ sector_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    def build_stacked_area_data(self, df, x_axis_col, data_columns_config, colors=None):
        """
        æ„å»ºå †å é¢ç§¯å›¾æ•°æ®çš„é€šç”¨å‡½æ•°
        
        Args:
            df: æ•°æ®DataFrame
            x_axis_col: xè½´åˆ—å
            data_columns_config: æ•°æ®åˆ—é…ç½®ï¼Œæ ¼å¼ä¸º [{"key": "æ˜¾ç¤ºåç§°", "column": "åˆ—å"}, ...]
            colors: é¢œè‰²åˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é¢œè‰²
        
        Returns:
            dict: åŒ…å«stackedAreaDataã€xAxisValuesã€tableDataçš„å­—å…¸
        """
        try:
            # æ„å»º stackedAreaChart æ•°æ®æ ¼å¼
            xAxisValues = df[x_axis_col].tolist()  # xè½´æ•°æ®
            
            # æ„å»ºæ•°æ®å­—å…¸
            data = {}
            table_data = {}
            hover_data = {}
            
            for _, row in df.iterrows():
                x_value = row[x_axis_col]
                
                # æŒ‰ç…§é…ç½®é¡ºåºæ„å»ºæ•°æ®
                row_data = {}
                row_hover = {}
                
                for config in data_columns_config:
                    key = config["key"]
                    column = config["column"]
                    value = int(row[column]) if pd.notna(row[column]) else 0
                    
                    row_data[key] = value
                    row_hover[key] = [f"{value}"]
                
                data[x_value] = row_data
                
                # è®¡ç®—æ€»æ•°
                total = sum(row_data.values())
                table_data[x_value] = f"{total}"
                hover_data[x_value] = row_hover
            
            # å®šä¹‰æ˜¾ç¤ºé¡ºåºå’Œé¢œè‰²
            keyOrder = [config["key"] for config in data_columns_config]
            
            if colors is None:
                # é»˜è®¤é¢œè‰²é…ç½®
                default_colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000', 
                                '#9932CC', '#FF69B4', '#FFD700', '#32CD32', '#FF4500', '#1E90FF']
                colors = default_colors[:len(keyOrder)]
            
            # æ„å»ºè¿”å›æ•°æ®
            return {
                "stackedAreaData": {
                    "data": data,
                    "keyOrder": keyOrder,
                    "colors": colors,
                    "hoverData": hover_data
                },
                "xAxisValues": xAxisValues,
                "tableData": table_data
            }
            
        except Exception as e:
            self.logger.error(f"æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥: {e}")
            return None

    def process_all_market_change_distribution(self):
        """
        è·å–å…¨å¸‚åœºæ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/all_market_change_distribution', self._original_all_market_change_distribution)
    
    def _original_all_market_change_distribution(self):
        """
        è·å–å…¨å¸‚åœºæ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™å…¨å¸‚åœºæ•°æ®
            df = df[df['name'] == 'å…¨å¸‚åœº']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_up5_shizhiyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„å„å¸‚å€¼åŸŸæ—¥çº¿çº§åˆ«çš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šå¾®ç›˜, å°ç›˜, ä¸­ç›˜, ä¸­å¤§ç›˜, å¤§ç›˜
        """
        return self._process_with_startup_cache('/api/up5_shizhiyu_distribution', self._original_up5_shizhiyu_distribution)
    
    def _original_up5_shizhiyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„å„å¸‚å€¼åŸŸæ—¥çº¿çº§åˆ«çš„è‚¡ç¥¨æ•°
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šå¾®ç›˜, å°ç›˜, ä¸­ç›˜, ä¸­å¤§ç›˜, å¤§ç›˜
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            
            df['å¸‚å€¼åŸŸ'] = pd.cut(df['total_mv'], bins=[0, 20e8, 50e8, 100e8, 300e8, float('inf')],
                           labels=['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'])
            df = df[df['change']>=5]  # å»æ‰æ²¡æœ‰å¸‚å€¼åŸŸçš„è¡Œ
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('count')
            # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
            # å°†å¸‚å€¼åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='å¸‚å€¼åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']].sum(axis=1)
            df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']] = df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']].div(df_pivot['æ€»æ•°'], axis=0) * 100 
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            # è®¡ç®—'å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'å„åˆ—çš„percentchange
            market_cap_columns = ['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "å¾®ç›˜", "column": "å¾®ç›˜"},
                {"key": "å°ç›˜", "column": "å°ç›˜"},
                {"key": "ä¸­ç›˜", "column": "ä¸­ç›˜"},
                {"key": "ä¸­å¤§ç›˜", "column": "ä¸­å¤§ç›˜"},
                {"key": "å¤§ç›˜", "column": "å¤§ç›˜"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_up5_zhubanyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„ä¸»æ¿ä¸åˆ›ä¸šæ¿åˆ†å¸ƒ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        return self._process_with_startup_cache('/api/up5_zhubanyu_distribution', self._original_up5_zhubanyu_distribution)
    
    def _original_up5_zhubanyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„ä¸»æ¿ä¸åˆ›ä¸šæ¿åˆ†å¸ƒ
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')

            df['å¸‚å€¼åŸŸ'] = pd.cut(df['id'], bins=[0, 300000, 400000, 499999, 680000, 800000, float('inf')],
                           labels=['ä¸»æ¿1', 'åˆ›ä¸šæ¿', 'æ–°ä¸‰æ¿', 'ä¸»æ¿2', 'ç§‘åˆ›ç‰ˆ', 'åŒ—äº¤æ‰€'])
            df = df[df['change']>=5]  # å»æ‰æ²¡æœ‰å¸‚å€¼åŸŸçš„è¡Œ
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('count')
            

            # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
            # å°†å¸‚å€¼åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='å¸‚å€¼åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['ä¸»æ¿1', 'åˆ›ä¸šæ¿', 'æ–°ä¸‰æ¿', 'ä¸»æ¿2', 'ç§‘åˆ›ç‰ˆ', 'åŒ—äº¤æ‰€']].sum(axis=1)
            df_pivot['ä¸»æ¿'] = df_pivot['ä¸»æ¿1'] + df_pivot['ä¸»æ¿2']  # åˆå¹¶ä¸»æ¿1å’Œä¸»æ¿2
            df_pivot = df_pivot.drop(columns=['ä¸»æ¿1', 'ä¸»æ¿2'])  # åˆ é™¤åˆå¹¶åçš„åˆ—
            df_pivot['æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'] = df_pivot['æ–°ä¸‰æ¿'] + df_pivot['åŒ—äº¤æ‰€']  # åˆå¹¶æ–°ä¸‰æ¿å’ŒåŒ—äº¤æ‰€
            df_pivot = df_pivot.drop(columns=['æ–°ä¸‰æ¿', 'åŒ—äº¤æ‰€'])  # åˆ é™¤åˆå¹¶åçš„åˆ—
            df_pivot[['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€']] = df_pivot[['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€']].div(df_pivot['æ€»æ•°'], axis=0) * 100
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "ä¸»æ¿", "column": "ä¸»æ¿"},
                {"key": "åˆ›ä¸šæ¿", "column": "åˆ›ä¸šæ¿"},
                {"key": "ç§‘åˆ›ç‰ˆ", "column": "ç§‘åˆ›ç‰ˆ"},
                {"key": "æ–°ä¸‰æ¿+åŒ—äº¤æ‰€", "column": "æ–°ä¸‰æ¿+åŒ—äº¤æ‰€"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_plate_stock_day_change_distribution(self):
        """
        ä»txtè¯»å–æ–‡ä»¶ä¸­çš„æ¿å—å,åœ¨csvæ–‡ä»¶ä¸­æ‰¾åˆ°è¯¥æ¿å—å¯¹åº”çš„è‚¡ç¥¨idï¼Œå¹¶è·å–å…¶æ—¥çº¿æ¶¨å¹…æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºå„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨ä¸ªæ•°
        """
        return self._process_with_startup_cache('/api/plate_stock_day_change_distribution', self._original_plate_stock_day_change_distribution)
    
    def _original_plate_stock_day_change_distribution(self):
        """
        ä»txtè¯»å–æ–‡ä»¶ä¸­çš„æ¿å—å,åœ¨csvæ–‡ä»¶ä¸­æ‰¾åˆ°è¯¥æ¿å—å¯¹åº”çš„è‚¡ç¥¨idï¼Œå¹¶è·å–å…¶æ—¥çº¿æ¶¨å¹…æ•°æ®
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºå„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨ä¸ªæ•°
        """
        try:
            # è·å–data\plate_name.txtä¸­çš„æ¿å—å
            plate_name_file = r'data\plate_name.txt'
            if not os.path.exists(plate_name_file):
                return self.error_response("æ¿å—åæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º data/plate_name.txt æ–‡ä»¶")
            with open(plate_name_file, 'r', encoding='utf-8') as f:
                sector_name = f.read().strip()
            
            # è¯»å–strategy\strategy001\data\all_sectors_stock_level.csvä¸­çš„æ•°æ®
            all_sectors_stock_df = pd.read_csv(r'strategy\strategy001\data\all_sectors_stock_level.csv')
            # è·å–Sectoråˆ—ä¸­åŒ…å«sector_nameçš„è¡Œçš„idåˆ—çš„list
            sector_ids = all_sectors_stock_df[all_sectors_stock_df['Sector'].str.contains(sector_name, na=False)]['id'].tolist()
            
            # è¯»å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df = df[df['id'].isin(sector_ids)]

            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')

            # è·å–æœ€è¿‘20æ—¥çš„è¡Œ
            df = df[df['trade_date'] >= (df['trade_date'].max() - pd.Timedelta(days=20))]
            # åˆ›å»ºæ¶¨å¹…åŸŸåˆ—
            df['æ¶¨å¹…åŸŸ'] = pd.cut(df['change'], bins=[float('-inf'), -9.7, -5, -2, 0, 2, 5, 9.7, float('inf')],
                           labels=['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf'])

            # æŒ‰trade_dateå’Œæ¶¨å¹…åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'æ¶¨å¹…åŸŸ'])['change'].transform('count')

            # æ¯å¤©çš„æ¶¨å¹…åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„æ¶¨å¹…åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'æ¶¨å¹…åŸŸ'])
            # å°†æ¶¨å¹…åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='æ¶¨å¹…åŸŸ', values='day_count').fillna(0).reset_index()
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "c_inf_c-10", "column": "c_inf_c-10"},
                {"key": "c-10_c-5", "column": "c-10_c-5"},
                {"key": "c-5_c-2", "column": "c-5_c-2"},
                {"key": "c-2_c0", "column": "c-2_c0"},
                {"key": "c0_c2", "column": "c0_c2"},
                {"key": "c2_c5", "column": "c2_c5"},
                {"key": "c5_c10", "column": "c5_c10"},
                {"key": "c10_c_inf", "column": "c10_c_inf"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', "#697472", '#FF6B6B', '#FF0000', '#9932CC', '#FF69B4']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_up5_fan_sencer_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº9.7çš„æ˜¨æ—¥ä¹°å…¥å¹³å‡æ¶¨å¹…åˆ†å¸ƒ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        return self._process_with_startup_cache('/api/up5_fan_sencer_distribution', self._original_up5_fan_sencer_distribution)
    
    def _original_up5_fan_sencer_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº9.7çš„æ˜¨æ—¥ä¹°å…¥å¹³å‡æ¶¨å¹…åˆ†å¸ƒ
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            df['next_day_change'] = df.groupby('id')['change'].shift(-1)  # è·å–ä¸‹ä¸€å¤©çš„æ¶¨è·Œå¹…
            # å› ä¸ºshiftäº†-1ï¼Œæ‰€ä»¥éœ€è¦groupbyåå»æ‰æœ€åä¸€è¡Œ
            df = df[df['next_day_change'].notna()]
            df = df[df['change']>=9.7] 
            # å»é™¤openï¼Œ close, high, lowä¸ºç›¸åŒå€¼çš„è¡Œ
            df = df[(df['open'] != df['close']) | (df['open'] != df['high']) | (df['open'] != df['low'])]
            
            df['æ¶¨å¹…åŸŸ'] = pd.cut(df['next_day_change'], bins=[float('-inf'), -9.7, -5, -2, 0, 2, 5, 9.7, float('inf')],
                           labels=['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf'])
           
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'æ¶¨å¹…åŸŸ'])['change'].transform('count')

            # æ¯å¤©çš„æ¶¨å¹…åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„æ¶¨å¹…åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'æ¶¨å¹…åŸŸ'])

            # å°†æ¶¨å¹…åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='æ¶¨å¹…åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']].sum(axis=1)
            df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']] = df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']].div(df_pivot['æ€»æ•°'], axis=0) * 100
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "c_inf_c-10", "column": "c_inf_c-10"},
                {"key": "c-10_c-5", "column": "c-10_c-5"},
                {"key": "c-5_c-2", "column": "c-5_c-2"},
                {"key": "c-2_c0", "column": "c-2_c0"},
                {"key": "c0_c2", "column": "c0_c2"},
                {"key": "c2_c5", "column": "c2_c5"},
                {"key": "c5_c10", "column": "c5_c10"},
                {"key": "c10_c_inf", "column": "c10_c_inf"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', "#697472", '#FF6B6B', '#FF0000', '#9932CC', '#FF69B4']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
                
    def process_chuangye_change_distribution(self):
        """
        è·å–åˆ›ä¸šæ¿æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/chuangye_change_distribution', self._original_chuangye_change_distribution)
    
    def _original_chuangye_change_distribution(self):
        """
        è·å–åˆ›ä¸šæ¿æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™åˆ›ä¸šæ¿æ•°æ®
            df = df[df['name'] == 'åˆ›ä¸šæ¿']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_st_change_distribution(self):
        """
        è·å–STè‚¡ç¥¨æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/st_change_distribution', self._original_st_change_distribution)
    
    def _original_st_change_distribution(self):
        """
        è·å–STè‚¡ç¥¨æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™STæ•°æ®
            df = df[df['name'] == 'ST']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")    
            
    def process_today_plate_up_limit_distribution(self):
        """
        è·å–ä»Šæ—¥å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒï¼ˆæ¿å—å†…è‚¡ç¥¨æ—¥çº¿æ¶¨å¹…åˆ†å¸ƒï¼‰ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨å †ç§¯å›¾å±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        """
        return self._process_with_startup_cache('/api/today_plate_up_limit_distribution', self._original_today_plate_up_limit_distribution)
    
    def _original_today_plate_up_limit_distribution(self):
        """
        è·å–ä»Šæ—¥å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒ
        æ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨å †ç§¯å›¾å±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        """
        try:
            stock_all_level_df = self.data_cache.load_data('stock_all_level_df')
            if stock_all_level_df.empty:
                return self.error_response("è‚¡ç¥¨è¿æ¿æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—
            stock_all_level_df = stock_all_level_df[~stock_all_level_df['Sector'].str.contains("æ²ªè‚¡é€š|æ·±è‚¡é€š|å­£æŠ¥|èèµ„èåˆ¸", na=False)]
            stock_all_level_df = stock_all_level_df[stock_all_level_df['è¿æ¿æ•°'] > 0]
           
            stock_all_level_df['Level'] = stock_all_level_df['è¿æ¿æ•°']
            # å°†è¿æ¿æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†å¼‚å¸¸å€¼
            stock_all_level_df['Level'] = pd.to_numeric(stock_all_level_df['Level'], errors='coerce').fillna(0).astype(int)

            # æŒ‰ç…§æ¿å—åˆ†ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒ
            sector_level_stats = stock_all_level_df.groupby(['Sector', 'Level']).size().unstack(fill_value=0)
            
            # è·å–æ‰€æœ‰çš„è¿æ¿æ•°ç­‰çº§
            max_level = min(stock_all_level_df['Level'].max(), 10)  # é™åˆ¶æœ€å¤§è¿æ¿æ•°ä¸º10ï¼Œé¿å…è¿‡å¤šåˆ†ç±»
            level_columns = list(range(1, max_level + 1))
            
            # ç¡®ä¿æ‰€æœ‰è¿æ¿æ•°ç­‰çº§çš„åˆ—éƒ½å­˜åœ¨
            for level in level_columns:
                if level not in sector_level_stats.columns:
                    sector_level_stats[level] = 0
            
            # åªä¿ç•™éœ€è¦çš„è¿æ¿æ•°åˆ—ï¼Œå¹¶æŒ‰é¡ºåºæ’åˆ—
            sector_level_stats = sector_level_stats[level_columns]
            
            # è®¡ç®—æ¯ä¸ªæ¿å—çš„æ€»è‚¡ç¥¨æ•°ï¼ŒæŒ‰æ€»æ•°æ’åºï¼Œåªæ˜¾ç¤ºå‰20ä¸ªæ¿å—
            sector_level_stats['total'] = sector_level_stats.sum(axis=1)
            sector_level_stats = sector_level_stats.sort_values('total', ascending=False).head(20)
            sector_level_stats = sector_level_stats.drop('total', axis=1)
            
            # æ„å»ºå †ç§¯å›¾æ•°æ® - ä½¿ç”¨ä¸å…¶ä»–å›¾è¡¨ä¸€è‡´çš„æ ¼å¼
            categories = sector_level_stats.index.tolist()  # æ¿å—åç§°ä½œä¸ºæ¨ªåæ ‡
            chart_data = []
            
            # ä¸ºæ¯ä¸ªè¿æ¿æ•°ç­‰çº§åˆ›å»ºä¸€ä¸ªç³»åˆ—
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#F1948A', '#D5DBDB', '#A569BD', '#F0B27A']
            
            for i, level in enumerate(level_columns):
                chart_data.append({
                    'name': f'{level}è¿æ¿',
                    'x': categories,  # æ¿å—åç§°
                    'y': sector_level_stats[level].tolist(),  # å¯¹åº”çš„è‚¡ç¥¨ä¸ªæ•°
                    'type': 'bar',  # æŸ±çŠ¶å›¾ç±»å‹
                    'marker': {'color': colors[i % len(colors)]}
                })
            
            return jsonify({
                "chartType": "bar",  # æŸ±çŠ¶å›¾
                "data": chart_data,  # å›¾è¡¨æ•°æ®
                "layout": {
                    "title": "å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒ",
                    "xaxis": {"title": "æ¿å—åç§°"},
                    "yaxis": {"title": "è‚¡ç¥¨ä¸ªæ•°"},
                    "barmode": "stack",  # å †ç§¯æ¨¡å¼
                    "legend": {"title": "è¿æ¿æ•°"}
                }
            })
            
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—è¿æ¿æ•°åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_today_plate_up_limit_distribution_v2(self):
        """
        è·å–ä»Šæ—¥å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒï¼Œæ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨stackedAreaChartå±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        
        å§‹ç»ˆè¿”å›æ‚¬æµ®æç¤ºæ•°æ®ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦ä½¿ç”¨
        """
        try:
            stock_all_level_df = self.data_cache.load_data('stock_all_level_df')
            if stock_all_level_df.empty:
                return self.error_response("è‚¡ç¥¨è¿æ¿æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—
            stock_all_level_df = stock_all_level_df[~stock_all_level_df['Sector'].str.contains("æ²ªè‚¡é€š|æ·±è‚¡é€š|å­£æŠ¥|èèµ„èåˆ¸", na=False)]
            stock_all_level_df = stock_all_level_df[stock_all_level_df['è¿æ¿æ•°'] > 0]
           
            stock_all_level_df['Level'] = stock_all_level_df['è¿æ¿æ•°']
            # å°†è¿æ¿æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†å¼‚å¸¸å€¼
            stock_all_level_df['Level'] = pd.to_numeric(stock_all_level_df['Level'], errors='coerce').fillna(0).astype(int)

            # æŒ‰ç…§æ¿å—åˆ†ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒ
            sector_level_stats = stock_all_level_df.groupby(['Sector', 'Level']).size().unstack(fill_value=0)
            
            # è·å–æ‰€æœ‰çš„è¿æ¿æ•°ç­‰çº§
            max_level = min(stock_all_level_df['Level'].max(), 6)  # é™åˆ¶æœ€å¤§è¿æ¿æ•°ä¸º6ï¼Œé€‚åˆå †å é¢ç§¯å›¾
            level_columns = list(range(1, max_level + 1))
            
            # ç¡®ä¿æ‰€æœ‰è¿æ¿æ•°ç­‰çº§çš„åˆ—éƒ½å­˜åœ¨
            for level in level_columns:
                if level not in sector_level_stats.columns:
                    sector_level_stats[level] = 0
            
            # åªä¿ç•™éœ€è¦çš„è¿æ¿æ•°åˆ—ï¼Œå¹¶æŒ‰é¡ºåºæ’åˆ—
            sector_level_stats = sector_level_stats[level_columns]
            
            # è®¡ç®—æ¯ä¸ªæ¿å—çš„æ€»è‚¡ç¥¨æ•°ï¼ˆæ‰€æœ‰è¿æ¿æ•°çš„åˆè®¡ï¼‰ï¼ŒæŒ‰æ€»è‚¡ç¥¨æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰ï¼Œåªæ˜¾ç¤ºå‰15ä¸ªæ¿å—ï¼ˆé€‚åˆé¢ç§¯å›¾æ˜¾ç¤ºï¼‰
            sector_level_stats['total_stocks'] = sector_level_stats.sum(axis=1)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.info("æ’åºå‰çš„æ¿å—é¡ºåºå’Œæ€»è‚¡ç¥¨æ•°:")
            for idx, total in sector_level_stats['total_stocks'].items():
                self.logger.info(f"  {idx}: {total}åª")
            
            sector_level_stats = sector_level_stats.sort_values('total_stocks', ascending=False).head(15)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.info("æ’åºåçš„æ¿å—é¡ºåºå’Œæ€»è‚¡ç¥¨æ•°:")
            for idx, total in sector_level_stats['total_stocks'].items():
                self.logger.info(f"  {idx}: {total}åª")
            
            sector_level_stats = sector_level_stats.drop('total_stocks', axis=1)
            
            # æ„å»º stackedAreaChart æ•°æ®æ ¼å¼
            xAxisValues = sector_level_stats.index.tolist()  # æ¿å—åç§°ä½œä¸ºæ¨ªåæ ‡
            
            # æ„å»ºæ•°æ®å­—å…¸ï¼Œæ¯ä¸ªæ¿å—å¯¹åº”å…¶å„è¿æ¿æ•°çš„è‚¡ç¥¨ä¸ªæ•°
            data = {}
            table_data = {}
            hover_data = {}  # å§‹ç»ˆåˆ›å»ºæ‚¬æµ®æ•°æ®
            
            for sector_name in xAxisValues:
                sector_data = {}
                sector_hover = {}  # å§‹ç»ˆå­˜å‚¨è‚¡ç¥¨åç§°åˆ—è¡¨
                total_stocks = 0
                
                for level in level_columns:
                    key = f"{level}è¿æ¿"
                    value = int(sector_level_stats.loc[sector_name, level])
                    sector_data[key] = value
                    total_stocks += value
                    
                    # å§‹ç»ˆè·å–è¯¥æ¿å—è¯¥è¿æ¿ç­‰çº§çš„è‚¡ç¥¨åç§°åˆ—è¡¨
                    level_stocks = stock_all_level_df[
                        (stock_all_level_df['Sector'] == sector_name) & 
                        (stock_all_level_df['Level'] == level)
                    ]['stock_name'].tolist()
                    
                    sector_hover[key] = level_stocks
                
                data[sector_name] = sector_data
                table_data[sector_name] = f"{total_stocks}åª"
                hover_data[sector_name] = sector_hover
            
            # å®šä¹‰è¿æ¿æ•°ç±»å‹çš„é¡ºåºå’Œé¢œè‰²
            keyOrder = [f"{level}è¿æ¿" for level in level_columns]
            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F'][:len(level_columns)]
            
            # æ„å»ºè¿”å›æ•°æ®ï¼Œå§‹ç»ˆåŒ…å«æ‚¬æµ®æ•°æ®
            return jsonify({
                "stackedAreaData": {
                    "data": data,          # æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒæ•°æ®
                    "keyOrder": keyOrder,  # è¿æ¿æ•°ç±»å‹çš„æ˜¾ç¤ºé¡ºåº
                    "colors": colors,      # æ¯ä¸ªè¿æ¿æ•°ç±»å‹çš„é¢œè‰²
                    "hoverData": hover_data  # é¼ æ ‡æ‚¬æµ®æ—¶æ˜¾ç¤ºçš„è‚¡ç¥¨åç§°åˆ—è¡¨
                },
                "xAxisValues": xAxisValues,  # æ¨ªè½´æ¿å—åç§°
                "tableData": table_data      # è¡¨æ ¼æ•°æ®ï¼ˆæ€»è‚¡ç¥¨æ•°ï¼‰
            })
            
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—è¿æ¿æ•°åˆ†å¸ƒå¤±è´¥: {e}")

    def process_sector_stacked_area_data(self):
        """æ¿å—å †å é¢ç§¯å›¾æ•°æ® - è°ƒç”¨ v2 æ–¹æ³•"""
        return self.process_today_plate_up_limit_distribution_v2()
    
    def process(self, method_name: str):
        """
        å¤„ç†è¯·æ±‚çš„ä¸»å…¥å£
        
        Args:
            method_name: æ–¹æ³•åç§°
            
        Returns:
            Flaskå“åº”å¯¹è±¡
        """
        try:
            # æ„å»ºç¼“å­˜å‚æ•°
            cache_params = self.build_cache_params(method=method_name)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å¤„ç†æ–¹æ³•
            if hasattr(self, method_name):
                method = getattr(self, method_name)
                
                # è·å–æºæ•°æ®ï¼ˆç”¨äºç¼“å­˜åˆ¤æ–­ï¼‰
                source_data = self.server._get_source_data_for_endpoint(f"/api/{method_name}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼“å­˜
                if self.should_use_cache(method_name, cache_params, source_data):
                    cached_response = self.response_cache.get_cached_response(method_name, cache_params)
                    if cached_response:
                        self.logger.info(f"è¿”å›market_reviewç¼“å­˜æ•°æ®: {method_name}")
                        return cached_response
                
                # æ‰§è¡Œæ–¹æ³•è·å–æ–°æ•°æ®
                self.logger.info(f"æ‰§è¡Œmarket_reviewæ–¹æ³•: {method_name}")
                result = method()
                
                # å­˜å‚¨åˆ°ç¼“å­˜
                self.store_cache(method_name, cache_params, source_data, result)
                
                return result
            else:
                return self.error_response(f"æ–¹æ³• {method_name} ä¸å­˜åœ¨", 404)
                
        except Exception as e:
            return self.error_response(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")
    
    def get_available_methods(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„å¤„ç†æ–¹æ³•"""
        methods = []
        for attr_name in dir(self):
            if not attr_name.startswith('_') and callable(getattr(self, attr_name)):
                if attr_name not in ['process', 'get_available_methods', 'get_request_params', 
                                   'build_cache_params', 'should_use_cache', 'store_cache', 'error_response']:
                    methods.append(attr_name)
        return methods
    
    # ===== ç¤ºä¾‹æ•°æ®å¤„ç†æ–¹æ³• =====
    # ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹æ–¹æ³•ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æˆ–æ·»åŠ æ–°æ–¹æ³•
    
    def chart1(self):
        """ç¤ºä¾‹å›¾è¡¨1æ•°æ®"""
        try:
            # TODO: æ ¹æ®æ‚¨çš„ä¸šåŠ¡éœ€æ±‚å®ç°å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘
            # è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°
            
            # è·å–è¯·æ±‚å‚æ•°
            params = self.get_request_params()
            
            # ä»æ•°æ®ç¼“å­˜åŠ è½½æ•°æ®
            # data = self.data_cache.load_data('your_data_file')
            
            # å¤„ç†æ•°æ®é€»è¾‘
            sample_data = {
                "title": "market_review - å›¾è¡¨1",
                "data": [
                    {"x": "2024-01", "y": 100},
                    {"x": "2024-02", "y": 120},
                    {"x": "2024-03", "y": 110}
                ],
                "timestamp": time.time(),
                "params": params
            }
            
            return jsonify(sample_data)
            
        except Exception as e:
            return self.error_response(f"è·å–chart1æ•°æ®å¤±è´¥: {str(e)}")
    
    def table1(self):
        """ç¤ºä¾‹è¡¨æ ¼1æ•°æ®"""
        try:
            # TODO: å®ç°è¡¨æ ¼æ•°æ®å¤„ç†é€»è¾‘
            
            params = self.get_request_params()
            
            sample_data = {
                "title": "market_review - è¡¨æ ¼1",
                "columns": ["åç§°", "æ•°å€¼", "å˜åŒ–"],
                "data": [
                    ["é¡¹ç›®1", 100, "+5%"],
                    ["é¡¹ç›®2", 200, "+3%"],
                    ["é¡¹ç›®3", 150, "-2%"]
                ],
                "timestamp": time.time(),
                "params": params
            }
            
            return jsonify(sample_data)
            
        except Exception as e:
            return self.error_response(f"è·å–table1æ•°æ®å¤±è´¥: {str(e)}")
    
    def config(self):
        """è·å–market_reviewé…ç½®ä¿¡æ¯"""
        try:
            config_data = {
                "processor_name": self.processor_name,
                "available_methods": self.get_available_methods(),
                "description": "å¤ç›˜é¡µé¢",
                "timestamp": time.time()
            }
            
            return jsonify(config_data)
            
        except Exception as e:
            return self.error_response(f"è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def get_sector_rankings_by_period(self, df, latest_date_str, days_list=[5, 10, 20], top_n=10):
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µå†…cumsumæ’åå‰Nçš„æ¿å—
        
        Args:
            df: åŒ…å«trade_date, name, changeåˆ—çš„DataFrame
            latest_date_str: æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼'YYYYMMDD'
            days_list: è¦ç»Ÿè®¡çš„å¤©æ•°åˆ—è¡¨ï¼Œé»˜è®¤[5, 10, 20]
            top_n: è¿”å›å‰Nåï¼Œé»˜è®¤10
            
        Returns:
            dict: å„æ—¶é—´æ®µçš„æ’åç»“æœ
        """
        ranking_results = {}
        
        for days in days_list:
            try:
                # è®¡ç®—å¼€å§‹æ—¥æœŸ
                period_start_str = get_trade_date_by_offset(latest_date_str, days)
                period_start = pd.to_datetime(period_start_str, format='%Y%m%d')
                latest_date = pd.to_datetime(latest_date_str, format='%Y%m%d')
                
                # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®
                period_df = df[(df['trade_date'] >= period_start) & (df['trade_date'] <= latest_date)]
                
                # é‡æ–°è®¡ç®—cumsumï¼ˆé¿å…ä½¿ç”¨å…¨å±€cumsumï¼‰
                period_df = period_df.copy()
                period_df = period_df.sort_values(['name', 'trade_date'])
                period_df['period_cumsum'] = period_df.groupby('name')['change'].cumsum()
                
                # è·å–æ¯ä¸ªæ¿å—åœ¨è¯¥æ—¶é—´æ®µçš„æœ€æ–°cumsumå€¼
                latest_cumsum = period_df.groupby('name')['period_cumsum'].last().reset_index()
                
                # æ’åºå¹¶è·å–å‰Nå
                top_sectors = latest_cumsum.sort_values('period_cumsum', ascending=False).head(top_n)
                
                ranking_results[f'è¿‘{days}æ—¥æ’å'] = [
                    {
                        'rank': idx + 1,
                        'sector_name': row['name'],
                        'cumsum_change': round(row['period_cumsum'], 4)
                    }
                    for idx, (_, row) in enumerate(top_sectors.iterrows())
                ]
                
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—è¿‘{days}æ—¥æ’åæ—¶å‡ºé”™: {e}")
                ranking_results[f'è¿‘{days}æ—¥æ’å'] = []
        
        return ranking_results
