"""
market_reviewå¤„ç†å™¨æ¨¡æ¿
Author: Auto-generated
Date: 2025-07-26
Description: å¤ç›˜é¡µé¢
"""
import os
from stock_data.factor.index.daily import FactorIndexDailyData
from stock_data.kpl.up_limit import KplUpLimitData
from stock_data.night_factor.daily import NightFactorData
from stock_data.sentiment.market.daily import MarketSentimentDailyData
from stock_data.stock.index_daily import IndexDailyData
from stock_data.stock.stock_daily import StockDailyData
from stock_data.stock_minute import StockMinuteData
from stock_data.ths.concept_data import ThsConceptData
from stock_data.ths.concept_index import ThsConceptIndexData
from utils.common import get_latest_stock_name_from_stock_id, get_trade_date_by_offset, get_trade_date_list, get_stock_name_code_list
from .base_processor import BaseDataProcessor
from flask import jsonify, request
import pandas as pd
import numpy as np
import time


class MarketReviewProcessor(BaseDataProcessor):
    """å¤ç›˜é¡µé¢æ•°æ®å¤„ç†å™¨"""
    
    def process_market_sentiment_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_sentiment_daily', self._original_market_sentiment_daily)
    
    def _original_market_sentiment_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = FactorIndexDailyData()
        df = d.get_daily_data(start_date='2025-03-01')
        # æˆäº¤é¢amountå˜ä¸ºä»¥äº¿ä¸ºå•ä½ï¼Œå¹¶ä¿ç•™2ä½å°æ•°
        df['amount'] = df['amount'] / 1e8
        df['amount'] = df['amount'].round(2)
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸå¹¶æŒ‰æ—¶é—´é¡ºåºæ’åºï¼Œç”¨äºå¼ºåˆ¶xè½´é¡ºåº
        all_dates = df['trade_date'].unique()
        all_dates = sorted(pd.to_datetime(all_dates))
        date_order = [date.strftime('%m/%d') for date in all_dates]
        
        market_list = ['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿', 'ST','å…¨å¸‚åœº']
        chart_data = []
        for market in market_list:
            market_data = df[df['name'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æˆäº¤é¢',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['amount'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„å¸‚åœºæˆäº¤é¢",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",
                    "categoryorder": "array",
                    "categoryarray": date_order
                },
                "yaxis": {"title": "æˆäº¤é¢(äº¿å…ƒ)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })
    
    def process_market_change_daily(self):
        """å„å¸‚åœºæ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_change_daily', self._original_market_change_daily)
    
    def _original_market_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = FactorIndexDailyData()
        df = d.get_daily_data(start_date='2025-03-01')
        # æˆäº¤é¢amountå˜ä¸ºä»¥äº¿ä¸ºå•ä½ï¼Œå¹¶ä¿ç•™2ä½å°æ•°
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸå¹¶æŒ‰æ—¶é—´é¡ºåºæ’åºï¼Œç”¨äºå¼ºåˆ¶xè½´é¡ºåº
        all_dates = df['trade_date'].unique()
        all_dates = sorted(pd.to_datetime(all_dates))
        date_order = [date.strftime('%m/%d') for date in all_dates]
        
        market_list = ['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ST','å…¨å¸‚åœº']
        # å¯¹nameåˆ—groupbyï¼Œè®¡ç®—changeåˆ—çš„cumsum
        df['change'] = df.groupby('name')['change'].cumsum()
        chart_data = []
        for market in market_list:
            market_data = df[df['name'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æ¶¨å¹…',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['change'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„å¸‚åœºæ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",
                    "categoryorder": "array",
                    "categoryarray": date_order
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })
    
    def process_plate_stocks_change_daily(self):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹… - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œæ—¥æœŸèŒƒå›´é€‰æ‹©"""
        # ä»è¯·æ±‚å‚æ•°ä¸­è·å–é€‰ä¸­çš„æ¿å—åç§°å’Œæ—¥æœŸèŒƒå›´
        from flask import request
        selected_sector = request.args.get('sector', None)
        start_date = request.args.get('startDate', '2025-07-01')  # é»˜è®¤å¼€å§‹æ—¥æœŸ
        end_date = request.args.get('endDate', None)  # é»˜è®¤ç»“æŸæ—¥æœŸä¸ºNoneï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸ
        
        # ç›´æ¥è°ƒç”¨åŸå§‹æ–¹æ³•ï¼Œä¸ä½¿ç”¨å¯åŠ¨ç¼“å­˜
        # å› ä¸ºå¯åŠ¨ç¼“å­˜ä¼šå¿½ç•¥sectorå’Œæ—¥æœŸå‚æ•°ï¼Œå¯¼è‡´é€‰æ‹©ä¸åŒå‚æ•°æ—¶è¿”å›ç›¸åŒæ•°æ®
        return self._original_plate_stocks_change_daily(selected_sector, start_date, end_date)
    
    def _original_plate_stocks_change_daily(self, selected_sector=None, start_date='2025-07-01', end_date=None):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹… - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œå…·ä½“æ—¥æœŸèŒƒå›´é€‰æ‹©"""
        
        d = ThsConceptIndexData()
        # ä½¿ç”¨ä¼ å…¥çš„å¼€å§‹æ—¥æœŸè·å–æ•°æ®
        df = d.get_daily_data(start_date=start_date)

        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        
        # ç¡®å®šå®é™…çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ
        if end_date is None or df['trade_date'].max() < pd.to_datetime(end_date, errors='coerce'):
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸ
            latest_date = df['trade_date'].max()
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
            latest_date = pd.to_datetime(end_date, errors='coerce')
        
        # å°†å¼€å§‹æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹
        start_date_dt = pd.to_datetime(start_date, errors='coerce')
        
        temp_df = df.copy()  # ä¿ç•™åŸå§‹æ•°æ®
        # ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        df = df[(df['trade_date'] >= start_date_dt) & (df['trade_date'] <= latest_date)]
        
        latest_date_str = latest_date.strftime('%Y%m%d')
        
        #è·å–è¿‘5æ—¥ï¼Œè¿‘10æ—¥ï¼Œè¿‘20æ—¥çš„cumsumå„æ’åå‰10çš„æ¿å—åˆ—è¡¨
        ranking_results = self.get_sector_rankings_by_period(temp_df, latest_date_str, [5, 10, 20], 10)
        

        # è·å–ranking_resultsçš„åˆ—è¡¨ï¼Œå¹¶åœ¨dfä¸­ç­›é€‰å‡ºè¿™äº›æ¿å—
        sector_names = [item['sector_name'] for sublist in ranking_results.values() for item in sublist]
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¿å—ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¸ªæ¿å—
        if selected_sector is None:
            sector_name = sector_names[0]
        else:
            sector_names.append(selected_sector)
            sector_name = selected_sector
        
        # å»é™¤é‡å¤çš„æ¿å—åç§°
        sector_names = list(set(sector_names))

        # è·å–æ—¥çº¿è‚¡ç¥¨æ•°æ® - ä½¿ç”¨ç›¸åŒçš„æ—¥æœŸèŒƒå›´
        d = StockDailyData()
        stock_df = d.get_daily_data(start_date=start_date)
        stock_df['trade_date'] = pd.to_datetime(stock_df['trade_date'], errors='coerce')
        
        # ç­›é€‰è‚¡ç¥¨æ•°æ®åˆ°ç›¸åŒçš„æ—¥æœŸèŒƒå›´
        stock_df = stock_df[(stock_df['trade_date'] >= start_date_dt) & (stock_df['trade_date'] <= latest_date)]
        
        # è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨
        d = ThsConceptData()
        concept_df = d.get_daily_data(start_date=start_date)
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        concept_latest_date = concept_df['trade_date'].max()
        concept_df = concept_df[concept_df['trade_date'] == concept_latest_date]
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = concept_df[concept_df['concept_name'] == sector_name]['stocks'].values[0].split(',')
        # å¹¶å˜æˆæ•´æ•°å‹
        stock_list = [int(stock) for stock in stock_list if stock.isdigit()]
        # è·å–stock_dfä¸­idåœ¨stock_listä¸­çš„æ•°æ®
        stock_df = stock_df[stock_df['id'].isin(stock_list)]
        # groupby idï¼Œè®¡ç®—changeåˆ—çš„cumsum
        stock_df['change_cumsum'] = stock_df.groupby('id')['change'].cumsum()
        stock_df['date_str'] = stock_df['trade_date'].dt.strftime('%m/%d')
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸå¹¶æŒ‰æ—¶é—´é¡ºåºæ’åºï¼Œç”¨äºå¼ºåˆ¶xè½´é¡ºåº
        all_dates = stock_df['trade_date'].unique()
        all_dates = sorted(pd.to_datetime(all_dates))
        date_order = [date.strftime('%m/%d') for date in all_dates]
        
        chart_data = []
        for stock_id in stock_list:
            stock_data = stock_df[stock_df['id'] == stock_id]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='trade_date')
                # è·å–è‚¡ç¥¨åç§° - å–ç¬¬ä¸€è¡Œçš„å€¼
                stock_name = stock_data['stock_name'].iloc[0]
                # ç»Ÿè®¡5æ—¥ï¼Œ10æ—¥ï¼Œ20æ—¥æ¶¨å¹…å¤§äº9.7çš„æ¬¡æ•°ï¼Œåˆ†åˆ«ç”¨up_limit_count5, up_limit_count_10,up_limit_count20
                x_data = stock_data['date_str'].tolist()
                y_data = stock_data['change_cumsum'].tolist()
                tmp_data = stock_data['change'].tolist()
                up_limit_count_5 = sum(1 for change in tmp_data[-5:] if change > 9.7)
                up_limit_count_10 = sum(1 for change in tmp_data[-10:] if change > 9.7)
                up_limit_count_15 = sum(1 for change in tmp_data[-15:] if change > 9.7)
                up_limit_count_20 = sum(1 for change in tmp_data[-20:] if change > 9.7)
                up_limit_count_20 = up_limit_count_20-up_limit_count_15
                up_limit_count_15 = up_limit_count_15-up_limit_count_10
                up_limit_count_10 = up_limit_count_10-up_limit_count_5


                chart_data.append({
                    "name": f'{stock_name}_{up_limit_count_20}-{up_limit_count_15}-{up_limit_count_10}-{up_limit_count_5}',
                    "x": x_data,
                    "y": y_data,
                    "mode": "lines+markers+text",  # æ·»åŠ  +text æ¨¡å¼
                    "line": {"width": 2},
                    "text": [f'{stock_name}â€”â€”{sector_name}' if i == len(x_data)-1 else '' for i in range(len(x_data))],
                    "textposition": "middle right",
                    "textfont": {"size": 10, "color": "black"},
                    "showlegend": True
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": f"æ¿å—å†…è‚¡ç¥¨æ—¥çº¿æ¶¨å¹… - {sector_name} ({start_date_dt.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')})",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",
                    "categoryorder": "array",
                    "categoryarray": date_order
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "è‚¡ç¥¨åç§°"}
            },
            "sectorInfo": {
                "currentSector": sector_name,
                "availableSectors": sector_names,
                "sectorCount": len(sector_names)
            },
            "dateRangeInfo": {
                "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                "currentEndDate": latest_date.strftime('%Y-%m-%d')
            }
        })
    
    def process_plate_stocks_change_minute(self):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹… - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œæ—¥æœŸèŒƒå›´é€‰æ‹©"""
        # ä»è¯·æ±‚å‚æ•°ä¸­è·å–é€‰ä¸­çš„æ¿å—åç§°å’Œæ—¥æœŸèŒƒå›´
        from flask import request
        selected_sector = request.args.get('sector', None)
        start_date = request.args.get('startDate', '2025-07-01')  # é»˜è®¤å¼€å§‹æ—¥æœŸ
        end_date = request.args.get('endDate', None)  # é»˜è®¤ç»“æŸæ—¥æœŸä¸ºNoneï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸ
        
        # ç›´æ¥è°ƒç”¨åŸå§‹æ–¹æ³•ï¼Œä¸ä½¿ç”¨å¯åŠ¨ç¼“å­˜
        # å› ä¸ºå¯åŠ¨ç¼“å­˜ä¼šå¿½ç•¥sectorå’Œæ—¥æœŸå‚æ•°ï¼Œå¯¼è‡´é€‰æ‹©ä¸åŒå‚æ•°æ—¶è¿”å›ç›¸åŒæ•°æ®
        return self._original_plate_stocks_change_minute(selected_sector, start_date, end_date)
    
    def _original_plate_stocks_change_minute(self, selected_sector=None, start_date='2025-07-01', end_date=None):
        """å„æ¿å—è‚¡ç¥¨æ¶¨å¹… - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œå…·ä½“æ—¥æœŸèŒƒå›´é€‰æ‹©"""
        # å°†å¼€å§‹æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹
        start_date_dt = pd.to_datetime(start_date, errors='coerce')

        d = ThsConceptIndexData()
        # ä½¿ç”¨ä¼ å…¥çš„å¼€å§‹æ—¥æœŸè·å–æ•°æ®
        df = d.get_daily_data(start_date=start_date)

        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        
        #ç¡®å®šå®é™…çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ
        if end_date is None or df['trade_date'].max() < pd.to_datetime(end_date, errors='coerce'):
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            latest_date = df['trade_date'].max()
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ç»“æŸæ—¥æœŸï¼Œä½†ä¸èƒ½è¶…è¿‡æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            end_date_dt = pd.to_datetime(end_date, errors='coerce')
            data_max_date = df['trade_date'].max()
            latest_date = min(end_date_dt, data_max_date)
        
        
        temp_df = df.copy()  # ä¿ç•™åŸå§‹æ•°æ®
        # ç­›é€‰æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        df = df[(df['trade_date'] >= start_date_dt) & (df['trade_date'] <= latest_date)]
        
        latest_date_str = latest_date.strftime('%Y%m%d')
        
        #è·å–è¿‘5æ—¥ï¼Œè¿‘10æ—¥ï¼Œè¿‘20æ—¥çš„cumsumå„æ’åå‰10çš„æ¿å—åˆ—è¡¨
        ranking_results = self.get_sector_rankings_by_period(temp_df, latest_date_str, [5, 10, 20], 10)
        
        # è·å–ranking_resultsçš„åˆ—è¡¨ï¼Œå¹¶åœ¨dfä¸­ç­›é€‰å‡ºè¿™äº›æ¿å—
        sector_names = [item['sector_name'] for sublist in ranking_results.values() for item in sublist]
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¿å—ï¼Œä½¿ç”¨é»˜è®¤çš„ç¬¬ä¸€ä¸ªæ¿å—
        if selected_sector is None :
            sector_name = sector_names[0]
        else:
            sector_name = selected_sector
            sector_names.append(selected_sector)
        
        # å»é™¤é‡å¤çš„æ¿å—åç§°
        sector_names = list(set(sector_names))

        # è·å–åˆ†é’Ÿçº¿è‚¡ç¥¨æ•°æ® - ä½¿ç”¨ç›¸åŒçš„æ—¥æœŸèŒƒå›´
        d = StockMinuteData()
        d.set_table_name()  # è®¾ç½®è¡¨å
        stock_df = d.get_minute_data_by_date(start_date=latest_date_str, end_date=latest_date_str) # åªå–latest_dateæ—¥çš„æ•°æ®
        stock_df['trade_date'] = pd.to_datetime(stock_df['trade_date'], errors='coerce')
        # timeåˆ—åº”è¯¥å·²ç»åŒ…å«æ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯ï¼Œç¡®ä¿å…¶ä¸ºdatetimeç±»å‹
        stock_df['time'] = pd.to_datetime(stock_df['time'], errors='coerce')
        
        # ç­›é€‰è‚¡ç¥¨æ•°æ®åˆ°ç›¸åŒçš„æ—¥æœŸèŒƒå›´
        # stock_df = stock_df[(stock_df['trade_date'] >= start_date_dt) & (stock_df['trade_date'] <= latest_date)]
        
        # è·å–æ¿å—å†…è‚¡ç¥¨åˆ—è¡¨
        d = ThsConceptData()
        concept_df = d.get_daily_data(start_date=start_date)
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        concept_latest_date = concept_df['trade_date'].max()
        concept_df = concept_df[concept_df['trade_date'] == concept_latest_date]
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_list = concept_df[concept_df['concept_name'] == sector_name]['stocks'].values[0].split(',')
        # å¹¶å˜æˆæ•´æ•°å‹
        stock_list = [int(stock) for stock in stock_list if stock.isdigit()]
        # è·å–stock_dfä¸­idåœ¨stock_listä¸­çš„æ•°æ®
        stock_df = stock_df[stock_df['id'].isin(stock_list)]
        stock_df['change'] = ((stock_df['close']- stock_df['pre_close'])/ stock_df['pre_close'] *100).round(2) # è®¡ç®—æ¯åˆ†é’Ÿçš„æ¶¨å¹…
        
        stock_df['time'] = pd.to_datetime(stock_df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        stock_df = stock_df[stock_df['time'].dt.strftime('%H:%M') >= '09:30']
        stock_df['time_str'] = stock_df['time'].dt.strftime('%m%d_%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        stock_df['pre_close'] = stock_df['pre_close'].replace(0, 1)
        
        stock_df = get_latest_stock_name_from_stock_id(stock_df)
        # stock_df['date_str'] = stock_df['trade_date'].dt.strftime('%m/%d')
        
        # è·å–æ‰€æœ‰å”¯ä¸€çš„æ—¥æœŸå¹¶æŒ‰æ—¶é—´é¡ºåºæ’åºï¼Œç”¨äºå¼ºåˆ¶xè½´é¡ºåº
        all_dates = stock_df['time'].unique()
        all_dates = sorted(pd.to_datetime(all_dates))
        date_order = [date.strftime('%m%d_%H:%M') for date in all_dates]
        
        chart_data = []
        for stock_id in stock_list:
            stock_data = stock_df[stock_df['id'] == stock_id]
            
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='time')
                # è·å–è‚¡ç¥¨åç§° - å–ç¬¬ä¸€è¡Œçš„å€¼
                stock_name = stock_data['stock_name'].iloc[0]
                
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                chart_data.append({
                    "name": f'{stock_name}â€”â€”{sector_name}',
                    "x": x_data,
                    "y": y_data,
                    "mode": "lines+markers+text",  # æ·»åŠ  +text æ¨¡å¼
                    "line": {"width": 2},
                    "text": [f'{stock_name}' if i == len(x_data)-1 else '' for i in range(len(x_data))],
                    "textposition": "middle right",
                    "textfont": {"size": 10, "color": "black"},
                    "showlegend": True
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": f"æ¿å—å†…è‚¡ç¥¨åˆ†é’Ÿæ¶¨å¹… - {sector_name} ({start_date_dt.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')})",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",
                    "categoryorder": "array",
                    "categoryarray": date_order
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "è‚¡ç¥¨åç§°"}
            },
            "sectorInfo": {
                "currentSector": sector_name,
                "availableSectors": sector_names,
                "sectorCount": len(sector_names)
            },
            "dateRangeInfo": {
                "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                "currentEndDate": latest_date.strftime('%Y-%m-%d')
            }
        })
    
    def process_plate_change_daily(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_change_daily', self._original_plate_change_daily)
    
    def process_sector_lianban_distribution(self):
        """æ¿å—è¿æ¿æ•°åˆ†å¸ƒå›¾ - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œæ—¥æœŸèŒƒå›´é€‰æ‹©"""
        from flask import request
        selected_sector = request.args.get('sector', None)
        start_date = request.args.get('startDate', '2025-07-01')
        end_date = request.args.get('endDate', None)
        
        return self._original_sector_lianban_distribution(selected_sector, start_date, end_date)
    
    def _original_sector_lianban_distribution(self, selected_sector=None, start_date='2025-07-01', end_date=None):
        """æ¿å—è¿æ¿æ•°åˆ†å¸ƒå›¾ - åŸå§‹å®ç°"""
        import pandas as pd
        from datetime import datetime, timedelta
        
        # å°†å¼€å§‹æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹
        start_date_dt = pd.to_datetime(start_date, errors='coerce')
        d = NightFactorData()
        df = d.get_daily_data(start_date=start_date)
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')

        # ç¡®å®šç»“æŸæ—¥æœŸ
        # ç¡®å®šå®é™…çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ
        if end_date is None or df['trade_date'].max() < pd.to_datetime(end_date, errors='coerce'):
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸ
            latest_date = df['trade_date'].max()
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
            latest_date = pd.to_datetime(end_date, errors='coerce')
        
        # TODO: è¿™é‡Œä½ éœ€è¦å®ç°ä» factor.night_factor è¡¨è·å–æ•°æ®çš„é€»è¾‘
        # æœŸæœ›çš„æ•°æ®ç»“æ„åº”è¯¥åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        # - trade_date: äº¤æ˜“æ—¥æœŸ
        # - sector: æ¿å—åç§°  
        # - stock_name: è‚¡ç¥¨åç§°
        # - lian_ban_shu: è¿æ¿æ•°
        
        df = df[(df['trade_date'] >= start_date_dt) & (df['trade_date'] <= latest_date)]
        
        
        # è·å–å¯ç”¨æ¿å—åˆ—è¡¨ - ä½ éœ€è¦æ›¿æ¢ä¸ºå®é™…çš„æ¿å—è·å–é€»è¾‘
        available_sectors = df['sector'].unique().tolist()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¿å—ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿å—
        if selected_sector is None:
            selected_sector = available_sectors[0] if available_sectors else "æ— æ¿å—"
        
        # ç­›é€‰æŒ‡å®šæ¿å—çš„æ•°æ®
        sector_data = df[df['sector'] == selected_sector].copy()

        if sector_data.empty:
            return jsonify({
                "chartType": "heatmap", 
                "data": [],
                "layout": {"title": f"æ¿å— {selected_sector} æ— è¿æ¿æ•°æ®"},
                "sectorInfo": {
                    "currentSector": selected_sector,
                    "availableSectors": available_sectors,
                    "sectorCount": len(available_sectors)
                },
                "dateRangeInfo": {
                    "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                    "currentEndDate": latest_date.strftime('%Y-%m-%d')
                }
            })
        
        # å¤„ç†è¿æ¿æ•°æ®
        lianban_data = self._process_lianban_data_for_heatmap(sector_data)
        
        # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        chart_data = self._generate_lianban_heatmap_chart(lianban_data, start_date_dt, latest_date)
        
        return jsonify({
            "chartType": "heatmap",
            "data": chart_data["data"],
            "layout": {
                "title": f"æ¿å—è¿æ¿æ•°åˆ†å¸ƒ - {selected_sector} ({start_date_dt.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')})",
                "xaxis": {
                    "title": "æ—¥æœŸ", 
                    "side": "bottom",
                    "tickangle": -45,
                    "automargin": True,
                    "type": "category",  # æ˜ç¡®æŒ‡å®šä¸ºåˆ†ç±»è½´
                    "categoryorder": "array",  # æŒ‰æ•°ç»„é¡ºåºæ’åˆ—
                    "categoryarray": chart_data.get("date_strings", [])  # ä½¿ç”¨è¿”å›çš„æ—¥æœŸå­—ç¬¦ä¸²æ•°ç»„
                },
                "yaxis": {
                    "title": "è¿æ¿æ•°", 
                    "side": "left"
                },
                "colorscale": "Viridis",
                "showscale": True,
                "height": 1000,  # å¢å¤§å›ºå®šé«˜åº¦ä»¥å®¹çº³åŠ¨æ€è¡Œé«˜
                "margin": {"l": 120, "r": 50, "t": 80, "b": 120},
                **chart_data["layout"]
            },
            "sectorInfo": {
                "currentSector": selected_sector,
                "availableSectors": available_sectors,
                "sectorCount": len(available_sectors)
            },
            "dateRangeInfo": {
                "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                "currentEndDate": latest_date.strftime('%Y-%m-%d')
            }
        })
    
    def process_sector_lianban_distribution_kpl(self):
        """æ¿å—è¿æ¿æ•°åˆ†å¸ƒå›¾ - æ”¯æŒåŠ¨æ€æ¿å—é€‰æ‹©å’Œæ—¥æœŸèŒƒå›´é€‰æ‹©"""
        from flask import request
        selected_sector = request.args.get('sector', None)
        start_date = request.args.get('startDate', '2025-07-01')
        end_date = request.args.get('endDate', None)
        
        return self._original_sector_lianban_distribution_kpl(selected_sector, start_date, end_date)

    def _original_sector_lianban_distribution_kpl(self, selected_sector=None, start_date='2025-07-01', end_date=None):
        """æ¿å—è¿æ¿æ•°åˆ†å¸ƒå›¾ - åŸå§‹å®ç°"""
        import pandas as pd
        from datetime import datetime, timedelta
        
        # å°†å¼€å§‹æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹
        start_date_dt = pd.to_datetime(start_date, errors='coerce')
        # d = NightFactorData()
        d = KplUpLimitData()
        df = d.get_daily_data(start_date=start_date)
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')

        # ç¡®å®šç»“æŸæ—¥æœŸ
        # ç¡®å®šå®é™…çš„å¼€å§‹å’Œç»“æŸæ—¥æœŸ
        if end_date is None or df['trade_date'].max() < pd.to_datetime(end_date, errors='coerce'):
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç»“æŸæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸ
            latest_date = df['trade_date'].max()
        else:
            # ä½¿ç”¨æŒ‡å®šçš„ç»“æŸæ—¥æœŸ
            latest_date = pd.to_datetime(end_date, errors='coerce')
        
        # TODO: è¿™é‡Œä½ éœ€è¦å®ç°ä» factor.night_factor è¡¨è·å–æ•°æ®çš„é€»è¾‘
        # æœŸæœ›çš„æ•°æ®ç»“æ„åº”è¯¥åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
        # - trade_date: äº¤æ˜“æ—¥æœŸ
        # - sector: æ¿å—åç§°  
        # - stock_name: è‚¡ç¥¨åç§°
        # - lian_ban_shu: è¿æ¿æ•°
        
        df = df[(df['trade_date'] >= start_date_dt) & (df['trade_date'] <= latest_date)]
        
        
        # è·å–å¯ç”¨æ¿å—åˆ—è¡¨ - ä½ éœ€è¦æ›¿æ¢ä¸ºå®é™…çš„æ¿å—è·å–é€»è¾‘
        available_sectors = df['up_limit_reason'].unique().tolist()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¿å—ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ¿å—
        if selected_sector is None:
            selected_sector = available_sectors[0] if available_sectors else "æ— æ¿å—"
        
        # ç­›é€‰æŒ‡å®šæ¿å—çš„æ•°æ®
        sector_data = df[df['up_limit_reason'] == selected_sector].copy()

        if sector_data.empty:
            return jsonify({
                "chartType": "heatmap", 
                "data": [],
                "layout": {"title": f"æ¿å— {selected_sector} æ— è¿æ¿æ•°æ®"},
                "sectorInfo": {
                    "currentSector": selected_sector,
                    "availableSectors": available_sectors,
                    "sectorCount": len(available_sectors)
                },
                "dateRangeInfo": {
                    "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                    "currentEndDate": latest_date.strftime('%Y-%m-%d')
                }
            })
        
        # å¤„ç†è¿æ¿æ•°æ®
        lianban_data = self._process_lianban_data_for_heatmap(sector_data, kpl=True)
        
        # ç”Ÿæˆçƒ­åŠ›å›¾æ•°æ®
        chart_data = self._generate_lianban_heatmap_chart(lianban_data, start_date_dt, latest_date)
        
        return jsonify({
            "chartType": "heatmap",
            "data": chart_data["data"],
            "layout": {
                "title": f"æ¿å—è¿æ¿æ•°åˆ†å¸ƒ - {selected_sector} ({start_date_dt.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')})",
                "xaxis": {
                    "title": "æ—¥æœŸ", 
                    "side": "bottom",
                    "tickangle": -45,
                    "automargin": True,
                    "type": "category",  # æ˜ç¡®æŒ‡å®šä¸ºåˆ†ç±»è½´
                    "categoryorder": "array",  # æŒ‰æ•°ç»„é¡ºåºæ’åˆ—
                    "categoryarray": chart_data.get("date_strings", [])  # ä½¿ç”¨è¿”å›çš„æ—¥æœŸå­—ç¬¦ä¸²æ•°ç»„
                },
                "yaxis": {
                    "title": "è¿æ¿æ•°", 
                    "side": "left"
                },
                "colorscale": "Viridis",
                "showscale": True,
                "height": 1000,  # å¢å¤§å›ºå®šé«˜åº¦ä»¥å®¹çº³åŠ¨æ€è¡Œé«˜
                "margin": {"l": 120, "r": 50, "t": 80, "b": 120},
                **chart_data["layout"]
            },
            "sectorInfo": {
                "currentSector": selected_sector,
                "availableSectors": available_sectors,
                "sectorCount": len(available_sectors)
            },
            "dateRangeInfo": {
                "currentStartDate": start_date_dt.strftime('%Y-%m-%d'),
                "currentEndDate": latest_date.strftime('%Y-%m-%d')
            }
        })
    
    def _process_lianban_data_for_heatmap(self, sector_data,kpl=False):
        """å¤„ç†è¿æ¿æ•°æ®ï¼ŒæŒ‰æ—¥æœŸå’Œè¿æ¿æ•°åˆ†ç»„"""
        lianban_dict = {}
        
        for _, row in sector_data.iterrows():
            date_str = row['trade_date'].strftime('%m%d')  # æ”¹ä¸º0701æ ¼å¼
            if kpl:
                lianban_days = int(row.get('up_limit_count', 1))
            else:
                lianban_days = int(row.get('lian_ban_shu', 1))  # ä»night_factorè¡¨çš„lian_ban_shuå­—æ®µè·å–
            stock_name = row.get('stock_name', 'æœªçŸ¥è‚¡ç¥¨')
            
            if date_str not in lianban_dict:
                lianban_dict[date_str] = {}
            
            # å°†10è¿æ¿ä»¥ä¸Šçš„å½’ä¸º"10è¿æ¿ä»¥ä¸Š"
            if lianban_days >= 10:
                lianban_key = "10è¿æ¿ä»¥ä¸Š"
            else:
                lianban_key = f"{lianban_days}è¿æ¿"
            
            if lianban_key not in lianban_dict[date_str]:
                lianban_dict[date_str][lianban_key] = []
            
            lianban_dict[date_str][lianban_key].append(stock_name)
        
        return lianban_dict
    
    def _generate_lianban_heatmap_chart(self, lianban_data, start_date, end_date):
        """ç”Ÿæˆè¿æ¿åˆ†å¸ƒçƒ­åŠ›å›¾æ•°æ® - æ”¯æŒåŠ¨æ€è¡Œé«˜"""
        # å®šä¹‰è¿æ¿æ•°ç±»åˆ« - é¡ºåºä»1è¿æ¿åˆ°10è¿æ¿ä»¥ä¸Šï¼ˆ1è¿æ¿åœ¨æœ€ä¸‹æ–¹ï¼‰
        lianban_categories = [f"{i}è¿æ¿" for i in range(1, 10)] + ["10è¿æ¿ä»¥ä¸Š"]
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ— - åªåŒ…å«å·¥ä½œæ—¥
        date_range = pd.date_range(start=start_date, end=end_date, freq='B')  # 'B'è¡¨ç¤ºå·¥ä½œæ—¥
        date_strings = [d.strftime('%m%d') for d in date_range]  # æ”¹ä¸º0701æ ¼å¼
        
        # åè½¬è¿æ¿ç±»åˆ«é¡ºåºï¼Œè®©1è¿æ¿åœ¨ä¸‹æ–¹
        reversed_categories = list(reversed(lianban_categories))
        
        # è®¡ç®—æ¯ä¸ªè¿æ¿ç±»å‹åœ¨æ‰€æœ‰æ—¥æœŸä¸­çš„æœ€å¤§è‚¡ç¥¨æ•°é‡ï¼Œç”¨äºåŠ¨æ€é«˜åº¦åˆ†é…
        max_stocks_per_category = {}
        for lianban_cat in lianban_categories:
            max_count = 0
            for date_str in date_strings:
                stocks = lianban_data.get(date_str, {}).get(lianban_cat, [])
                max_count = max(max_count, len(stocks))
            # ç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰æœ€å°é«˜åº¦ï¼ˆå¯å®¹çº³1åªè‚¡ç¥¨ï¼‰
            max_stocks_per_category[lianban_cat] = max(max_count, 1)
        
        # è®¡ç®—åŠ¨æ€yåæ ‡ä½ç½® - åŸºäºæœ€å¤§è‚¡ç¥¨æ•°é‡åˆ†é…é«˜åº¦
        y_positions = {}
        current_y = 0
        base_height = 1.2  # è¿›ä¸€æ­¥å¢åŠ åŸºç¡€é«˜åº¦
        stock_height = 1  # å¤§å¹…å¢åŠ æ¯åªè‚¡ç¥¨çš„é«˜åº¦ï¼Œç¡®ä¿10åªè‚¡ç¥¨æ—¶æœ‰è¶³å¤Ÿç©ºé—´
        
        # æŒ‰ç…§æ­£åºæ’åˆ—ï¼ˆ1è¿æ¿åœ¨ä¸‹æ–¹ï¼‰ï¼Œä»ä¸‹å¾€ä¸Šè®¡ç®—yåæ ‡
        for lianban_cat in lianban_categories:  # ä½¿ç”¨åŸå§‹é¡ºåºï¼Œ1è¿æ¿å…ˆè®¡ç®—ï¼ˆåœ¨ä¸‹æ–¹ï¼‰
            max_stocks = max_stocks_per_category[lianban_cat]
            # è®¡ç®—è¯¥ç±»åˆ«æ‰€éœ€çš„æ€»é«˜åº¦ï¼ˆåŸºç¡€é«˜åº¦ + è‚¡ç¥¨æ•°é‡*å•è‚¡é«˜åº¦ï¼‰
            category_height = base_height + max_stocks * stock_height
            y_positions[lianban_cat] = current_y + category_height / 2  # ä¸­å¿ƒä½ç½®
            current_y += category_height
        
        total_height = current_y
        
        # åˆ›å»ºçŸ©å½¢traceså’Œæ³¨é‡Š
        traces = []
        annotations = []
        
        for i, date_str in enumerate(date_strings):
            for lianban_cat in lianban_categories:
                stocks = lianban_data.get(date_str, {}).get(lianban_cat, [])
                stock_count = len(stocks)
                
                # ä¸ºæ‰€æœ‰æ ¼å­åˆ›å»ºçŸ©å½¢ï¼ŒåŒ…æ‹¬æ²¡æœ‰æ•°æ®çš„æ ¼å­
                # è®¡ç®—é¢œè‰²å¼ºåº¦ï¼ˆåŸºäºè‚¡ç¥¨æ•°é‡ï¼‰
                if stock_count > 0:
                    max_count_overall = max([len(stocks) for date_data in lianban_data.values() 
                                           for stocks in date_data.values()]) or 1
                    intensity = stock_count / max_count_overall
                    
                    # ç™½è‰²åˆ°çº¢è‰²æ¸å˜
                    red_value = int(255 * intensity)
                    color = f"rgb({255}, {255-red_value}, {255-red_value})"
                else:
                    # æ²¡æœ‰æ•°æ®çš„æ ¼å­æ˜¾ç¤ºä¸ºç™½è‰²
                    color = "rgb(255, 255, 255)"
                    intensity = 0
                
                # è®¡ç®—çŸ©å½¢çš„ä½ç½®å’Œå¤§å°
                center_y = y_positions[lianban_cat]
                max_stocks = max_stocks_per_category[lianban_cat]
                
                # æ ¹æ®å®é™…è‚¡ç¥¨æ•°é‡è®¡ç®—å½“å‰æ ¼å­çš„é«˜åº¦
                current_stock_count = len(stocks)
                if current_stock_count == 0:
                    # ç©ºæ ¼å­ä½¿ç”¨æœ€å°é«˜åº¦
                    actual_rect_height = base_height
                else:
                    # æœ‰è‚¡ç¥¨çš„æ ¼å­æ ¹æ®å®é™…è‚¡ç¥¨æ•°é‡è®¡ç®—é«˜åº¦
                    actual_rect_height = base_height + current_stock_count * stock_height
                
                # æœ€å¤§é«˜åº¦é™åˆ¶ï¼ˆä¸è¶…è¿‡åˆ†é…ç»™è¯¥ç±»åˆ«çš„æ€»é«˜åº¦ï¼‰
                max_allowed_height = base_height + max_stocks * stock_height
                actual_rect_height = min(actual_rect_height, max_allowed_height)
                
                y_bottom = center_y - actual_rect_height / 2
                y_top = center_y + actual_rect_height / 2
                
                # åˆ›å»ºæ‚¬æµ®æ¡†æ¨¡æ¿ - è‚¡ç¥¨åˆ—è¡¨çºµå‘æ’åˆ—
                if stock_count == 0:
                    hover_template = f'%{{text}}<extra></extra>'
                    hover_text = f'{lianban_cat}<br>æ—¥æœŸ: 20{date_str}<br>æš‚æ— è‚¡ç¥¨'
                else:
                    # æ„å»ºè‚¡ç¥¨åˆ—è¡¨æ˜¾ç¤º - çºµå‘æ’åˆ—ï¼Œæ¯ä¸ªè‚¡ç¥¨ä¸€è¡Œ
                    stock_list = '<br>'.join([f'â€¢ {stock}' for stock in stocks])
                    hover_template = f'%{{text}}<extra></extra>'
                    hover_text = f'{lianban_cat}<br>æ—¥æœŸ: 20{date_str}<br>è‚¡ç¥¨æ•°é‡: {stock_count}åª<br><br>è‚¡ç¥¨åˆ—è¡¨:<br>{stock_list}'
                
                # åˆ›å»ºçŸ©å½¢trace - æ·»åŠ æ‚¬æµ®æ¡†æ ·å¼é…ç½®
                traces.append({
                    'type': 'scatter',
                    'x': [i-0.4, i+0.4, i+0.4, i-0.4, i-0.4],
                    'y': [y_bottom, y_bottom, y_top, y_top, y_bottom],
                    'mode': 'lines',
                    'fill': 'toself',
                    'fillcolor': color,
                    'line': {'color': '#ddd', 'width': 1},
                    'showlegend': False,
                    'name': f'{lianban_cat}-{date_str}',
                    'text': hover_text,
                    'hovertemplate': hover_template,
                    'hoverlabel': {
                        'bgcolor': 'rgba(30, 30, 30, 0.9)',  # æ·±ç°è‰²åŠé€æ˜èƒŒæ™¯
                        'bordercolor': 'white',               # ç™½è‰²è¾¹æ¡†
                        'font': {
                            'color': 'white',                 # ç™½è‰²å­—ä½“
                            'size': 14,                       # å­—ä½“å¤§å°
                            'family': 'Arial, sans-serif'    # å­—ä½“å®¶æ—
                        }
                    },
                    'customdata': stocks  # ä¼ é€’åŸå§‹è‚¡ç¥¨åˆ—è¡¨æ•°ç»„
                })
                
                # æ·»åŠ æ–‡æœ¬æ³¨é‡Šï¼ˆåªæœ‰å½“æœ‰è‚¡ç¥¨æ—¶æ‰æ·»åŠ è‚¡ç¥¨åç§°ï¼‰
                if stocks:
                    # æ ¹æ®çŸ©å½¢é«˜åº¦å’Œè‚¡ç¥¨æ•°é‡åŠ¨æ€è°ƒæ•´å­—ä½“å¤§å°
                    # è€ƒè™‘è‚¡ç¥¨æ•°é‡ï¼šè‚¡ç¥¨è¶Šå¤šï¼Œå­—ä½“ç›¸å¯¹å°ä¸€äº›ä»¥é¿å…æ‹¥æŒ¤
                    stock_count_factor = max(0.7, 1.0 - (len(stocks) - 1) * 0.05)  # è‚¡ç¥¨æ•°é‡è¶Šå¤šï¼Œå­—ä½“ç¨å°
                    base_font_size = min(16, max(13, int(actual_rect_height * 8)))
                    font_size = int(base_font_size * stock_count_factor)
                    stock_text = '<br>'.join(stocks)
                    
                    annotations.append({
                        'x': i,
                        'y': center_y,
                        'text': stock_text,
                        'showarrow': False,
                        'font': {
                            'size': font_size,
                            'color': 'black' if intensity < 0.5 else 'white',
                            'family': 'Arial, sans-serif'  # æ·»åŠ å­—ä½“æ—ä»¥æé«˜å¯è¯»æ€§
                        },
                        'xanchor': 'center',
                        'yanchor': 'middle'
                    })
        
        return {
            "data": traces,
            "layout": {
                "plot_bgcolor": 'white',
                "paper_bgcolor": 'white',
                "margin": {"l": 100, "r": 50, "t": 50, "b": 80},
                "height": max(600, int(total_height * 100)),  # åŠ¨æ€è°ƒæ•´æ€»é«˜åº¦
                "xaxis": {
                    "title": "æ—¥æœŸ",
                    "tickangle": -45,
                    "tickmode": "array",
                    "tickvals": list(range(len(date_strings))),
                    "ticktext": date_strings,
                    "range": [-0.5, len(date_strings)-0.5]
                },
                "yaxis": {
                    "title": "è¿æ¿ç±»å‹",
                    "tickmode": "array",
                    "tickvals": [y_positions[cat] for cat in lianban_categories],  # ä½¿ç”¨åŸå§‹é¡ºåº
                    "ticktext": lianban_categories,  # ä½¿ç”¨åŸå§‹é¡ºåºï¼Œ1è¿æ¿åœ¨ä¸‹æ–¹
                    "range": [0, total_height],
                    "showgrid": True,
                    "gridcolor": "#f0f0f0"
                },
                "annotations": annotations,
                "showlegend": False
            },
            "date_strings": date_strings  # è¿”å›æ—¥æœŸå­—ç¬¦ä¸²æ•°ç»„
        }

    def _original_plate_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = ThsConceptIndexData()
        df = d.get_daily_data(start_date='2025-07-01')

        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        #è·å–æœ€æ–°æ—¥æœŸ
        latest_date = df['trade_date'].max()
        latest_date_str = latest_date.strftime('%Y%m%d')
        start_date_str = get_trade_date_by_offset(latest_date_str, 20)
        # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdatetimeç±»å‹ä»¥ä¾¿æ¯”è¾ƒ
        start_date = pd.to_datetime(start_date_str, format='%Y%m%d')
        # è·å–ä»start_dateåˆ°latest_dateçš„æ‰€æœ‰æ•°æ®
        df = df[(df['trade_date'] >= start_date) & (df['trade_date'] <= latest_date)]

        
        #è·å–è¿‘5æ—¥ï¼Œè¿‘10æ—¥ï¼Œè¿‘20æ—¥çš„cumsumå„æ’åå‰10çš„æ¿å—åˆ—è¡¨
        ranking_results = self.get_sector_rankings_by_period(df, latest_date_str, [5, 10, 20], 10)
        
        print(f"ğŸ“Š æ¿å—æ’åç»Ÿè®¡:")
        for period_key in ranking_results:
            if ranking_results[period_key]:  # ç¡®ä¿æœ‰æ•°æ®
                top_3_names = [item['sector_name'] for item in ranking_results[period_key][:3]]
                print(f"   {period_key}å‰3: {top_3_names}")

        # è·å–ranking_resultsçš„åˆ—è¡¨ï¼Œå¹¶åœ¨dfä¸­ç­›é€‰å‡ºè¿™äº›æ¿å—
        sector_names = [item['sector_name'] for sublist in ranking_results.values() for item in sublist]
        # å»é™¤é‡å¤çš„æ¿å—åç§°
        sector_names = list(set(sector_names))
        
        print(f"ğŸ” ç­›é€‰å‰æ•°æ®èŒƒå›´: {df['date_str'].min()} åˆ° {df['date_str'].max()}")
        print(f"ğŸ” ç­›é€‰å‰æ€»æ¿å—æ•°: {df['name'].nunique()}")
        print(f"ğŸ” æ’åæ¿å—æ•°: {len(sector_names)}")
        
        df = df[df['name'].isin(sector_names)]
        
        print(f"ğŸ” ç­›é€‰åæ•°æ®èŒƒå›´: {df['date_str'].min()} åˆ° {df['date_str'].max()}")
        print(f"ğŸ” ç­›é€‰åæ€»è®°å½•æ•°: {len(df)}")
        
        # æ£€æŸ¥æ¯ä¸ªæ¿å—çš„æ•°æ®èŒƒå›´
        for sector in sector_names[:3]:  # åªæ£€æŸ¥å‰3ä¸ª
            sector_data = df[df['name'] == sector]
            if not sector_data.empty:
                print(f"ğŸ” æ¿å— '{sector}' æ•°æ®èŒƒå›´: {sector_data['date_str'].min()} åˆ° {sector_data['date_str'].max()}")
        
        # groupby nameï¼Œè®¡ç®—changeåˆ—çš„cumsum
        df['change'] = df.groupby('name')['change'].cumsum()

        
        chart_data = []
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['date_str'].unique())
        print(f"ğŸ“… å®Œæ•´æ—¥æœŸèŒƒå›´: {all_dates}")
        
        for sector in sector_names:
            sector_data = df[df['name'] == sector]
            if not sector_data.empty:
                sector_data = sector_data.sort_values(by='trade_date')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = sector_data['date_str'].tolist()
                y_data = sector_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = sector_data[['date_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{sector}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['trade_date'].strftime('%Y-%m-%d') for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
                
                print(f"ğŸ“Š æ¿å— '{sector}': æ•°æ®èŒƒå›´ {sector_data['date_str'].min()} åˆ° {sector_data['date_str'].max()}, å…±{len(sector_data)}å¤©")
                
                # è¯¦ç»†æ£€æŸ¥å‰å‡ ä¸ªæ¿å—çš„xè½´æ•°æ®é¡ºåº
                if sector in sector_names[:2]:  # åªæ£€æŸ¥å‰2ä¸ªæ¿å—
                    print(f"ğŸ” æ¿å— '{sector}' xè½´æ•°æ®é¡ºåº: {x_data}")
                    print(f"ğŸ” æ¿å— '{sector}' å‰5ä¸ªyå€¼: {y_data[:5]}")
        
        # æ£€æŸ¥æ€»çš„chart_dataç»“æ„
        print(f"ğŸ“ˆ ç”Ÿæˆå›¾è¡¨æ•°æ®: å…±{len(chart_data)}ä¸ªç³»åˆ—")
        if chart_data:
            first_series = chart_data[0]
            print(f"ğŸ” ç¬¬ä¸€ä¸ªç³»åˆ— '{first_series['name']}' xè½´: {first_series['x']}")
        
        
        
        
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })
    
    def process_market_stocks_change_daily(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily', self._original_market_stocks_change_daily)

    def _original_market_stocks_change_daily(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df[df['time_str'] == '15:00']
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['change'] > 5]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
        bottom_stocks = tempdf[tempdf['change'] < -5]['id'].unique().tolist()
        # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
        all_stocks = list(set(top_stocks + bottom_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })

    def process_market_stocks_change_daily_uplimit(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily_uplimit', self._original_market_stocks_change_daily_uplimit)


    def _original_market_stocks_change_daily_uplimit(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df[df['time_str'] == '15:00']
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['change'] > 9.7]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
        bottom_stocks = tempdf[tempdf['change'] < -9.7]['id'].unique().tolist()
        # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
        all_stocks = list(set(top_stocks + bottom_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            if not stock_data.empty:
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })

    def process_market_stocks_change_daily_speed(self):
        """å„æ¿å—æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/market_stocks_change_daily_speed', self._original_market_stocks_change_daily_speed)


    def _original_market_stocks_change_daily_speed(self):
        """å¸‚åœºæƒ…ç»ªæ—¥æ•°æ®çš„ä¸»æ¿ï¼Œåˆ›ä¸šæ¿ï¼Œç§‘åˆ›ç‰ˆï¼ŒSTæ¿æˆäº¤é¢"""
        d = StockMinuteData()
        # è·å–æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
        trade_date_list = get_trade_date_list()
        latest_date = trade_date_list[-1] 
        latest_date = '20250807' # chen for test
        df = d.get_minute_data_by_date(start_date=latest_date)

        df['time'] = pd.to_datetime(df['time'], errors='coerce')
        # å»é™¤09:30ä¹‹å‰çš„æ•°æ®
        df = df[df['time'].dt.strftime('%H:%M') >= '09:30']
        df['time_str'] = df['time'].dt.strftime('%H:%M')
        # å¦‚æœpre_closeåˆ—æ²¡æœ‰å€¼æˆ–ä¸º0ï¼Œå¡«å……pre_closeåˆ—çš„å€¼ä¸º1
        df['pre_close'] = df['pre_close'].replace(0, 1)
        # fillna pre_close with 1 if it is NaN
        df['pre_close'] = df['pre_close'].fillna(1)
        # æŒ‰id groupby,æ–°å¢åˆ—changeï¼Œè®¡ç®—closeåˆ—å¯¹pre_closeåˆ—çš„ç™¾åˆ†æ¯”å˜åŒ–ï¼Œå¹¶ä¹˜ä»¥100åä¿ç•™2ä½å°æ•°
        df['change'] = ((df['close'] - df['pre_close']) / df['pre_close'] * 100).round(2)
        # è·å–5åˆ†æ¶¨é€Ÿ
        df['speed5'] = df.groupby('id')['change'].diff(5).fillna(0)
        # é€‰å‡º15:00æ—¶åˆ»çš„changeåˆ—æ¶¨å¹…å¤§äº5çš„idçš„listï¼Œå¹¶å»é‡
        tempdf = df.copy()
        # å»é™¤change<-31,>31çš„æ•°æ®
        tempdf = tempdf[(tempdf['change'] > -31) & (tempdf['change'] < 31)]
        top_stocks = tempdf[tempdf['speed5'] > 5]['id'].unique().tolist()
        # é€‰å‡ºchangeåˆ—æ¶¨å¹…å°äº-4çš„idçš„listï¼Œå¹¶å»é‡
       
        all_stocks = list(set(top_stocks))
        chart_data = []
        for stock in all_stocks:
            stock_data = df[df['id'] == stock]
            # è¿‡æ»¤æ‰å¼‚å¸¸æ•°æ®ç‚¹ï¼Œä½†ä¿ç•™æ­£å¸¸æ•°æ®
            stock_data = stock_data[(stock_data['change'] >= -31) & (stock_data['change'] <= 31)]
            
            if not stock_data.empty:  # å¦‚æœè¿‡æ»¤åè¿˜æœ‰æ•°æ®
                stock_data = stock_data.sort_values(by='time')
                
                # è·å–xè½´å’Œyè½´æ•°æ®
                x_data = stock_data['time_str'].tolist()
                y_data = stock_data['change'].tolist()
                
                # æ·»åŠ å®Œæ•´çš„æ—¥æœŸä¿¡æ¯ä»¥ç¡®ä¿æ’åºæ­£ç¡®
                date_info = stock_data[['time_str', 'trade_date']].to_dict('records')
                
                # æ–¹æ¡ˆ2ï¼šåªæ˜¾ç¤ºæœ‰æ•°æ®çš„æ—¥æœŸï¼ˆçœŸå®æ•°æ®ï¼Œä¸å¡«å……ï¼‰
                chart_data.append({
                    "name": f'{stock}æ¶¨å¹…',
                    "x": x_data,
                    "y": y_data,
                    "dates": [d['time_str'] for d in date_info],  # æ·»åŠ å®Œæ•´æ—¥æœŸç”¨äºæ’åº
                    "mode": "lines+markers"  # æ˜ç¡®æŒ‡å®šçº¿æ¡æ¨¡å¼
                })
        
        # è·å–å®Œæ•´çš„æ—¥æœŸèŒƒå›´ç”¨äºè°ƒè¯•
        all_dates = sorted(df['time_str'].unique())
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "æ¿å—æ¶¨å¹…",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "category",  # å¼ºåˆ¶æŒ‰åˆ†ç±»æ’åºï¼Œä¸è‡ªåŠ¨é‡æ’
                    "categoryorder": "array",  # ä½¿ç”¨æ•°ç»„é¡ºåº
                    "categoryarray": all_dates  # æŒ‡å®šxè½´çš„é¡ºåº
                },
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "æ¿å—åç§°"}
            },
            "debug_info": {
                "total_series": len(chart_data),
                "date_range": f"{all_dates[0]} åˆ° {all_dates[-1]}",
                "total_dates": len(all_dates)
            }
        })
    
    def process_shizhiyu_change_daily(self):
        """å„å¸‚å€¼åŸŸæƒ…ç»ªæ—¥æ•°æ®çš„å¹³å‡æ¶¨å¹… - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/shizhiyu_change_daily', self._original_shizhiyu_change_daily)
    
    def _original_shizhiyu_change_daily(self):
        """å„å¸‚å€¼åŸŸæƒ…ç»ªæ—¥æ•°æ®çš„å¹³å‡æ¶¨å¹…"""
        d = StockDailyData()

        df = d.get_daily_data(start_date='2025-07-01')
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        # æ–°å»ºä¸€åˆ—â€˜å¸‚å€¼åŸŸâ€™ï¼ŒæŒ‰ç…§total_mvåˆ—çš„å€¼è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»è§„åˆ™ä¸º
        # 0-20äº¿ï¼šå¾®ç›˜ 20-50äº¿ï¼šå°ç›˜ 50-100äº¿ï¼šä¸­ç›˜ 100äº¿-300äº¿ï¼šä¸­å¤§ç›˜ï¼Œ300äº¿ä»¥ä¸Šï¼šå¤§ç›˜
        df['å¸‚å€¼åŸŸ'] = pd.cut(df['total_mv'], bins=[0, 20e8, 50e8, 100e8, 300e8, float('inf')],
                           labels=['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'])
        # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—changeåˆ—çš„å¹³å‡å€¼
        df['day_change'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('mean')
        # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
        df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
        # æŒ‰å¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—æ¯ä¸ªå¸‚å€¼åŸŸçš„cumsum
        df_temp['cumsum_change'] = df_temp.groupby('å¸‚å€¼åŸŸ')['day_change'].cumsum()
        market_list = ['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']
        chart_data = []
        for market in market_list:
            market_data = df_temp[df_temp['å¸‚å€¼åŸŸ'] == market]
            if not market_data.empty:
                market_data = market_data.sort_values(by='trade_date')
                chart_data.append({
                    "name": f'{market}æ¶¨å¹…',
                    "x": market_data['date_str'].tolist(),
                    "y": market_data['cumsum_change'].tolist()
                })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å¸‚å€¼åŸŸæ—¥çº¿æ¶¨å¹…",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æ¶¨å¹…(%)"},
                "legend": {"title": "å¸‚åœºåç§°"}
            }
        })

    def process_lianban_jiji_rate(self):
        """è¿æ¿æ™‹çº§ç‡ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/lianban_jiji_rate', self._original_lianban_jiji_rate)
    
    def _original_lianban_jiji_rate(self):
        """è¿æ¿æ™‹çº§ç‡"""
        d = StockDailyData()

        df = d.get_daily_data(start_date='2025-03-01')
       
        df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
        df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
        # æ–°å»ºä¸€åˆ—â€˜å¸‚å€¼åŸŸâ€™ï¼ŒæŒ‰ç…§total_mvåˆ—çš„å€¼è¿›è¡Œåˆ†ç±»ï¼Œåˆ†ç±»è§„åˆ™ä¸º
        # 0-20äº¿ï¼šå¾®ç›˜ 20-50äº¿ï¼šå°ç›˜ 50-100äº¿ï¼šä¸­ç›˜ 100äº¿-300äº¿ï¼šä¸­å¤§ç›˜ï¼Œ300äº¿ä»¥ä¸Šï¼šå¤§ç›˜
        df['next_day_change'] = df.groupby('id')['change'].shift(-1)  # è·å–ä¸‹ä¸€å¤©çš„æ¶¨è·Œå¹…
        # å› ä¸ºshiftäº†-1ï¼Œæ‰€ä»¥éœ€è¦groupbyåå»æ‰æœ€åä¸€è¡Œ
        df = df[df['next_day_change'].notna()]
        df = df[df['change']>=9.7]
        
        # å…ˆè®¡ç®—æ¯ä¸ªäº¤æ˜“æ—¥çš„æ¶¨åœæ™‹çº§ç‡
        daily_stats = df.groupby('trade_date')['next_day_change'].agg([
            ('total_count', 'count'),
            ('upgrade_count', lambda x: (x >= 9.7).sum())
        ]).reset_index()
        daily_stats['æ¶¨åœæ™‹çº§ç‡'] = daily_stats['upgrade_count'] / daily_stats['total_count']
        
        # æ·»åŠ date_stråˆ—
        daily_stats['date_str'] = pd.to_datetime(daily_stats['trade_date']).dt.strftime('%m/%d')
        
        df = daily_stats[['trade_date', 'date_str', 'æ¶¨åœæ™‹çº§ç‡']]
        chart_data = []
        chart_data.append({
            "name": f'æ¶¨åœæ™‹çº§ç‡',
            "x": df['date_str'].tolist(),
            "y": df['æ¶¨åœæ™‹çº§ç‡'].tolist()
        })
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å¸‚å€¼åŸŸæ—¥çº¿æ¶¨å¹…",
                "xaxis": {"title": "æ—¶é—´"},
                "yaxis": {"title": "æ¶¨åœæ™‹çº§ç‡(%)"},
                "legend": {"title": "æ¶¨åœæ™‹çº§ç‡"}
            }
        })

    def process_every_lianban_jiji_rate(self):
        """å„è¿æ¿æ™‹çº§ç‡ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/every_lianban_jiji_rate', self._original_every_lianban_jiji_rate)
    
    def _original_every_lianban_jiji_rate(self):
        """å„è¿æ¿æ™‹çº§ç‡"""
        # è¯»å–data\kpl_market_sentiment_data.csvçš„æ•°æ®
        kpl_data = pd.read_csv('data/kpl_market_sentiment_data.csv', encoding='gbk')
        kpl_data['trade_date'] = pd.to_datetime(kpl_data['trade_date'], errors='coerce')
        # å‡åºæ’åˆ—
        kpl_data = kpl_data.sort_values(by='trade_date').reset_index(drop=True)
        # å–æœ€æ–°çš„100å¤©æ•°æ®
        kpl_data = kpl_data.tail(100)
        kpl_data['date_str'] = kpl_data['trade_date'].dt.strftime('%m/%d')
        # rename up_limit_2_rateï¼š2è¿æ¿æ™‹çº§ç‡ï¼Œup_limit_3_rateï¼š3è¿æ¿æ™‹çº§ç‡ï¼Œup_limit_4_rateï¼š4è¿æ¿æ™‹çº§ç‡
        kpl_data = kpl_data.rename(columns={
            'up_limit_2_rate': '1è¿›2æ™‹çº§ç‡',
            'up_limit_3_rate': '2è¿›3æ™‹çº§ç‡',
            'up_limit_high_rate': '3æ¿ä»¥ä¸Šæ™‹çº§ç‡'
        })
        
        chart_data = []
        for column in ['1è¿›2æ™‹çº§ç‡', '2è¿›3æ™‹çº§ç‡', '3æ¿ä»¥ä¸Šæ™‹çº§ç‡']:
            if column in kpl_data.columns:
                chart_data.append({
                    "name": column,
                    "x": kpl_data['trade_date'].dt.strftime('%Y-%m-%d').tolist(),  # ä½¿ç”¨å®Œæ•´æ—¥æœŸæ ¼å¼
                    "y": kpl_data[column].tolist()
                })
       
        
        return jsonify({
            "chartType": "line",
            "data": chart_data,
            "layout": {
                "title": "å„è¿æ¿æ™‹çº§ç‡",
                "xaxis": {
                    "title": "æ—¶é—´",
                    "type": "date",  # æŒ‡å®šxè½´ä¸ºæ—¥æœŸç±»å‹
                    "tickformat": "%m/%d"  # æ˜¾ç¤ºæ ¼å¼ä»ä¸ºæœˆ/æ—¥
                },
                "yaxis": {"title": "æ¶¨åœæ™‹çº§ç‡(%)"},
                "legend": {"title": "æ¶¨åœæ™‹çº§ç‡"}
            }
        })


    def process_sector_line_chart_change(self):
        """æ¿å—æ¶¨å¹…æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_change', self._original_sector_line_chart_change)
    
    def _original_sector_line_chart_change(self):
        """æ¿å—æ¶¨å¹…æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(10)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(20)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}æ¶¨å¹…',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['æ¿å—æ¶¨å¹…'].tolist()
                    })
            
            return jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—æ¶¨å¹…",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "æ¶¨å¹…(%)"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨å¹…æ•°æ®å¤±è´¥: {e}")

    def process_sector_speed_chart(self):
        """æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_speed_chart', self._original_sector_speed_chart)
    
    def _original_sector_speed_chart(self):
        """æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨æ•°æ®"""
        try:
            self.logger.info("å¼€å§‹å¤„ç†æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨")
            
            # è¯»å–åŸºç¡€æ•°æ®
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "chartType": "line",
                    "data": [],
                    "layout": {
                        "title": "æ¿å—æ¶¨é€Ÿå˜åŒ–ç´¯è®¡",
                        "xaxis": {"title": "æ—¶é—´"},
                        "yaxis": {"title": "ç´¯è®¡æ¶¨é€Ÿ"},
                        "legend": {"title": "æ¿å—åç§°"}
                    },
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # è·å–æ¿å—æ¶¨å¹…æ’åº
            top_sectors = stock_df['Sector'].value_counts().head(10).index.tolist()
            if not top_sectors:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¿å—æ•°æ®"
                })
            
            stock_minute_df = self.data_cache.load_data('stock_minute_df')
            if stock_minute_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "åˆ†é’Ÿæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            stock_df['time'] = pd.to_datetime(stock_df['time'])
            latest_time = stock_df['time'].max()
            
            source_data = {
                'top_sectors': sorted(top_sectors[:10]),  # åªå–å‰10ä¸ªç”¨äºå“ˆå¸Œï¼Œé¿å…æ•°æ®é‡è¿‡å¤§
                'stock_data_time': str(latest_time),
                'stock_minute_count': len(stock_minute_df),
                'file_timestamps': {
                    'stock_df': self.data_cache.timestamps.get('stock_df', 0),
                    'stock_minute_df': self.data_cache.timestamps.get('stock_minute_df', 0),
                    'affinity_df': self.data_cache.timestamps.get('affinity_df', 0)
                }
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector_speed_chart'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—æ¶¨é€Ÿç´¯åŠ å›¾è¡¨")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            self.logger.info(f"é‡æ–°è®¡ç®—æ¿å—æ¶¨é€Ÿæ•°æ®ï¼Œå¤„ç† {len(top_sectors)} ä¸ªæ¿å—")
            
            # æ•°æ®æ¸…ç†å’Œé¢„å¤„ç†
            stock_df = stock_df.replace([np.inf, -np.inf], 0).fillna(0)
            stock_minute_df = stock_minute_df.replace([np.inf, -np.inf], 0).fillna(0)
            
            stock_df['Sector'] = stock_df['Sector'].astype(str)
            stock_df['id'] = stock_df['id'].astype(int)
            stock_df['change'] = stock_df['change'].astype(float)
            stock_df['time'] = pd.to_datetime(stock_df['time'])
            
            latest_time = stock_df['time'].max()
            stock_df = stock_df[stock_df['time'] == latest_time]
            
            affinity_df = self.data_cache.load_data('affinity_df')
            if affinity_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—å…³è”æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            chart_data = []
            
            # ä¼˜åŒ–ï¼šé™åˆ¶å¤„ç†çš„æ¿å—æ•°é‡ä»¥æå‡æ€§èƒ½
            process_sectors = top_sectors[:20]  # åªå¤„ç†å‰20ä¸ªæ¿å—ï¼Œæå‡æ€§èƒ½
            self.logger.info(f"å®é™…å¤„ç†æ¿å—æ•°é‡: {len(process_sectors)}")
            
            for sector_name in process_sectors:
                # æ¨¡ç³ŠåŒ¹é…æ¿å—
                sector_affinity_df = affinity_df[
                    affinity_df['æ¿å—'].str.contains(sector_name, na=False, case=False) |
                    affinity_df['æ¿å—'].apply(lambda x: sector_name in str(x) if pd.notna(x) else False)
                ]
                
                if sector_affinity_df.empty:
                    continue
                
                stock_ids = list(set(sector_affinity_df['è‚¡ç¥¨id'].tolist()))
                stock_count = len(stock_ids)
                
                # è¿‡æ»¤è‚¡ç¥¨ID
                stock_ids = [id for id in stock_ids if id < 680000 and (id < 400000 or id > 600000)]
                filtered_count = len(stock_ids)
                
                stock_minute_df_temp = stock_minute_df[stock_minute_df['id'].isin(stock_ids)]
                stock_minute_df_temp = stock_minute_df_temp[stock_minute_df_temp['change'] > 2]
                
                if stock_minute_df_temp.empty:
                    continue
                
                # æŒ‰æ—¶é—´ç»„èšåˆ
                stock_minute_df_temp = stock_minute_df_temp.groupby('time').agg({
                    'speed_change_1min': 'mean',
                    'id': 'count'
                }).reset_index()
                
                stock_minute_df_temp = stock_minute_df_temp.rename(columns={'id': 'stock_count'})
                stock_minute_df_temp = stock_minute_df_temp.sort_values('time')
                
                stock_minute_df_temp['speed_change_1min_rate'] = (
                    stock_minute_df_temp['speed_change_1min'] * 
                    stock_minute_df_temp['stock_count'] / filtered_count
                )
                stock_minute_df_temp['speed_change_1min_cumsum'] = (
                    stock_minute_df_temp['speed_change_1min_rate'].cumsum()
                )
                
                stock_minute_df_temp['time'] = pd.to_datetime(stock_minute_df_temp['time'])
                
                chart_data.append({
                    "name": f'{sector_name}æ¶¨é€Ÿå˜åŒ–ç´¯è®¡',
                    "x": stock_minute_df_temp['time'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                    "y": stock_minute_df_temp['speed_change_1min_cumsum'].tolist()
                })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—æ¶¨é€Ÿå˜åŒ–ç´¯è®¡",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "ç´¯è®¡æ¶¨é€Ÿ"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            self.logger.info(f"æ¿å—æ¶¨é€Ÿæ•°æ®è®¡ç®—å®Œæˆï¼Œç”Ÿæˆ {len(chart_data)} ä¸ªæ¿å—çš„å›¾è¡¨æ•°æ®")
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨é€Ÿæ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uplimit(self):
        """æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uplimit', self._original_sector_line_chart_uplimit)
    
    def _original_sector_line_chart_uplimit(self):
        """æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            latest_time = sector_df['æ—¶é—´'].max()
            source_data = {
                'data_time': str(latest_time),
                'data_count': len(sector_df),
                'sector_names': sorted(sector_names),  # æ’åºç¡®ä¿ä¸€è‡´æ€§
                'dynamic_titles': self.server.dynamic_titles.copy(),
                'file_timestamp': self.data_cache.timestamps.get('plate_df', 0)  # æ·»åŠ æ–‡ä»¶æ—¶é—´æˆ³
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector-line-chart_uplimit'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—è¿‘ä¼¼æ¶¨åœæŠ˜çº¿å›¾")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # æ·»åŠ è¿‘ä¼¼æ¶¨åœæ•°åˆ—
            sector_df['è¿‘ä¼¼æ¶¨åœæ•°'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(
                lambda x: int(x.split('-')[-1]) if '-' in x else 0
            )
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(220)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}è¿‘ä¼¼æ¶¨åœæ•°',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['è¿‘ä¼¼æ¶¨åœæ•°'].tolist()
                    })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—è¿‘ä¼¼æ¶¨åœæ•°",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "è¿‘ä¼¼æ¶¨åœæ•°"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uprate(self):
        """æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uprate', self._original_sector_line_chart_uprate)
    
    def _original_sector_line_chart_uprate(self):
        """æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            latest_time = sector_df['æ—¶é—´'].max()
            source_data = {
                'data_time': str(latest_time),
                'data_count': len(sector_df),
                'sector_names': sorted(sector_names),  # æ’åºç¡®ä¿ä¸€è‡´æ€§
                'dynamic_titles': self.server.dynamic_titles.copy(),
                'file_timestamp': self.data_cache.timestamps.get('plate_df', 0)  # æ·»åŠ æ–‡ä»¶æ—¶é—´æˆ³
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/chart-data/sector-line-chart_uprate'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—çº¢ç›˜ç‡æŠ˜çº¿å›¾")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            sector_df['uprate'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(lambda x: self.server._calculate_tail_ratio(x, n=6))
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}çº¢ç›˜ç‡',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['uprate'].tolist()
                    })
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—çº¢ç›˜ç‡",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "çº¢ç›˜ç‡"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—çº¢ç›˜ç‡æ•°æ®å¤±è´¥: {e}")

    def process_sector_line_chart_uprate5(self):
        """æ¿å—uprate5æŠ˜çº¿å›¾æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/sector_line_chart_uprate5', self._original_sector_line_chart_uprate5)
    
    def _original_sector_line_chart_uprate5(self):
        """æ¿å—uprate5æŠ˜çº¿å›¾æ•°æ®"""
        try:
            sector_names = self.server._get_dynamic_titles_list()
            sector_df = self.data_cache.load_data('plate_df')
            
            if sector_df.empty:
                return self.error_response("æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            sector_df = sector_df[sector_df['æ—¶é—´'].dt.date == sector_df['æ—¶é—´'].dt.date.max()]
            sector_df['uprate5'] = sector_df['æ¶¨å¹…åˆ†å¸ƒ'].apply(lambda x: self.server._calculate_tail_ratio(x, n=3))
            
            chart_data = []
            latest_time = sector_df['æ—¶é—´'].max()
            temp_df = sector_df[sector_df['æ—¶é—´'] == latest_time]
            
            sector_names_speed = temp_df.sort_values(by='æ¿å—5åˆ†æ¶¨é€Ÿ', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names_change = temp_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(3)['æ¿å—å'].tolist()
            sector_names = list(set(sector_names + sector_names_speed + sector_names_change))
            
            for sector_name in sector_names:
                sector_data = sector_df[sector_df['æ¿å—å'] == sector_name]
                if not sector_data.empty:
                    sector_data = sector_data.sort_values(by='æ—¶é—´')
                    chart_data.append({
                        "name": f'{sector_name}çº¢ç›˜ç‡',
                        "x": sector_data['æ—¶é—´'].dt.strftime('%Y-%m-%d %H:%M').tolist(),
                        "y": sector_data['uprate5'].tolist()
                    })
            
            return jsonify({
                "chartType": "line",
                "data": chart_data,
                "layout": {
                    "title": "æ¿å—uprate5",
                    "xaxis": {"title": "æ—¶é—´"},
                    "yaxis": {"title": "uprate5"},
                    "legend": {"title": "æ¿å—åç§°"}
                }
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—uprate5æ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # è¡¨æ ¼å¤„ç†æ–¹æ³• (åŸ table_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    
    def process_plate_info(self):
        """æ¿å—æ¦‚è¦æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_info', self._original_plate_info)
    
    def _original_plate_info(self):
        """æ¿å—æ¦‚è¦æ•°æ®è¡¨"""
        try:
            sector_df = self.data_cache.load_data('plate_df')
            if sector_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆæ•°æ®å¤„ç†
            sector_df['æ—¶é—´'] = pd.to_datetime(sector_df['æ—¶é—´'])
            latest_data = sector_df[sector_df['æ—¶é—´'] == sector_df['æ—¶é—´'].max()]
            
            # æ„å»ºè¡¨æ ¼æ•°æ®
            table_data = []
            for _, row in latest_data.iterrows():
                table_data.append({
                    "æ¿å—å": row['æ¿å—å'],
                    "æ¿å—æ¶¨å¹…": f"{row['æ¿å—æ¶¨å¹…']:.2f}%",
                    "æ¿å—5åˆ†æ¶¨é€Ÿ": f"{row['æ¿å—5åˆ†æ¶¨é€Ÿ']:.2f}%",
                    "æ¶¨å¹…åˆ†å¸ƒ": row['æ¶¨å¹…åˆ†å¸ƒ']
                })
            
            return jsonify({
                "columns": ["æ¿å—å", "æ¿å—æ¶¨å¹…", "æ¿å—5åˆ†æ¶¨é€Ÿ", "æ¶¨å¹…åˆ†å¸ƒ"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—æ¦‚è¦æ•°æ®å¤±è´¥: {e}")

    def process_stocks(self):
        """è‚¡ç¥¨æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/stocks', self._original_stocks)
    
    def _original_stocks(self):
        """è‚¡ç¥¨æ•°æ®è¡¨"""
        try:
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†
            latest_data = stock_df.head(100)  # é™åˆ¶æ•°æ®é‡
            
            table_data = []
            for _, row in latest_data.iterrows():
                table_data.append({
                    "è‚¡ç¥¨ä»£ç ": row.get('id', ''),
                    "è‚¡ç¥¨åç§°": row.get('name', ''),
                    "æ¶¨è·Œå¹…": f"{row.get('change', 0):.2f}%",
                    "æ¿å—": row.get('Sector', '')
                })
            
            return jsonify({
                "columns": ["è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°", "æ¶¨è·Œå¹…", "æ¿å—"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # è¡¨æ ¼å¤„ç†æ–¹æ³• (åŸ table_processor.py ä¸­çš„æ–¹æ³•)  
    # =========================================================================
    
    def process_plate_info_table_data(self):
        """è¿”å›æ¿å—æ¦‚è¦æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/plate_info_table_data', self._original_plate_info_table_data)
    
    def _original_plate_info_table_data(self):
        """è¿”å›æ¿å—æ¦‚è¦æ•°æ®è¡¨"""
        try:
            start_time = time.time()
            sector_name = request.args.get('sectors', 'èˆªè¿æ¦‚å¿µ')
            
            # æ„å»ºç¼“å­˜å‚æ•°
            cache_params = self.build_cache_params(sector_name=sector_name)
            
            plate_df = self.data_cache.load_data('plate_df')
            if plate_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¿å—æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ•°æ®å¤„ç†
            plate_df['æ—¶é—´'] = pd.to_datetime(plate_df['æ—¶é—´'])
            latest_time = plate_df['æ—¶é—´'].max()
            plate_df = plate_df[plate_df['æ—¶é—´'] == latest_time]
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            source_data = {
                'sector_name': sector_name,
                'data_time': str(latest_time),
                'data_count': len(plate_df),
                # æ·»åŠ å½±å“ç»“æœçš„å…³é”®å­—æ®µçš„å“ˆå¸Œ
                'plate_summary': plate_df[['æ¿å—å', 'æ¿å—æ¶¨å¹…', 'æ¿å—5åˆ†æ¶¨é€Ÿ']].to_dict('records')[:10]  # åªå–å‰10ä¸ªä½œä¸ºæ‘˜è¦
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/table-data/plate_info'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, cache_params, source_data)
            
            if should_cache and cached_response:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¿å—æ¦‚è¦è¡¨æ ¼: {sector_name}")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # è®¡ç®—å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ
            speed_bins = [-10, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 10]
            speed_counts = []
            for i in range(len(speed_bins)-1):
                count = len(plate_df[
                    (plate_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] >= speed_bins[i]) & 
                    (plate_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] < speed_bins[i+1])
                ])
                speed_counts.append(str(count))
            market_speed_distribution = "-".join(speed_counts)
            plate_df['å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ'] = market_speed_distribution
            
            plate_df['æ¿å—å'] = plate_df['æ¿å—å'].astype(str)
            
            # è·å–æ’åå‰15çš„æ¿å—
            top_by_change = plate_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False).head(15)['æ¿å—å'].tolist()
            top_by_turnover = plate_df.sort_values(by='å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”', ascending=False).head(15)['æ¿å—å'].tolist()
            top_plates_list = list(set(top_by_change + top_by_turnover))
            
            if sector_name not in top_plates_list:
                top_plates_list.append(sector_name)
            
            plate_df = plate_df[plate_df['æ¿å—å'].isin(top_plates_list)]
            plate_df = plate_df.sort_values(by='æ¿å—æ¶¨å¹…', ascending=False)
            plate_df = plate_df.drop_duplicates(subset=['æ¿å—å']).reset_index(drop=True)
            
            # å®šä¹‰è¡¨æ ¼åˆ—
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "æ¿å—å", "header": "æ¿å—å"},
                {"field": "æ¿å—æ¶¨å¹…", "header": "æ¿å—æ¶¨å¹…", "backgroundColor": "redGreen"},
                {"field": "æ¿å—æ˜¨æ—¥æ¶¨å¹…", "header": "æ¿å—æ˜¨æ—¥æ¶¨å¹…", "backgroundColor": "redGreen"},
                {"field": "å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”", "header": "å¼ºåŠ¿åˆ†æ—¶æ¢æ‰‹å æ¯”", "backgroundColor": "redGreen"},
                {"field": "æ¿å—5åˆ†æ¶¨é€Ÿ", "header": "æ¿å—5åˆ†æ¶¨é€Ÿ", "backgroundColor": "redGreen"},
                {"field": "æ¿å—é‡æ¯”", "header": "æ¿å—é‡æ¯”", "backgroundColor": "redGreen"},
                {"field": "æ¶¨é€Ÿåˆ†å¸ƒ", "header": "æ¶¨é€Ÿåˆ†å¸ƒ"},
                {"field": "æ¶¨å¹…åˆ†å¸ƒ", "header": "æ¶¨å¹…åˆ†å¸ƒ"},
                {"field": "æ¶¨åœæ¢¯åº¦", "header": "æ¶¨åœæ¢¯åº¦"},
                {"field": "æ¶¨é€Ÿæ’å", "header": "æ¶¨é€Ÿæ’å"},
                {"field": "æ¶¨å¹…æ’å", "header": "æ¶¨å¹…æ’å"},
                {"field": "å¤§ç›˜é‡æ¯”", "header": "å¤§ç›˜é‡æ¯”"},
                {"field": "å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ", "header": "å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ"},
            ]
            
            valid_columns = [col for col in columns if col["field"] in plate_df.columns]
            
            rows = []
            for _, row_data in plate_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, cache_params, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—ä¿¡æ¯å¤±è´¥: {e}")

    def process_stocks_table_data(self):
        """è¿”å›è‚¡ç¥¨æ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/stocks_table_data', self._original_stocks_table_data)
    
    def _original_stocks_table_data(self):
        """è¿”å›è‚¡ç¥¨æ•°æ®è¡¨"""
        try:
            # TODO: ä» table_processor.py è¿ç§»å®Œæ•´å®ç°
            stock_df = self.data_cache.load_data('stock_df')
            if stock_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "è‚¡ç¥¨æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†é€»è¾‘
            columns = [
                {"field": "name", "header": "è‚¡ç¥¨åç§°"},
                {"field": "change", "header": "æ¶¨è·Œå¹…", "backgroundColor": "redGreen"},
                {"field": "Sector", "header": "æ¿å—"},
                {"field": "speed_change_1min", "header": "1åˆ†é’Ÿæ¶¨é€Ÿ", "backgroundColor": "redGreen"}
            ]
            
            rows = []
            for _, row in stock_df.head(100).iterrows():  # é™åˆ¶è¿”å›æ•°é‡
                row_data = {}
                for col in columns:
                    field = col["field"]
                    value = row.get(field, '')
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    row_data[field] = value
                rows.append(row_data)
            
            return jsonify({
                "columns": columns,
                "rows": rows
            })
        
        except Exception as e:
            return self.error_response(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
    def process_get_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®ï¼Œä»å›ºå®šCSVæ–‡ä»¶è¯»å– - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/get_up_limit_table_data', self._original_get_up_limit_table_data)
    
    def _original_get_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®ï¼Œä»å›ºå®šCSVæ–‡ä»¶è¯»å–"""
        try:
            # è¯»å–CSVæ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šè·¯å¾„ï¼‰
            
            # è¯»å–æ¶¨åœæ•°æ®
            up_limit_df = pd.read_csv(r'strategy\showhtml\server\up_limit_df.csv')
            
            # # ç¡®ä¿æ—¶é—´åˆ—è¢«æ­£ç¡®å¤„ç†
            
            # up_limit_df['æ—¶é—´'] = pd.to_datetime(up_limit_df['æ—¶é—´'])
            # # è·å–æœ€æ–°çš„æ—¶é—´çš„è¡Œ
            # latest_time = up_limit_df['æ—¶é—´'].max()
            # # for test è·å–09ï¼š41çš„æ—¶é—´
            # # latest_time = plate_df['æ—¶é—´'].max().replace(hour=10, minute=5, second=0)
            # up_limit_df = up_limit_df[up_limit_df['æ—¶é—´'] == latest_time]
            # # è·å–è¯¥æ—¶åˆ»çš„å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒï¼Œè®¡ç®—æ–¹æ³•ä¸ºï¼šç»Ÿè®¡è¯¥æ—¶åˆ»â€˜æ¿å—5åˆ†æ¶¨é€Ÿåˆ—â€™-10~-0.6ï¼Œ-0.6~-0.4ï¼Œ-0.4~-0.2ï¼Œ-0.2~0, 0~0.2ï¼Œ0.2~0.4ï¼Œ0.4~0.6,0.6~10çš„æ•°é‡ï¼Œå¹¶ç”¨-å·è¿æ¥
            # speed_bins = [-10, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 10]
            # speed_counts = []
            # for i in range(len(speed_bins)-1):
            #     count = len(up_limit_df[(up_limit_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] >= speed_bins[i]) & 
            #                         (up_limit_df['æ¿å—5åˆ†æ¶¨é€Ÿ'] < speed_bins[i+1])])
            #     speed_counts.append(str(count))
            # market_speed_distribution = "-".join(speed_counts)
            # up_limit_df['å¤§ç›˜æ¶¨é€Ÿåˆ†å¸ƒ'] = market_speed_distribution
            # # è·å–åˆ—Sectorçš„å€¼ä¸ºsector_nameçš„è¡Œ
            # up_limit_df['æ¿å—å'] = up_limit_df['æ¿å—å'].astype(str)
            # up_limit_df = up_limit_df[up_limit_df['æ¿å—å'] == sector_name]
            
            
            # å®šä¹‰è¡¨æ ¼åˆ—ï¼ˆæ ¹æ®CSVæ–‡ä»¶çš„å®é™…åˆ—è¿›è¡Œè°ƒæ•´ï¼‰
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "è‚¡ç¥¨ID", "header": "è‚¡ç¥¨ID", "visible": False},
                {"field": "è‚¡ç¥¨åç§°", "header": "è‚¡ç¥¨åç§°"},
                {"field": "æ¿å—1", "header": "æ¿å—1"},
                {"field": "æ¿å—2", "header": "æ¿å—2"},
                {"field": "æ¿å—3", "header": "æ¿å—3", "visible": False},
                {"field": "æ¿å—4", "header": "æ¿å—4", "visible": False},
                {"field": "æ¿å—5", "header": "æ¿å—5", "visible": False},
                {"field": "10æ—¥æ¶¨åœæ•°", "header": "10æ—¥æ¶¨åœæ•°"},
                {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "visible": False},

            ]
            
            # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½å­˜åœ¨äºCSVæ–‡ä»¶ä¸­
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            # è½¬æ¢DataFrameä¸ºæ‰€éœ€æ ¼å¼
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    # å¤„ç†æ•°å€¼æ ¼å¼ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            return jsonify({
                "columns": valid_columns,
                "rows": rows
            })
        
        except Exception as e:
            import traceback
            traceback.print_exc()
            return jsonify({"error": str(e)}), 500
        
    def process_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/up_limit_table_data', self._original_up_limit_table_data)
    
    def _original_up_limit_table_data(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨"""
        # try:
        #     # TODO: ä» table_processor.py è¿ç§»å®Œæ•´å®ç°
        #     up_limit_df = self.data_cache.load_data('up_limit_df')
        #     if up_limit_df.empty:
        #         return jsonify({
        #             "columns": [],
        #             "rows": [],
        #             "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
        #         })
            
        #     # ç®€åŒ–ç‰ˆå¤„ç†é€»è¾‘
        #     columns = [
        #         {"field": "name", "header": "è‚¡ç¥¨åç§°"},
        #         {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "backgroundColor": "redGreen"},
        #         {"field": "Sector", "header": "æ¿å—"},
        #         {"field": "æ¶¨åœæ—¶é—´", "header": "æ¶¨åœæ—¶é—´"}
        #     ]
            
        #     rows = []
        #     for _, row in up_limit_df.iterrows():
        #         row_data = {}
        #         for col in columns:
        #             field = col["field"]
        #             value = row.get(field, '')
        #             if isinstance(value, (float, np.float64, np.float32)):
        #                 value = round(value, 2)
        #             row_data[field] = value
        #         rows.append(row_data)
            
        #     return jsonify({
        #         "columns": columns,
        #         "rows": rows
        #     })
        
        # except Exception as e:
        #     return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")
        try:
            up_limit_df = self.data_cache.load_data('up_limit_df')
            
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ„å»ºç”¨äºå“ˆå¸Œæ¯”è¾ƒçš„æºæ•°æ®
            source_data = {
                'data_count': len(up_limit_df),
                'file_timestamp': self.data_cache.timestamps.get('up_limit_df', 0),
                'data_sample': up_limit_df.head(5).to_dict('records') if not up_limit_df.empty else []
            }
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜
            cache_endpoint = '/api/table-data/up_limit'
            should_cache, cached_response = self.should_use_cache(cache_endpoint, None, source_data)
            
            if should_cache and cached_response:
                self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®è¿”å›æ¶¨åœæ•°æ®è¡¨")
                return cached_response
            
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # å®šä¹‰è¡¨æ ¼åˆ— - æ ¹æ®å®é™…CSVæ–‡ä»¶çš„åˆ—å
            columns = [
                {"field": "æ—¶é—´", "header": "æ—¶é—´"},
                {"field": "è‚¡ç¥¨åç§°", "header": "è‚¡ç¥¨åç§°"},
                {"field": "æ¿å—1", "header": "æ¿å—1"},
                {"field": "æ¿å—2", "header": "æ¿å—2"},  
                {"field": "æ¿å—3", "header": "æ¿å—3", "visible": False},
                {"field": "æ¿å—4", "header": "æ¿å—4", "visible": False},
                {"field": "æ¿å—5", "header": "æ¿å—5", "visible": False},
                {"field": "10æ—¥æ¶¨åœæ•°", "header": "10æ—¥æ¶¨åœæ•°", "backgroundColor": "redGreen"},
                {"field": "è¿æ¿æ•°", "header": "è¿æ¿æ•°", "backgroundColor": "redGreen"},
                {"field": "è‚¡ç¥¨ID", "header": "è‚¡ç¥¨ID", "visible": False},
            ]
            
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.store_cache(cache_endpoint, None, source_data, response_data)
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    def process_up_limit_stocks_review(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨ - æ”¯æŒæ—¥æœŸå‚æ•°ï¼Œä¸ä½¿ç”¨ç¼“å­˜"""
        return self._original_up_limit_stocks_review()

    def _original_up_limit_stocks_review(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨"""
        
        try:
            # è·å–æ—¶é—´å‚æ•°ï¼Œé»˜è®¤ä¸ºä»Šå¤©
            from flask import request
            selected_date = request.args.get('date', '2025-07-25')  # æœŸæœ›æ ¼å¼ï¼šYYYY-MM-DD
            
            d = KplUpLimitData()
            up_limit_df = d.get_daily_data(start_date='2025-07-01')
            
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # æ—¶é—´è¿‡æ»¤é€»è¾‘
            up_limit_df['trade_date'] = pd.to_datetime(up_limit_df['trade_date'])
            
            if selected_date:
               
                target_date = pd.to_datetime(selected_date).date()
                # å¦‚æœæ—¥æœŸä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸçš„æ•°æ®
                if target_date not in up_limit_df['trade_date'].dt.date.unique():
                    self.logger.warning(f"æŒ‡å®šæ—¥æœŸ {target_date} ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€æ–°æ—¥æœŸçš„æ•°æ®")
                    up_limit_df = up_limit_df[up_limit_df['trade_date'].dt.date == up_limit_df['trade_date'].dt.date.max()]
                # è¿‡æ»¤æŒ‡å®šæ—¥æœŸçš„æ•°æ®
                else:
                    up_limit_df = up_limit_df[up_limit_df['trade_date'].dt.date == target_date]
            else:
                # è·å–æœ€æ–°çš„æ—¥æœŸçš„è¡Œ
                up_limit_df = up_limit_df[up_limit_df['trade_date'].dt.date == up_limit_df['trade_date'].dt.date.max()]
                
            # å°†trade_dateæ”¹ä¸ºæœˆæ—¥çš„æ ¼å¼
            up_limit_df['trade_date'] = up_limit_df['trade_date'].dt.strftime('%m-%d')
            # å°†up_limit_amountï¼Œfamcåˆ—è½¬æ¢ä¸ºäº¿å…ƒä¸ºå•ä½
            up_limit_df['up_limit_amount'] = up_limit_df['up_limit_amount'] / 1e8
            up_limit_df['famc'] = up_limit_df['famc'] / 1e8
            # éœ€è¦é‡æ–°è®¡ç®—ï¼Œç»§ç»­æ‰§è¡ŒåŸæœ‰é€»è¾‘
            # å®šä¹‰è¡¨æ ¼åˆ— - æ ¹æ®å®é™…CSVæ–‡ä»¶çš„åˆ—å
            columns = [
                {"field": "trade_date", "header": "æ—¶é—´"},
                {"field": "stock_name", "header": "è‚¡ç¥¨åç§°"},
                {"field": "up_limit_reason", "header": "æ¶¨åœåŸå› "},
                {"field": "sector", "header": "æ¿å—"},  
                {"field": "up_limit_strength", "header": "æ¶¨åœå¼ºåº¦"},
                {"field": "up_limit_amount", "header": "å°å•é‡‘é¢", "backgroundColor": "redGreen"},
                {"field": "day_up_limit_count", "header": "æ¶¨åœæ—¥æ•°", "backgroundColor": "redGreen"},
                {"field": "up_limit_count", "header": "æ¶¨åœæ¬¡æ•°", "backgroundColor": "redGreen"},
                {"field": "famc", "header": "æµé€šå¸‚å€¼", "backgroundColor": "redGreen"},
                {"field": "intro", "header": "å…¬å¸ä»‹ç»"},
                {"field": "id", "header": "è‚¡ç¥¨ID", "visible": False},
            ]
            
            valid_columns = [col for col in columns if col["field"] in up_limit_df.columns]
            
            rows = []
            for _, row_data in up_limit_df.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    def process_upLimitStocksDist_v2(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/upLimitStocksDist_v2', self._original_upLimitStocksDist_v2)

    def _original_upLimitStocksDist_v2(self):
        """è¿”å›æ¶¨åœæ•°æ®è¡¨"""
        
        try:
            d = KplUpLimitData()
            up_limit_df = d.get_daily_data(start_date='2025-07-01')
            
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # è·å–æœ€æ–°çš„æ—¥æœŸçš„è¡Œ
            up_limit_df['trade_date'] = pd.to_datetime(up_limit_df['trade_date'])
            # å°†trade_dateæ”¹ä¸ºæœˆæ—¥çš„æ ¼å¼
            up_limit_df['trade_date'] = up_limit_df['trade_date'].dt.strftime('%m-%d')
            # å°†up_limit_amountï¼Œfamcåˆ—è½¬æ¢ä¸ºäº¿å…ƒä¸ºå•ä½
            up_limit_df['up_limit_amount'] = up_limit_df['up_limit_amount'] / 1e8
            up_limit_df['famc'] = up_limit_df['famc'] / 1e8

            # å¯¹trade_dateè¿›è¡Œåˆ†ç»„ï¼Œæå–æ¿å—åˆ—ä¸­çš„æ‰€æœ‰æ¿å—åï¼Œæ¿å—åˆ—ä¸­çš„å€¼æ˜¯ä»¥é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—åå½“æ—¥å‡ºç°çš„æ¬¡æ•°
            # åšæˆä¸€ä¸ªæ–°çš„DataFrameï¼ŒåŒ…å«trade_dateå’Œæ¿å—åä¸¤åˆ—
            sector_count_data = []
            for trade_date, group in up_limit_df.groupby('trade_date'):
                for _, row in group.iterrows():
                    if pd.notna(row['sector']) and row['sector']:  # ç¡®ä¿æ¿å—åˆ—ä¸ä¸ºç©º
                        # åˆ†å‰²é€—å·åˆ†éš”çš„æ¿å—å­—ç¬¦ä¸²
                        sectors = [sector.strip() for sector in str(row['up_limit_reason']).split('ã€') if sector.strip()]
                        for sector in sectors:
                            if sector != 'å…¶ä»–':
                                sector_count_data.append({
                                    'trade_date': trade_date,
                                    'sector_name': sector
                                })
            
            # åˆ›å»ºæ¿å—ç»Ÿè®¡DataFrame
            sector_df = pd.DataFrame(sector_count_data)
            if not sector_df.empty:
                # æŒ‰æ—¥æœŸå’Œæ¿å—ååˆ†ç»„ç»Ÿè®¡æ¬¡æ•°
                sector_count_df = sector_df.groupby(['trade_date', 'sector_name']).size().reset_index(name='count')
                
                # æ•´ç†æˆä»¥trade_dateä¸ºåˆ—ï¼Œæ¿å—åä¸ºè¡Œï¼Œcountä¸ºå€¼çš„æ–°çš„DataFrame
                sector_pivot_df = sector_count_df.pivot(index='sector_name', columns='trade_date', values='count')
                # å¡«å……NaNå€¼ä¸º0
                sector_pivot_df = sector_pivot_df.fillna(0).astype(int)
                
                # å¯¹sector_count_dfæ’åºï¼Œè·å–æœ€æ–°çš„5ä¸ªæ—¥æœŸï¼ŒæŒ‰æ¿å—ååˆ†ç»„è®¡ç®—countåˆè®¡ï¼Œè·å–æ’åºé¡ºåº
                if not sector_count_df.empty:
                    # è·å–æœ€æ–°çš„5ä¸ªäº¤æ˜“æ—¥æœŸ
                    latest_dates = sorted(sector_count_df['trade_date'].unique(), reverse=True)[:5]
                    
                    # ç­›é€‰æœ€æ–°5ä¸ªæ—¥æœŸçš„æ•°æ®å¹¶æŒ‰æ¿å—åˆ†ç»„æ±‚å’Œ
                    recent_sector_totals = (sector_count_df[sector_count_df['trade_date'].isin(latest_dates)]
                                           .groupby('sector_name')['count'].sum()
                                           .sort_values(ascending=False))  # æŒ‰countæ€»å’Œé™åºæ’åˆ—
                    
                    # è·å–æ’åºåçš„æ¿å—åé¡ºåº
                    sector_order = recent_sector_totals.index.tolist()
                    
                    # é‡æ–°æ’åˆ—sector_pivot_dfçš„è¡Œé¡ºåºï¼ŒåªåŒ…å«å­˜åœ¨çš„æ¿å—
                    existing_sectors = [sector for sector in sector_order if sector in sector_pivot_df.index]
                    sector_pivot_df = sector_pivot_df.reindex(existing_sectors)
                    
                    # æ·»åŠ totalåˆ—ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—çš„countæ€»æ•°
                    sector_pivot_df['total'] = sector_pivot_df.sum(axis=1)
            else:
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç©ºçš„DataFrame
                sector_count_df = pd.DataFrame(columns=['trade_date', 'sector_name', 'count'])
                sector_pivot_df = pd.DataFrame()
            
            # ä¸ºsector_pivot_dfåŠ¨æ€å®šä¹‰åˆ— - é«˜æ•ˆæ–¹æ³•
            if not sector_pivot_df.empty:
                # åŠ¨æ€ç”Ÿæˆæ—¥æœŸåˆ—
                date_columns = []
                for date_col in sector_pivot_df.columns:
                    if date_col != 'total':  # æ’é™¤totalåˆ—
                        date_columns.append({
                            "field": date_col,
                            "header": f"25{date_col}",  # æ·»åŠ å¹´ä»½å‰ç¼€
                            "backgroundColor": "redGreen"  # æ—¥æœŸåˆ—ä½¿ç”¨redGreené¢œè‰²
                        })
                
                # æ„å»ºå®Œæ•´çš„sectoråˆ—å®šä¹‰
                sector_columns = [
                    {"field": "sector_name", "header": "æ¿å—åç§°"}  # æ¿å—åç§°åˆ—
                ] + date_columns + [
                    {"field": "total", "header": "æ€»è®¡", "backgroundColor": "redGreen"}  # æ€»è®¡åˆ—
                ]
            else:
                sector_columns = []

            # ä¸ºsector_pivot_dfå‡†å¤‡æ•°æ® - å°†ç´¢å¼•è½¬ä¸ºåˆ—
            if not sector_pivot_df.empty:
                # å°†ç´¢å¼•(sector_name)è½¬ä¸ºåˆ—
                sector_pivot_df_with_names = sector_pivot_df.reset_index()
            else:
                sector_pivot_df_with_names = pd.DataFrame()
            
            valid_columns = [col for col in sector_columns if col["field"] in sector_pivot_df_with_names.columns]
            
            rows = []
            for _, row_data in sector_pivot_df_with_names.iterrows():
                row = {}
                for col in valid_columns:
                    field = col["field"]
                    value = row_data[field]
                    
                    # å¤„ç†ä¸åŒæ•°æ®ç±»å‹çš„JSONåºåˆ—åŒ–
                    if isinstance(value, (float, np.float64, np.float32)):
                        value = round(value, 2)
                    elif isinstance(value, (int, np.int64, np.int32)):
                        value = int(value)  # è½¬æ¢ä¸ºPythonåŸç”Ÿintç±»å‹
                    
                    row[field] = value
                
                rows.append(row)
            
            # æ„å»ºå“åº”æ•°æ®
            response_data = jsonify({
                "columns": valid_columns,
                "rows": rows
            })
            
            return response_data
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")
           
    def process_up_limit(self):
        """æ¶¨åœæ•°æ®è¡¨ - å¸¦å¯åŠ¨ç¼“å­˜"""
        return self._process_with_startup_cache('/api/up_limit', self._original_up_limit)
    
    def _original_up_limit(self):
        """æ¶¨åœæ•°æ®è¡¨"""
        try:
            up_limit_df = self.data_cache.load_data('up_limit_df')
            if up_limit_df.empty:
                return jsonify({
                    "columns": [],
                    "rows": [],
                    "message": "æ¶¨åœæ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥"
                })
            
            # ç®€åŒ–ç‰ˆå¤„ç†
            table_data = []
            for _, row in up_limit_df.iterrows():
                table_data.append({
                    "è‚¡ç¥¨åç§°": row.get('name', ''),
                    "è¿æ¿æ•°": row.get('è¿æ¿æ•°', 0),
                    "æ¿å—": row.get('Sector', ''),
                    "æ¶¨åœæ—¶é—´": row.get('æ¶¨åœæ—¶é—´', '')
                })
            
            return jsonify({
                "columns": ["è‚¡ç¥¨åç§°", "è¿æ¿æ•°", "æ¿å—", "æ¶¨åœæ—¶é—´"],
                "rows": table_data
            })
        
        except Exception as e:
            return self.error_response(f"è·å–æ¶¨åœæ•°æ®å¤±è´¥: {e}")

    # =========================================================================
    # æ¿å—å¤„ç†æ–¹æ³• (åŸ sector_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    
    # =========================================================================
    # æ¿å—å¤„ç†æ–¹æ³• (åŸ sector_processor.py ä¸­çš„æ–¹æ³•)
    # =========================================================================
    def build_stacked_area_data(self, df, x_axis_col, data_columns_config, colors=None):
        """
        æ„å»ºå †å é¢ç§¯å›¾æ•°æ®çš„é€šç”¨å‡½æ•°
        
        Args:
            df: æ•°æ®DataFrame
            x_axis_col: xè½´åˆ—å
            data_columns_config: æ•°æ®åˆ—é…ç½®ï¼Œæ ¼å¼ä¸º [{"key": "æ˜¾ç¤ºåç§°", "column": "åˆ—å"}, ...]
            colors: é¢œè‰²åˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é¢œè‰²
        
        Returns:
            dict: åŒ…å«stackedAreaDataã€xAxisValuesã€tableDataçš„å­—å…¸
        """
        try:
            # æ„å»º stackedAreaChart æ•°æ®æ ¼å¼
            xAxisValues = df[x_axis_col].tolist()  # xè½´æ•°æ®
            
            # æ„å»ºæ•°æ®å­—å…¸
            data = {}
            table_data = {}
            hover_data = {}
            
            for _, row in df.iterrows():
                x_value = row[x_axis_col]
                
                # æŒ‰ç…§é…ç½®é¡ºåºæ„å»ºæ•°æ®
                row_data = {}
                row_hover = {}
                
                for config in data_columns_config:
                    key = config["key"]
                    column = config["column"]
                    value = int(row[column]) if pd.notna(row[column]) else 0
                    
                    row_data[key] = value
                    row_hover[key] = [f"{value}"]
                
                data[x_value] = row_data
                
                # è®¡ç®—æ€»æ•°
                total = sum(row_data.values())
                table_data[x_value] = f"{total}"
                hover_data[x_value] = row_hover
            
            # å®šä¹‰æ˜¾ç¤ºé¡ºåºå’Œé¢œè‰²
            keyOrder = [config["key"] for config in data_columns_config]
            
            if colors is None:
                # é»˜è®¤é¢œè‰²é…ç½®
                default_colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000', 
                                '#9932CC', '#FF69B4', '#FFD700', '#32CD32', '#FF4500', '#1E90FF']
                colors = default_colors[:len(keyOrder)]
            
            # æ„å»ºè¿”å›æ•°æ®
            return {
                "stackedAreaData": {
                    "data": data,
                    "keyOrder": keyOrder,
                    "colors": colors,
                    "hoverData": hover_data
                },
                "xAxisValues": xAxisValues,
                "tableData": table_data
            }
            
        except Exception as e:
            self.logger.error(f"æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥: {e}")
            return None

    def process_all_market_change_distribution(self):
        """
        è·å–å…¨å¸‚åœºæ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/all_market_change_distribution', self._original_all_market_change_distribution)
    
    def _original_all_market_change_distribution(self):
        """
        è·å–å…¨å¸‚åœºæ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™å…¨å¸‚åœºæ•°æ®
            df = df[df['name'] == 'å…¨å¸‚åœº']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_up5_shizhiyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„å„å¸‚å€¼åŸŸæ—¥çº¿çº§åˆ«çš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šå¾®ç›˜, å°ç›˜, ä¸­ç›˜, ä¸­å¤§ç›˜, å¤§ç›˜
        """
        return self._process_with_startup_cache('/api/up5_shizhiyu_distribution', self._original_up5_shizhiyu_distribution)
    
    def _original_up5_shizhiyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„å„å¸‚å€¼åŸŸæ—¥çº¿çº§åˆ«çš„è‚¡ç¥¨æ•°
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šå¾®ç›˜, å°ç›˜, ä¸­ç›˜, ä¸­å¤§ç›˜, å¤§ç›˜
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            
            df['å¸‚å€¼åŸŸ'] = pd.cut(df['total_mv'], bins=[0, 20e8, 50e8, 100e8, 300e8, float('inf')],
                           labels=['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'])
            df = df[df['change']>=5]  # å»æ‰æ²¡æœ‰å¸‚å€¼åŸŸçš„è¡Œ
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('count')
            # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
            # å°†å¸‚å€¼åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='å¸‚å€¼åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']].sum(axis=1)
            df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']] = df_pivot[['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']].div(df_pivot['æ€»æ•°'], axis=0) * 100 
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            # è®¡ç®—'å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜'å„åˆ—çš„percentchange
            market_cap_columns = ['å¾®ç›˜', 'å°ç›˜', 'ä¸­ç›˜', 'ä¸­å¤§ç›˜', 'å¤§ç›˜']
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "å¾®ç›˜", "column": "å¾®ç›˜"},
                {"key": "å°ç›˜", "column": "å°ç›˜"},
                {"key": "ä¸­ç›˜", "column": "ä¸­ç›˜"},
                {"key": "ä¸­å¤§ç›˜", "column": "ä¸­å¤§ç›˜"},
                {"key": "å¤§ç›˜", "column": "å¤§ç›˜"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_up5_zhubanyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„ä¸»æ¿ä¸åˆ›ä¸šæ¿åˆ†å¸ƒ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        return self._process_with_startup_cache('/api/up5_zhubanyu_distribution', self._original_up5_zhubanyu_distribution)
    
    def _original_up5_zhubanyu_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº5çš„ä¸»æ¿ä¸åˆ›ä¸šæ¿åˆ†å¸ƒ
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')

            df['å¸‚å€¼åŸŸ'] = pd.cut(df['id'], bins=[0, 300000, 400000, 499999, 680000, 800000, float('inf')],
                           labels=['ä¸»æ¿1', 'åˆ›ä¸šæ¿', 'æ–°ä¸‰æ¿', 'ä¸»æ¿2', 'ç§‘åˆ›ç‰ˆ', 'åŒ—äº¤æ‰€'])
            df = df[df['change']>=5]  # å»æ‰æ²¡æœ‰å¸‚å€¼åŸŸçš„è¡Œ
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'å¸‚å€¼åŸŸ'])['change'].transform('count')
            

            # æ¯å¤©çš„å¸‚å€¼åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„å¸‚å€¼åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'å¸‚å€¼åŸŸ'])
            # å°†å¸‚å€¼åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='å¸‚å€¼åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['ä¸»æ¿1', 'åˆ›ä¸šæ¿', 'æ–°ä¸‰æ¿', 'ä¸»æ¿2', 'ç§‘åˆ›ç‰ˆ', 'åŒ—äº¤æ‰€']].sum(axis=1)
            df_pivot['ä¸»æ¿'] = df_pivot['ä¸»æ¿1'] + df_pivot['ä¸»æ¿2']  # åˆå¹¶ä¸»æ¿1å’Œä¸»æ¿2
            df_pivot = df_pivot.drop(columns=['ä¸»æ¿1', 'ä¸»æ¿2'])  # åˆ é™¤åˆå¹¶åçš„åˆ—
            df_pivot['æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'] = df_pivot['æ–°ä¸‰æ¿'] + df_pivot['åŒ—äº¤æ‰€']  # åˆå¹¶æ–°ä¸‰æ¿å’ŒåŒ—äº¤æ‰€
            df_pivot = df_pivot.drop(columns=['æ–°ä¸‰æ¿', 'åŒ—äº¤æ‰€'])  # åˆ é™¤åˆå¹¶åçš„åˆ—
            df_pivot[['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€']] = df_pivot[['ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€']].div(df_pivot['æ€»æ•°'], axis=0) * 100
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "ä¸»æ¿", "column": "ä¸»æ¿"},
                {"key": "åˆ›ä¸šæ¿", "column": "åˆ›ä¸šæ¿"},
                {"key": "ç§‘åˆ›ç‰ˆ", "column": "ç§‘åˆ›ç‰ˆ"},
                {"key": "æ–°ä¸‰æ¿+åŒ—äº¤æ‰€", "column": "æ–°ä¸‰æ¿+åŒ—äº¤æ‰€"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")

    def process_plate_stock_day_change_distribution(self):
        """
        ä»txtè¯»å–æ–‡ä»¶ä¸­çš„æ¿å—å,åœ¨csvæ–‡ä»¶ä¸­æ‰¾åˆ°è¯¥æ¿å—å¯¹åº”çš„è‚¡ç¥¨idï¼Œå¹¶è·å–å…¶æ—¥çº¿æ¶¨å¹…æ•°æ® - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºå„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨ä¸ªæ•°
        """
        return self._process_with_startup_cache('/api/plate_stock_day_change_distribution', self._original_plate_stock_day_change_distribution)
    
    def _original_plate_stock_day_change_distribution(self):
        """
        ä»txtè¯»å–æ–‡ä»¶ä¸­çš„æ¿å—å,åœ¨csvæ–‡ä»¶ä¸­æ‰¾åˆ°è¯¥æ¿å—å¯¹åº”çš„è‚¡ç¥¨idï¼Œå¹¶è·å–å…¶æ—¥çº¿æ¶¨å¹…æ•°æ®
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºå„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨ä¸ªæ•°
        """
        try:
            # è·å–data\plate_name.txtä¸­çš„æ¿å—å
            plate_name_file = r'data\plate_name.txt'
            if not os.path.exists(plate_name_file):
                return self.error_response("æ¿å—åæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»º data/plate_name.txt æ–‡ä»¶")
            with open(plate_name_file, 'r', encoding='utf-8') as f:
                sector_name = f.read().strip()
            
            # è¯»å–strategy\strategy001\data\all_sectors_stock_level.csvä¸­çš„æ•°æ®
            all_sectors_stock_df = pd.read_csv(r'strategy\strategy001\data\all_sectors_stock_level.csv')
            # è·å–Sectoråˆ—ä¸­åŒ…å«sector_nameçš„è¡Œçš„idåˆ—çš„list
            sector_ids = all_sectors_stock_df[all_sectors_stock_df['Sector'].str.contains(sector_name, na=False)]['id'].tolist()
            
            # è¯»å–è‚¡ç¥¨æ—¥çº¿æ•°æ®
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df = df[df['id'].isin(sector_ids)]

            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')

            # è·å–æœ€è¿‘20æ—¥çš„è¡Œ
            df = df[df['trade_date'] >= (df['trade_date'].max() - pd.Timedelta(days=20))]
            # åˆ›å»ºæ¶¨å¹…åŸŸåˆ—
            df['æ¶¨å¹…åŸŸ'] = pd.cut(df['change'], bins=[float('-inf'), -9.7, -5, -2, 0, 2, 5, 9.7, float('inf')],
                           labels=['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf'])

            # æŒ‰trade_dateå’Œæ¶¨å¹…åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'æ¶¨å¹…åŸŸ'])['change'].transform('count')

            # æ¯å¤©çš„æ¶¨å¹…åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„æ¶¨å¹…åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'æ¶¨å¹…åŸŸ'])
            # å°†æ¶¨å¹…åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='æ¶¨å¹…åŸŸ', values='day_count').fillna(0).reset_index()
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "c_inf_c-10", "column": "c_inf_c-10"},
                {"key": "c-10_c-5", "column": "c-10_c-5"},
                {"key": "c-5_c-2", "column": "c-5_c-2"},
                {"key": "c-2_c0", "column": "c-2_c0"},
                {"key": "c0_c2", "column": "c0_c2"},
                {"key": "c2_c5", "column": "c2_c5"},
                {"key": "c5_c10", "column": "c5_c10"},
                {"key": "c10_c_inf", "column": "c10_c_inf"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', "#697472", '#FF6B6B', '#FF0000', '#9932CC', '#FF69B4']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_up5_fan_sencer_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº9.7çš„æ˜¨æ—¥ä¹°å…¥å¹³å‡æ¶¨å¹…åˆ†å¸ƒ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        return self._process_with_startup_cache('/api/up5_fan_sencer_distribution', self._original_up5_fan_sencer_distribution)
    
    def _original_up5_fan_sencer_distribution(self):
        """
        è·å–æ¶¨å¹…å¤§äº9.7çš„æ˜¨æ—¥ä¹°å…¥å¹³å‡æ¶¨å¹…åˆ†å¸ƒ
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šä¸»æ¿, åˆ›ä¸šæ¿, ç§‘åˆ›ç‰ˆ, åŒ—äº¤æ‰€+æ–°ä¸‰æ¿
        """
        try:
            d = StockDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            df['next_day_change'] = df.groupby('id')['change'].shift(-1)  # è·å–ä¸‹ä¸€å¤©çš„æ¶¨è·Œå¹…
            # å› ä¸ºshiftäº†-1ï¼Œæ‰€ä»¥éœ€è¦groupbyåå»æ‰æœ€åä¸€è¡Œ
            df = df[df['next_day_change'].notna()]
            df = df[df['change']>=9.7] 
            # å»é™¤openï¼Œ close, high, lowä¸ºç›¸åŒå€¼çš„è¡Œ
            df = df[(df['open'] != df['close']) | (df['open'] != df['high']) | (df['open'] != df['low'])]
            
            df['æ¶¨å¹…åŸŸ'] = pd.cut(df['next_day_change'], bins=[float('-inf'), -9.7, -5, -2, 0, 2, 5, 9.7, float('inf')],
                           labels=['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf'])
           
            # æŒ‰trade_dateå’Œå¸‚å€¼åŸŸåˆ†ç»„ï¼Œè®¡ç®—ä¸ªæ•°
            df['day_count'] = df.groupby(['trade_date', 'æ¶¨å¹…åŸŸ'])['change'].transform('count')

            # æ¯å¤©çš„æ¶¨å¹…åŸŸå„å–1ä¸ªï¼Œå»æ‰é‡å¤çš„æ¶¨å¹…åŸŸ
            df_temp = df.drop_duplicates(subset=['trade_date', 'æ¶¨å¹…åŸŸ'])

            # å°†æ¶¨å¹…åŸŸåˆ—å†…çš„å€¼ä½œä¸ºåˆ—åï¼Œpivotè¡¨æ ¼
            df_pivot = df_temp.pivot(index='date_str', columns='æ¶¨å¹…åŸŸ', values='day_count').fillna(0).reset_index()
            # å°†'ä¸»æ¿', 'åˆ›ä¸šæ¿', 'ç§‘åˆ›ç‰ˆ', 'æ–°ä¸‰æ¿+åŒ—äº¤æ‰€'å„åˆ—çš„å€¼å˜ä¸ºç™¾åˆ†æ¯”ï¼Œåˆ†æ¯ä¸ºæ¯è¡Œçš„æ€»å’Œ
            df_pivot['æ€»æ•°'] = df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']].sum(axis=1)
            df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']] = df_pivot[['c_inf_c-10', 'c-10_c-5', 'c-5_c-2', 'c-2_c0', 'c0_c2', 'c2_c5', 'c5_c10', 'c10_c_inf']].div(df_pivot['æ€»æ•°'], axis=0) * 100
            df_pivot = df_pivot.drop(columns=['æ€»æ•°'])
            
        
            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "c_inf_c-10", "column": "c_inf_c-10"},
                {"key": "c-10_c-5", "column": "c-10_c-5"},
                {"key": "c-5_c-2", "column": "c-5_c-2"},
                {"key": "c-2_c0", "column": "c-2_c0"},
                {"key": "c0_c2", "column": "c0_c2"},
                {"key": "c2_c5", "column": "c2_c5"},
                {"key": "c5_c10", "column": "c5_c10"},
                {"key": "c10_c_inf", "column": "c10_c_inf"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', "#697472", '#FF6B6B', '#FF0000', '#9932CC', '#FF69B4']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df_pivot, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
                
    def process_chuangye_change_distribution(self):
        """
        è·å–åˆ›ä¸šæ¿æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/chuangye_change_distribution', self._original_chuangye_change_distribution)
    
    def _original_chuangye_change_distribution(self):
        """
        è·å–åˆ›ä¸šæ¿æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™åˆ›ä¸šæ¿æ•°æ®
            df = df[df['name'] == 'åˆ›ä¸šæ¿']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_st_change_distribution(self):
        """
        è·å–STè‚¡ç¥¨æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•° - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªåæ ‡ä¸ºæ—¥æœŸï¼ˆæœˆ/æ—¥æ ¼å¼ï¼‰ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        æŒ‰ç…§yè½´ä»é«˜åˆ°ä½çš„ç´¯è®¡é¡ºåºæ˜¾ç¤ºï¼šlimit_up_count, up5, up0, down0, down5, limit_down_count
        """
        return self._process_with_startup_cache('/api/st_change_distribution', self._original_st_change_distribution)
    
    def _original_st_change_distribution(self):
        """
        è·å–STè‚¡ç¥¨æ—¥çº¿çº§åˆ«å„æ¶¨å¹…åˆ†å¸ƒçš„è‚¡ç¥¨æ•°
        """
        try:
            d = MarketSentimentDailyData()
            df = d.get_daily_data(start_date='2025-03-01')
            
            df['trade_date'] = pd.to_datetime(df['trade_date'], errors='coerce')
            df['date_str'] = df['trade_date'].dt.strftime('%m/%d')
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—ï¼Œåªä¿ç•™STæ•°æ®
            df = df[df['name'] == 'ST']
            
            # è®¡ç®—å„ä¸ªåŒºé—´çš„è‚¡ç¥¨æ•°
            df['up5'] = df['up5_count'] - df['limit_up_count']
            df['down5'] = df['down5_count'] - df['limit_down_count']
            df['up0'] = df['up_count'] - df['up5_count']      
            df['down0'] = df['down_count'] - df['down5_count']
            
            # æŒ‰æ—¥æœŸæ’åº
            df = df.sort_values('trade_date')

            # å®šä¹‰æ•°æ®åˆ—é…ç½®ï¼ˆæŒ‰ç…§ä»ä¸‹åˆ°ä¸Šçš„å †å é¡ºåºï¼‰
            data_columns_config = [
                {"key": "è·Œåœ", "column": "limit_down_count"},
                {"key": "è·Œ5-10%", "column": "down5"},
                {"key": "è·Œ0-5%", "column": "down0"},
                {"key": "æ¶¨0-5%", "column": "up0"},
                {"key": "æ¶¨5-10%", "column": "up5"},
                {"key": "æ¶¨åœ", "column": "limit_up_count"},
            ]
            
            # å®šä¹‰é¢œè‰²ï¼ˆä»ä¸‹åˆ°ä¸Šï¼šè·Œåœåˆ°æ¶¨åœï¼‰
            colors = ['#00008B', '#4169E1', '#87CEEB', '#FFA07A', '#FF6B6B', '#FF0000']
            
            # ä½¿ç”¨é€šç”¨å‡½æ•°æ„å»ºæ•°æ®
            result = self.build_stacked_area_data(df, 'date_str', data_columns_config, colors)
            
            if result is None:
                return self.error_response("æ„å»ºå †å é¢ç§¯å›¾æ•°æ®å¤±è´¥")
            
            # æ„å»ºè¿”å›æ•°æ®
            return jsonify(result)
            
        except Exception as e:
            return self.error_response(f"è·å–å…¨å¸‚åœºæ¶¨è·Œå¹…åˆ†å¸ƒå¤±è´¥: {e}")    
            
    def process_today_plate_up_limit_distribution(self):
        """
        è·å–æŒ‡å®šæ—¥æœŸå„æ¿å—è¿æ¿æ•°åˆ†å¸ƒï¼ˆæ¿å—å†…è‚¡ç¥¨æ—¥çº¿æ¶¨å¹…åˆ†å¸ƒï¼‰ - å¸¦å¯åŠ¨ç¼“å­˜
        æ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨å †ç§¯å›¾å±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        
        å‚æ•°:
        - date: å¯é€‰ï¼Œæ ¼å¼ä¸ºYYYY-MM-DDï¼Œå¦‚ï¼š2024-07-21ã€‚ä¸æä¾›åˆ™ä½¿ç”¨ä»Šæ—¥æ•°æ®
        
        ç”¨æ³•ç¤ºä¾‹:
        - /api/today_plate_up_limit_distribution (ä½¿ç”¨ä»Šæ—¥æ•°æ®)
        - /api/today_plate_up_limit_distribution?date=2024-07-21 (ä½¿ç”¨æŒ‡å®šæ—¥æœŸæ•°æ®)
        """
        return self._original_today_plate_up_limit_distribution()
    
    def _original_today_plate_up_limit_distribution(self):
        """
        è·å–ä»Šæ—¥å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒ
        æ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨å †ç§¯å›¾å±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        æ”¯æŒæ—¶é—´é€‰æ‹©åŠŸèƒ½ï¼Œå¯ä»¥æŒ‡å®šæŸä¸€å¤©çš„æ•°æ®
        """
        try:
            # è·å–æ—¶é—´å‚æ•°ï¼Œé»˜è®¤ä¸ºä»Šå¤©
            from flask import request
            selected_date = request.args.get('date','2025-07-25')  # æœŸæœ›æ ¼å¼ï¼šYYYY-MM-DD
            d= NightFactorData()
            stock_all_level_df = d.get_daily_data()
            
            if stock_all_level_df.empty:
                return self.error_response("è‚¡ç¥¨è¿æ¿æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            # æ—¶é—´è¿‡æ»¤é€»è¾‘
            if selected_date:
                try:
                    # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºdatetimeå¯¹è±¡
                    target_date = pd.to_datetime(selected_date).date()
                    
                    # ç¡®ä¿trade_dateåˆ—æ˜¯datetimeç±»å‹
                    if 'trade_date' in stock_all_level_df.columns:
                        stock_all_level_df['trade_date'] = pd.to_datetime(stock_all_level_df['trade_date'])
                        # å¦‚æœtarget_dateåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œè¿”å›æœ€å¤§æ—¥æœŸçš„æ•°æ®
                        if target_date not in stock_all_level_df['trade_date'].dt.date.unique():
                            target_date = stock_all_level_df['trade_date'].dt.date.max()
                            selected_date = target_date.strftime('%Y-%m-%d')

                        stock_all_level_df = stock_all_level_df[stock_all_level_df['trade_date'].dt.date == target_date]
                        
                        if stock_all_level_df.empty:
                            return self.error_response(f"æŒ‡å®šæ—¥æœŸ {selected_date} æ²¡æœ‰æ•°æ®")
                    else:
                        # å¦‚æœæ²¡æœ‰trade_dateåˆ—ï¼Œä½¿ç”¨ä»Šå¤©çš„æ•°æ®ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
                        pass
                        
                except Exception as date_error:
                    return self.error_response(f"æ—¥æœŸæ ¼å¼é”™è¯¯: {date_error}ï¼Œè¯·ä½¿ç”¨YYYY-MM-DDæ ¼å¼")
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—
            stock_all_level_df = stock_all_level_df[~stock_all_level_df['sector'].str.contains("æ²ªè‚¡é€š|æ·±è‚¡é€š|å­£æŠ¥|èèµ„èåˆ¸", na=False)]
            stock_all_level_df = stock_all_level_df[stock_all_level_df['lian_ban_shu'] > 0]
           
            stock_all_level_df['Level'] = stock_all_level_df['lian_ban_shu']
            # å°†è¿æ¿æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†å¼‚å¸¸å€¼
            stock_all_level_df['Level'] = pd.to_numeric(stock_all_level_df['Level'], errors='coerce').fillna(0).astype(int)

            # æŒ‰ç…§æ¿å—åˆ†ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒ
            sector_level_stats = stock_all_level_df.groupby(['sector', 'Level']).size().unstack(fill_value=0)
            
            # åŒæ—¶è·å–æ¯ä¸ªæ¿å—æ¯ä¸ªè¿æ¿æ•°å¯¹åº”çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºhoveræ˜¾ç¤º
            sector_stock_lists = {}
            for sector in stock_all_level_df['sector'].unique():
                sector_stock_lists[sector] = {}
                sector_data = stock_all_level_df[stock_all_level_df['sector'] == sector]
                for level in sector_data['Level'].unique():
                    level_data = sector_data[sector_data['Level'] == level]
                    sector_stock_lists[sector][level] = level_data['stock_name'].tolist() if 'stock_name' in level_data.columns else level_data['code'].tolist()
            
            # è·å–æ‰€æœ‰çš„è¿æ¿æ•°ç­‰çº§
            max_level = min(stock_all_level_df['Level'].max(), 10)  # é™åˆ¶æœ€å¤§è¿æ¿æ•°ä¸º10ï¼Œé¿å…è¿‡å¤šåˆ†ç±»
            level_columns = list(range(1, max_level + 1))
            
            # ç¡®ä¿æ‰€æœ‰è¿æ¿æ•°ç­‰çº§çš„åˆ—éƒ½å­˜åœ¨
            for level in level_columns:
                if level not in sector_level_stats.columns:
                    sector_level_stats[level] = 0
            
            # åªä¿ç•™éœ€è¦çš„è¿æ¿æ•°åˆ—ï¼Œå¹¶æŒ‰é¡ºåºæ’åˆ—
            sector_level_stats = sector_level_stats[level_columns]
            
            # è®¡ç®—æ¯ä¸ªæ¿å—çš„æ€»è‚¡ç¥¨æ•°ï¼ŒæŒ‰æ€»æ•°æ’åºï¼Œåªæ˜¾ç¤ºå‰20ä¸ªæ¿å—
            sector_level_stats['total'] = sector_level_stats.sum(axis=1)
            top_sectors = sector_level_stats.sort_values('total', ascending=False).head(20).index.tolist()
            sector_level_stats = sector_level_stats.loc[top_sectors]
            sector_level_stats = sector_level_stats.drop('total', axis=1)
            
            # æ„å»ºå †ç§¯å›¾æ•°æ® - ä½¿ç”¨ä¸å…¶ä»–å›¾è¡¨ä¸€è‡´çš„æ ¼å¼
            categories = sector_level_stats.index.tolist()  # æ¿å—åç§°ä½œä¸ºæ¨ªåæ ‡
            chart_data = []
            
            # ä¸ºæ¯ä¸ªè¿æ¿æ•°ç­‰çº§åˆ›å»ºä¸€ä¸ªç³»åˆ— - ä½¿ç”¨å†·è‰²åˆ°æš–è‰²çš„æ¸å˜
            # 1è¿æ¿ï¼šæ·±è“è‰²ï¼ˆæœ€å†·ï¼‰ -> é«˜è¿æ¿ï¼šæ·±çº¢è‰²ï¼ˆæœ€æš–ï¼‰
            colors = [
                '#1E3A8A',  # 1è¿æ¿ - æ·±è“è‰²
                '#0D9488',  # 2è¿æ¿ - è“è‰²
                '#CA8A04',  # 3è¿æ¿ - é’ç»¿è‰²  
                "#F17676",  # 4è¿æ¿ - ç»¿è‰²
                '#DC2626',  # 5è¿æ¿ - é‡‘é»„è‰²
                '#7C2D12',  # 6è¿æ¿ - æ©™è‰²
                '#450A0A',  # 7è¿æ¿ - çº¢è‰²
                '#450A0A',  # 8è¿æ¿ - æ·±çº¢è‰²
                "#450A0A",  # 9è¿æ¿ - æ£•çº¢è‰²
                '#450A0A'   # 10è¿æ¿+ - æ·±è¤çº¢è‰²
            ]
            
            for i, level in enumerate(level_columns):
                # ä¸ºæ¯ä¸ªæ¿å—æ„å»ºè¯¥è¿æ¿æ•°çš„æ•°æ®å’Œhoverä¿¡æ¯
                y_values = []
                hover_texts = []
                
                for sector in categories:
                    stock_count = sector_level_stats.loc[sector, level]
                    # è½¬æ¢ä¸ºPythonåŸç”Ÿintç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é—®é¢˜
                    stock_count = int(stock_count)
                    y_values.append(stock_count)
                    
                    # æ„å»ºhoveræ–‡æœ¬
                    if stock_count > 0 and sector in sector_stock_lists and level in sector_stock_lists[sector]:
                        stock_list = sector_stock_lists[sector][level]
                        # é™åˆ¶æ˜¾ç¤ºçš„è‚¡ç¥¨æ•°é‡ï¼Œé¿å…hoverå¤ªé•¿
                        if len(stock_list) > 10:
                            stock_display = stock_list[:10] + [f"...ç­‰{len(stock_list)}åªè‚¡ç¥¨"]
                        else:
                            stock_display = stock_list
                        
                        stock_text = '<br>'.join([f'â€¢ {stock}' for stock in stock_display])
                        hover_text = f'{sector}<br>{level}è¿æ¿: {stock_count}åª<br><br>è‚¡ç¥¨åˆ—è¡¨:<br>{stock_text}'
                    else:
                        hover_text = f'{sector}<br>{level}è¿æ¿: {stock_count}åª'
                    
                    hover_texts.append(hover_text)
                
                chart_data.append({
                    'name': f'{level}è¿æ¿',
                    'x': categories,  # æ¿å—åç§°
                    'y': y_values,  # å¯¹åº”çš„è‚¡ç¥¨ä¸ªæ•°
                    'type': 'bar',  # æŸ±çŠ¶å›¾ç±»å‹
                    'marker': {'color': colors[i % len(colors)]},
                    'text': hover_texts,  # hoveræ˜¾ç¤ºçš„æ–‡æœ¬
                    'hovertemplate': '%{text}<extra></extra>',  # ä½¿ç”¨è‡ªå®šä¹‰hoveræ¨¡æ¿
                    'hoverlabel': {
                        'bgcolor': 'rgba(30, 30, 30, 0.9)',  # æ·±ç°è‰²åŠé€æ˜èƒŒæ™¯
                        'bordercolor': 'white',               # ç™½è‰²è¾¹æ¡†
                        'font': {
                            'color': 'white',                 # ç™½è‰²å­—ä½“
                            'size': 24,                       # å­—ä½“å¤§å°
                            'family': 'Arial, sans-serif'    # å­—ä½“å®¶æ—
                        }
                    }
                })
            
            # æ„å»ºåŠ¨æ€æ ‡é¢˜
            if selected_date:
                chart_title = f"å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒ ({selected_date})"
            else:
                chart_title = "å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒ (ä»Šæ—¥)"
            
            return jsonify({
                "chartType": "bar",  # æŸ±çŠ¶å›¾
                "data": chart_data,  # å›¾è¡¨æ•°æ®
                "layout": {
                    "title": chart_title,
                    "xaxis": {"title": "æ¿å—åç§°"},
                    "yaxis": {"title": "è‚¡ç¥¨ä¸ªæ•°"},
                    "barmode": "stack",  # å †ç§¯æ¨¡å¼
                    "legend": {"title": "è¿æ¿æ•°"}
                }
            })
            
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—è¿æ¿æ•°åˆ†å¸ƒå¤±è´¥: {e}")
        
    def process_today_plate_up_limit_distribution_v2(self):
        """
        è·å–ä»Šæ—¥å„æ¿å—è¿æ¿æ•°åˆ†å¸ƒï¼Œæ¨ªè½´ä¸ºæ¿å—åç§°ï¼Œçºµè½´ä¸ºè‚¡ç¥¨ä¸ªæ•°
        åˆ†åˆ«ç”¨stackedAreaChartå±•ç¤ºå„æ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒï¼Œä»ä¸‹å¾€ä¸Šä¸º1æ¿ï¼Œ2æ¿ï¼Œ3æ¿ç­‰
        
        å§‹ç»ˆè¿”å›æ‚¬æµ®æç¤ºæ•°æ®ï¼Œç”±å‰ç«¯å†³å®šæ˜¯å¦ä½¿ç”¨
        """
        try:
            stock_all_level_df = self.data_cache.load_data('stock_all_level_df')
            if stock_all_level_df.empty:
                return self.error_response("è‚¡ç¥¨è¿æ¿æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥")
            
            # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ¿å—
            stock_all_level_df = stock_all_level_df[~stock_all_level_df['Sector'].str.contains("æ²ªè‚¡é€š|æ·±è‚¡é€š|å­£æŠ¥|èèµ„èåˆ¸", na=False)]
            stock_all_level_df = stock_all_level_df[stock_all_level_df['è¿æ¿æ•°'] > 0]
           
            stock_all_level_df['Level'] = stock_all_level_df['è¿æ¿æ•°']
            # å°†è¿æ¿æ•°è½¬æ¢ä¸ºæ•´æ•°ï¼Œå¤„ç†å¼‚å¸¸å€¼
            stock_all_level_df['Level'] = pd.to_numeric(stock_all_level_df['Level'], errors='coerce').fillna(0).astype(int)

            # æŒ‰ç…§æ¿å—åˆ†ç»„ï¼Œç»Ÿè®¡æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒ
            sector_level_stats = stock_all_level_df.groupby(['Sector', 'Level']).size().unstack(fill_value=0)
            
            # åŒæ—¶è·å–æ¯ä¸ªæ¿å—æ¯ä¸ªè¿æ¿æ•°å¯¹åº”çš„è‚¡ç¥¨åˆ—è¡¨ï¼Œç”¨äºhoveræ˜¾ç¤º
            sector_stock_lists_v2 = {}
            for sector in stock_all_level_df['Sector'].unique():
                sector_stock_lists_v2[sector] = {}
                sector_data = stock_all_level_df[stock_all_level_df['Sector'] == sector]
                for level in sector_data['Level'].unique():
                    level_data = sector_data[sector_data['Level'] == level]
                    # ä¼˜å…ˆä½¿ç”¨stock_nameï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨code
                    if 'stock_name' in level_data.columns:
                        sector_stock_lists_v2[sector][level] = level_data['stock_name'].tolist()
                    elif 'code' in level_data.columns:
                        sector_stock_lists_v2[sector][level] = level_data['code'].tolist()
                    else:
                        # å¦‚æœéƒ½æ²¡æœ‰ï¼Œå°è¯•ä½¿ç”¨ç´¢å¼•
                        sector_stock_lists_v2[sector][level] = level_data.index.tolist()
            
            # è·å–æ‰€æœ‰çš„è¿æ¿æ•°ç­‰çº§
            max_level = min(stock_all_level_df['Level'].max(), 6)  # é™åˆ¶æœ€å¤§è¿æ¿æ•°ä¸º6ï¼Œé€‚åˆå †å é¢ç§¯å›¾
            level_columns = list(range(1, max_level + 1))
            
            # ç¡®ä¿æ‰€æœ‰è¿æ¿æ•°ç­‰çº§çš„åˆ—éƒ½å­˜åœ¨
            for level in level_columns:
                if level not in sector_level_stats.columns:
                    sector_level_stats[level] = 0
            
            # åªä¿ç•™éœ€è¦çš„è¿æ¿æ•°åˆ—ï¼Œå¹¶æŒ‰é¡ºåºæ’åˆ—
            sector_level_stats = sector_level_stats[level_columns]
            
            # è®¡ç®—æ¯ä¸ªæ¿å—çš„æ€»è‚¡ç¥¨æ•°ï¼ˆæ‰€æœ‰è¿æ¿æ•°çš„åˆè®¡ï¼‰ï¼ŒæŒ‰æ€»è‚¡ç¥¨æ•°æ’åºï¼ˆä»å¤§åˆ°å°ï¼‰ï¼Œåªæ˜¾ç¤ºå‰15ä¸ªæ¿å—ï¼ˆé€‚åˆé¢ç§¯å›¾æ˜¾ç¤ºï¼‰
            sector_level_stats['total_stocks'] = sector_level_stats.sum(axis=1)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.info("æ’åºå‰çš„æ¿å—é¡ºåºå’Œæ€»è‚¡ç¥¨æ•°:")
            for idx, total in sector_level_stats['total_stocks'].items():
                self.logger.info(f"  {idx}: {total}åª")
            
            top_sectors_v2 = sector_level_stats.sort_values('total_stocks', ascending=False).head(15).index.tolist()
            sector_level_stats = sector_level_stats.loc[top_sectors_v2]
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            self.logger.info("æ’åºåçš„æ¿å—é¡ºåºå’Œæ€»è‚¡ç¥¨æ•°:")
            for idx, total in sector_level_stats['total_stocks'].items():
                self.logger.info(f"  {idx}: {total}åª")
            
            sector_level_stats = sector_level_stats.drop('total_stocks', axis=1)
            
            # æ„å»º stackedAreaChart æ•°æ®æ ¼å¼
            xAxisValues = sector_level_stats.index.tolist()  # æ¿å—åç§°ä½œä¸ºæ¨ªåæ ‡
            
            # æ„å»ºæ•°æ®å­—å…¸ï¼Œæ¯ä¸ªæ¿å—å¯¹åº”å…¶å„è¿æ¿æ•°çš„è‚¡ç¥¨ä¸ªæ•°
            data = {}
            table_data = {}
            hover_data = {}  # å§‹ç»ˆåˆ›å»ºæ‚¬æµ®æ•°æ®
            
            for sector_name in xAxisValues:
                sector_data = {}
                sector_hover = {}  # å§‹ç»ˆå­˜å‚¨è‚¡ç¥¨åç§°åˆ—è¡¨
                total_stocks = 0
                
                for level in level_columns:
                    key = f"{level}è¿æ¿"
                    value = int(sector_level_stats.loc[sector_name, level])
                    sector_data[key] = value
                    total_stocks += value
                    
                    # ä½¿ç”¨é¢„æ„å»ºçš„è‚¡ç¥¨åˆ—è¡¨æ•°æ®
                    if sector_name in sector_stock_lists_v2 and level in sector_stock_lists_v2[sector_name]:
                        level_stocks = sector_stock_lists_v2[sector_name][level]
                    else:
                        level_stocks = []
                    
                    # æ„å»ºhoveræ˜¾ç¤ºæ–‡æœ¬
                    if len(level_stocks) > 0:
                        # é™åˆ¶æ˜¾ç¤ºçš„è‚¡ç¥¨æ•°é‡ï¼Œé¿å…hoverå¤ªé•¿
                        if len(level_stocks) > 8:
                            stock_display = level_stocks[:8] + [f"...ç­‰{len(level_stocks)}åªè‚¡ç¥¨"]
                        else:
                            stock_display = level_stocks
                        sector_hover[key] = stock_display
                    else:
                        sector_hover[key] = []
                
                data[sector_name] = sector_data
                table_data[sector_name] = f"{total_stocks}åª"
                hover_data[sector_name] = sector_hover
            
            # å®šä¹‰è¿æ¿æ•°ç±»å‹çš„é¡ºåºå’Œé¢œè‰² - ä½¿ç”¨å†·è‰²åˆ°æš–è‰²çš„æ¸å˜
            keyOrder = [f"{level}è¿æ¿" for level in level_columns]
            # 1è¿æ¿ï¼šæ·±è“è‰²ï¼ˆæœ€å†·ï¼‰ -> é«˜è¿æ¿ï¼šæ·±çº¢è‰²ï¼ˆæœ€æš–ï¼‰
            colors = [
                '#1E3A8A',  # 1è¿æ¿ - æ·±è“è‰²
                '#1E40AF',  # 2è¿æ¿ - è“è‰²
                '#0D9488',  # 3è¿æ¿ - é’ç»¿è‰²  
                '#059669',  # 4è¿æ¿ - ç»¿è‰²
                '#CA8A04',  # 5è¿æ¿ - é‡‘é»„è‰²
                '#EA580C'   # 6è¿æ¿ - æ©™è‰²
            ][:len(level_columns)]
            
            # æ„å»ºè¿”å›æ•°æ®ï¼Œå§‹ç»ˆåŒ…å«æ‚¬æµ®æ•°æ®
            return jsonify({
                "stackedAreaData": {
                    "data": data,          # æ¯ä¸ªæ¿å—çš„è¿æ¿æ•°åˆ†å¸ƒæ•°æ®
                    "keyOrder": keyOrder,  # è¿æ¿æ•°ç±»å‹çš„æ˜¾ç¤ºé¡ºåº
                    "colors": colors,      # æ¯ä¸ªè¿æ¿æ•°ç±»å‹çš„é¢œè‰²
                    "hoverData": hover_data  # é¼ æ ‡æ‚¬æµ®æ—¶æ˜¾ç¤ºçš„è‚¡ç¥¨åç§°åˆ—è¡¨
                },
                "xAxisValues": xAxisValues,  # æ¨ªè½´æ¿å—åç§°
                "tableData": table_data      # è¡¨æ ¼æ•°æ®ï¼ˆæ€»è‚¡ç¥¨æ•°ï¼‰
            })
            
        except Exception as e:
            return self.error_response(f"è·å–æ¿å—è¿æ¿æ•°åˆ†å¸ƒå¤±è´¥: {e}")
        
        
    def process_sector_stacked_area_data(self):
        """æ¿å—å †å é¢ç§¯å›¾æ•°æ® - è°ƒç”¨ v2 æ–¹æ³•"""
        return self.process_today_plate_up_limit_distribution_v2()
    
    def process(self, method_name: str):
        """
        å¤„ç†è¯·æ±‚çš„ä¸»å…¥å£
        
        Args:
            method_name: æ–¹æ³•åç§°
            
        Returns:
            Flaskå“åº”å¯¹è±¡
        """
        try:
            # æ„å»ºç¼“å­˜å‚æ•°
            cache_params = self.build_cache_params(method=method_name)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å¤„ç†æ–¹æ³•
            if hasattr(self, method_name):
                method = getattr(self, method_name)
                
                # è·å–æºæ•°æ®ï¼ˆç”¨äºç¼“å­˜åˆ¤æ–­ï¼‰
                source_data = self.server._get_source_data_for_endpoint(f"/api/{method_name}")
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ç¼“å­˜
                if self.should_use_cache(method_name, cache_params, source_data):
                    cached_response = self.response_cache.get_cached_response(method_name, cache_params)
                    if cached_response:
                        self.logger.info(f"è¿”å›market_reviewç¼“å­˜æ•°æ®: {method_name}")
                        return cached_response
                
                # æ‰§è¡Œæ–¹æ³•è·å–æ–°æ•°æ®
                self.logger.info(f"æ‰§è¡Œmarket_reviewæ–¹æ³•: {method_name}")
                result = method()
                
                # å­˜å‚¨åˆ°ç¼“å­˜
                self.store_cache(method_name, cache_params, source_data, result)
                
                return result
            else:
                return self.error_response(f"æ–¹æ³• {method_name} ä¸å­˜åœ¨", 404)
                
        except Exception as e:
            return self.error_response(f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")
    
    def get_available_methods(self):
        """è·å–æ‰€æœ‰å¯ç”¨çš„å¤„ç†æ–¹æ³•"""
        methods = []
        for attr_name in dir(self):
            if not attr_name.startswith('_') and callable(getattr(self, attr_name)):
                if attr_name not in ['process', 'get_available_methods', 'get_request_params', 
                                   'build_cache_params', 'should_use_cache', 'store_cache', 'error_response']:
                    methods.append(attr_name)
        return methods
    
    # ===== ç¤ºä¾‹æ•°æ®å¤„ç†æ–¹æ³• =====
    # ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹æ–¹æ³•ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æˆ–æ·»åŠ æ–°æ–¹æ³•
    
    def chart1(self):
        """ç¤ºä¾‹å›¾è¡¨1æ•°æ®"""
        try:
            # TODO: æ ¹æ®æ‚¨çš„ä¸šåŠ¡éœ€æ±‚å®ç°å…·ä½“çš„æ•°æ®å¤„ç†é€»è¾‘
            # è¿™é‡Œæ˜¯ä¸€ä¸ªç¤ºä¾‹å®ç°
            
            # è·å–è¯·æ±‚å‚æ•°
            params = self.get_request_params()
            
            # ä»æ•°æ®ç¼“å­˜åŠ è½½æ•°æ®
            # data = self.data_cache.load_data('your_data_file')
            
            # å¤„ç†æ•°æ®é€»è¾‘
            sample_data = {
                "title": "market_review - å›¾è¡¨1",
                "data": [
                    {"x": "2024-01", "y": 100},
                    {"x": "2024-02", "y": 120},
                    {"x": "2024-03", "y": 110}
                ],
                "timestamp": time.time(),
                "params": params
            }
            
            return jsonify(sample_data)
            
        except Exception as e:
            return self.error_response(f"è·å–chart1æ•°æ®å¤±è´¥: {str(e)}")
    
    def table1(self):
        """ç¤ºä¾‹è¡¨æ ¼1æ•°æ®"""
        try:
            # TODO: å®ç°è¡¨æ ¼æ•°æ®å¤„ç†é€»è¾‘
            
            params = self.get_request_params()
            
            sample_data = {
                "title": "market_review - è¡¨æ ¼1",
                "columns": ["åç§°", "æ•°å€¼", "å˜åŒ–"],
                "data": [
                    ["é¡¹ç›®1", 100, "+5%"],
                    ["é¡¹ç›®2", 200, "+3%"],
                    ["é¡¹ç›®3", 150, "-2%"]
                ],
                "timestamp": time.time(),
                "params": params
            }
            
            return jsonify(sample_data)
            
        except Exception as e:
            return self.error_response(f"è·å–table1æ•°æ®å¤±è´¥: {str(e)}")
    
    def config(self):
        """è·å–market_reviewé…ç½®ä¿¡æ¯"""
        try:
            config_data = {
                "processor_name": self.processor_name,
                "available_methods": self.get_available_methods(),
                "description": "å¤ç›˜é¡µé¢",
                "timestamp": time.time()
            }
            
            return jsonify(config_data)
            
        except Exception as e:
            return self.error_response(f"è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def get_sector_rankings_by_period(self, df, latest_date_str, days_list=[5, 10, 20], top_n=10):
        """
        è·å–æŒ‡å®šæ—¶é—´æ®µå†…cumsumæ’åå‰Nçš„æ¿å—
        
        Args:
            df: åŒ…å«trade_date, name, changeåˆ—çš„DataFrame
            latest_date_str: æœ€æ–°æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼'YYYYMMDD'
            days_list: è¦ç»Ÿè®¡çš„å¤©æ•°åˆ—è¡¨ï¼Œé»˜è®¤[5, 10, 20]
            top_n: è¿”å›å‰Nåï¼Œé»˜è®¤10
            
        Returns:
            dict: å„æ—¶é—´æ®µçš„æ’åç»“æœ
        """
        ranking_results = {}
        
        for days in days_list:
            try:
                # è®¡ç®—å¼€å§‹æ—¥æœŸ
                period_start_str = get_trade_date_by_offset(latest_date_str, days)
                period_start = pd.to_datetime(period_start_str, format='%Y%m%d')
                latest_date = pd.to_datetime(latest_date_str, format='%Y%m%d')
                
                # ç­›é€‰æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®
                period_df = df[(df['trade_date'] >= period_start) & (df['trade_date'] <= latest_date)]
                
                # é‡æ–°è®¡ç®—cumsumï¼ˆé¿å…ä½¿ç”¨å…¨å±€cumsumï¼‰
                period_df = period_df.copy()
                period_df = period_df.sort_values(['name', 'trade_date'])
                period_df['period_cumsum'] = period_df.groupby('name')['change'].cumsum()
                
                # è·å–æ¯ä¸ªæ¿å—åœ¨è¯¥æ—¶é—´æ®µçš„æœ€æ–°cumsumå€¼
                latest_cumsum = period_df.groupby('name')['period_cumsum'].last().reset_index()
                
                # æ’åºå¹¶è·å–å‰Nå
                top_sectors = latest_cumsum.sort_values('period_cumsum', ascending=False).head(top_n)
                
                ranking_results[f'è¿‘{days}æ—¥æ’å'] = [
                    {
                        'rank': idx + 1,
                        'sector_name': row['name'],
                        'cumsum_change': round(row['period_cumsum'], 4)
                    }
                    for idx, (_, row) in enumerate(top_sectors.iterrows())
                ]
                
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—è¿‘{days}æ—¥æ’åæ—¶å‡ºé”™: {e}")
                ranking_results[f'è¿‘{days}æ—¥æ’å'] = []
        
        return ranking_results
